

<!DOCTYPE html>
<html lang="" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/Blog/img/favicon.png">
  <link rel="icon" href="/Blog/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Daolin">
  <meta name="keywords" content="">
  
    <meta name="description" content="&lt;">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow 学习笔记">
<meta property="og:url" content="https://daolinzhou.github.io/Blog/2020/12/21/play-tensorflow/index.html">
<meta property="og:site_name" content="Daolin&#39;s Repository">
<meta property="og:description" content="&lt;">
<meta property="og:locale">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/tf/ccnm.PNG">
<meta property="article:published_time" content="2020-12-21T23:01:36.000Z">
<meta property="article:modified_time" content="2021-01-02T06:45:19.093Z">
<meta property="article:author" content="Daolin">
<meta property="article:tag" content="python3">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daolinzhou.github.io/Blog/intro/tf/ccnm.PNG">
  
  
  <title>TensorFlow 学习笔记 - Daolin&#39;s Repository</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/Blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"daolinzhou.github.io","root":"/Blog/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/Blog/local-search.xml"};
  </script>
  <script  src="/Blog/js/utils.js" ></script>
  <script  src="/Blog/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/Blog/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/">
                <i class="iconfont icon-home-fill"></i>
                Hejmpaĝo
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Arkivoj
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                Kategorioj
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Etikedoj
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/about/">
                <i class="iconfont icon-user-fill"></i>
                Pri
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/Blog/intro/imprinting2.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="TensorFlow 学习笔记">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-12-21 15:01" pubdate>
        December 21, 2020 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      26k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      216 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">TensorFlow 学习笔记</h1>
            
            <div class="markdown-body">
              <p>&lt;</p>
<span id="more"></span>

<h1 id="TensorFlow-2-学习笔记"><a href="#TensorFlow-2-学习笔记" class="headerlink" title="TensorFlow 2 学习笔记"></a>TensorFlow 2 学习笔记</h1><p>写一个简单的神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 获取数据集</span><br>    fashion_mnist = keras.datasets.fashion_mnist<br>    (x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()<br>    x_valid, x_train = x_train_all[:<span class="hljs-number">5000</span>], x_train_all[<span class="hljs-number">5000</span>:]<br>    y_valid, y_train = y_train_all[:<span class="hljs-number">5000</span>], y_train_all[<span class="hljs-number">5000</span>:]<br><br>    <span class="hljs-comment"># 预处理, 方差归一化</span><br>    stand_scaler = StandardScaler()<br>    stand_scaler.fit(x_train.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    x_train_scaled = stand_scaler.transform(x_train.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>    x_valid_scaled = stand_scaler.transform(x_valid.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>    x_test_scaled = stand_scaler.transform(x_test.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br><br>    <span class="hljs-comment"># 建立模型</span><br>    model = keras.models.Sequential()<br>    model.add(keras.layers.Flatten(input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>]))	<span class="hljs-comment"># 把2维图片铺平为一维向量</span><br>    model.add(keras.layers.Dense(<span class="hljs-number">400</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;softmax&quot;</span>))<br><br>    <span class="hljs-comment"># 设置使用的损失函数, y 原本是index值, 需要用 onehot变成向量才行</span><br>    <span class="hljs-comment"># 所以用 sparse_categorical_crossentropy</span><br>    <span class="hljs-comment"># 而如果 y 本身就是 onehot 可以直接用 categorical_crossentropy, </span><br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>,<br>                  optimizer=<span class="hljs-string">&quot;adam&quot;</span>,   <span class="hljs-comment"># 模型求解方法</span><br>                  metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])<br>    history = model.fit(x_train_scaled, y_train, epochs=<span class="hljs-number">10</span>, validation_data=(x_valid_scaled, y_valid))<br><br>    <span class="hljs-comment"># 信息</span><br>    <span class="hljs-built_in">print</span>(model.summary())<br>    <span class="hljs-built_in">print</span>(history.history)<br></code></pre></td></tr></table></figure>

<br>

<br>

<h2 id="callbacks"><a href="#callbacks" class="headerlink" title="callbacks"></a>callbacks</h2><p>tensorflow 有很多callbacks. EarlyStopping 时提前终止的callback</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Tensorboard, earlystopping, ModelCheckpoint</span><br>logdir = <span class="hljs-string">&#x27;./callbacks&#x27;</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(logdir):<br>    os.mkdir(logdir)<br>output_model_file = os.path.join(logdir, <span class="hljs-string">&quot;fashion_mnist_model.h5&quot;</span>)<br>callbacks = [<br>    keras.callbacks.TensorBoard(logdir),<br>    keras.callbacks.ModelCheckpoint(output_model_file,<br>                                    save_best_only=<span class="hljs-literal">True</span>),       <span class="hljs-comment"># 存储模型</span><br>    keras.callbacks.EarlyStopping(patience=<span class="hljs-number">5</span>, min_delta=<span class="hljs-number">1e-3</span>)   <span class="hljs-comment"># 当 有5次 loss &lt; min_delta 时提前结束</span><br>]<br><br>history = model.fit(x_train_scaled, y_train, epochs=<span class="hljs-number">10</span>, validation_data=(x_valid_scaled, y_valid),<br>                    callbacks=callbacks)<br></code></pre></td></tr></table></figure>

<br>

<h2 id="批归一化"><a href="#批归一化" class="headerlink" title="批归一化"></a>批归一化</h2><p>归一化是指在训练之前, 对数据进行一次归一化.</p>
<p>而批归一化指的是在神经网络的每一层都进行一次归一化.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批归一化放在激活函数之后</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    model.add(keras.layers.Dense(<span class="hljs-number">100</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.BatchNormalization())    <span class="hljs-comment"># 归一化</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批归一化放在激活函数之前</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    model.add(keras.layers.Dense(<span class="hljs-number">100</span>))<br>    model.add(keras.layers.BatchNormalization())<br>    model.add(keras.layers.Activation(<span class="hljs-string">&quot;relu&quot;</span>))<br></code></pre></td></tr></table></figure>

<p>批归一化可以缓解梯度消失</p>
<br>

<h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p><img src="/Blog/Blog/intro/tf/dropout.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>随机的让某些单元失效(每一次训练弃用的单元都是随机的). 这样节点就无法<strong>组合</strong>地去学习, 每个节点必须独立学习数据的规律. 这样就可以防止过拟合</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model.add(keras.layers.AlphaDropout(rate=<span class="hljs-number">0.5</span>))<br>model.add(keras.layers.Dropout(rate=<span class="hljs-number">0.5</span>))<br></code></pre></td></tr></table></figure>

<p>dropout在tf中也是一层, 是对前一层的节点进行dropout</p>
<br>

<p><code>AlphaDropout</code> 比 <code>Dropout</code>更优秀, 它可以保证均值方差不变, 因此归一化的性质也不变.</p>
<br>

<h2 id="Wide-amp-Deep-模型"><a href="#Wide-amp-Deep-模型" class="headerlink" title="Wide &amp; Deep 模型"></a>Wide &amp; Deep 模型</h2><p>这个模型可以用于分类问题也可以用于回归问题</p>
<p>这个模型把数据分为稀疏特征和密集特征两种, 基于两种特征构建模型</p>
<ul>
<li>稀疏特征<ul>
<li>离散值特征</li>
<li>可以用one-hot表示</li>
<li>例如: 专业 &#x3D; {计算机, 人文, 其他}. 人文 &#x3D; $[0, 1, 0]$</li>
<li>例如: 词表&#x3D;{人工智能, 你, 它}, 它 &#x3D; $[0, 0, 1]$</li>
<li>叉乘: {(计算机, 人工智能), (计算机, 你) …}</li>
</ul>
</li>
<li>密集特征<ul>
<li>向量表达, 例如 专业 &#x3D; {计算机, 人文, 其他}. 人文 &#x3D; [0.3, 0.2], 此时就可以用向量之间的距离来表示两个词之间的距离</li>
<li>word2vec 工具就可以把单词转换为向量<ul>
<li>男-女 &#x3D; 国王-王后</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/Blog/Blog/intro/tf/wide_and_deep.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/tf/wide_and_deep_1.png" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/tf/wide_and_deep_2.png" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>此时就不能使用 Sequential 了, 因为此时一层有两种模式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = keras.layers.Input(shape=x_train.shape[<span class="hljs-number">1</span>:])<br><br><span class="hljs-comment"># 隐藏层1, 输入是input</span><br>hidden1 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)(<span class="hljs-built_in">input</span>)<br><br><span class="hljs-comment"># 隐藏层2, 输入是hidden1, deep model的输出</span><br>hidden2 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)(hidden1)<br><br><span class="hljs-comment"># 拼接 wide model 的输入和 deep model 的输出作整体的输入</span><br>concat = keras.layers.concatenate([<span class="hljs-built_in">input</span>, hidden2])<br><br><span class="hljs-comment"># 输出层 输入是拼接后的结果</span><br>output = keras.layers.Dense(<span class="hljs-number">1</span>)(concat)<br><br><span class="hljs-comment"># 固化 model</span><br>model = keras.models.Model(inputs=[<span class="hljs-built_in">input</span>], outputs=[output])<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>,<br>              optimizer=<span class="hljs-string">&quot;adam&quot;</span>,<br>              metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])<br></code></pre></td></tr></table></figure>

<br>

<p>另一种方法是重写子类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 函数式 API</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WideDeepModel</span>(keras.models.Model):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(WideDeepModel, self).__init__()<br>        <span class="hljs-string">&quot;&quot;&quot;定义模型层次&quot;&quot;&quot;</span><br>        self.hidden1_layer = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>        self.hidden2_layer = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)<br>        self.output_layer = keras.layers.Dense(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">call</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;完成模型的正向运算&quot;&quot;&quot;</span><br>        hidden1 = self.hidden1_layer(<span class="hljs-built_in">input</span>)<br>        hidden2 = self.hidden2_layer(hidden1)<br>        concat = keras.layers.concatenate([<span class="hljs-built_in">input</span>, hidden2])<br>        output = self.output_layer(concat)<br>        <span class="hljs-keyword">return</span> output<br><br>model = WideDeepModel()<br>model.build(input_shape=(<span class="hljs-literal">None</span>, <span class="hljs-number">8</span>))	<span class="hljs-comment"># None 是数据的个数, 8 是数据有多少特征</span><br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;mean_squared_error&quot;</span>,<br>              optimizer=<span class="hljs-string">&quot;adam&quot;</span>,<br>              metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])<br><br>history = model.fit(x_train_scaled, y_train, epochs=<span class="hljs-number">50</span>, validation_data=(x_valid_scaled, y_valid))<br>plot_learning_curve(history)<br></code></pre></td></tr></table></figure>

<br>

<h3 id="多输入"><a href="#多输入" class="headerlink" title="多输入"></a>多输入</h3><p>如果总共有8个特征, 我让前5个特征作为wide的输入, 后6个特征作为deep的输入.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    housing = fetch_california_housing()<br>    x_train_all, x_test, y_train_all, y_test = train_test_split(<br>        housing.data, housing.target, random_state=<span class="hljs-number">7</span>)<br>    x_train, x_valid, y_train, y_valid = train_test_split(<br>        x_train_all, y_train_all, random_state=<span class="hljs-number">11</span>)<br><br>    scaler = StandardScaler()<br>    x_train_scaled = scaler.fit_transform(x_train)<br>    x_valid_scaled = scaler.transform(x_valid)<br>    x_test_scaled = scaler.transform(x_test)<br><br>    <span class="hljs-comment"># 多输入</span><br>    input_wide = keras.layers.Input(shape=[<span class="hljs-number">5</span>])<br>    input_deep = keras.layers.Input(shape=[<span class="hljs-number">6</span>])<br>    hidden1 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(input_deep)<br>    hidden2 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(hidden1)<br>    concat = keras.layers.concatenate([input_wide, hidden2])<br>    output = keras.layers.Dense(<span class="hljs-number">1</span>)(concat)<br>    model = keras.models.Model(inputs=[input_wide, input_deep],<br>                               outputs=[output])<br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;mean_squared_error&quot;</span>, optimizer=<span class="hljs-string">&quot;sgd&quot;</span>)<br>    callbacks = [keras.callbacks.EarlyStopping(<br>        patience=<span class="hljs-number">5</span>, min_delta=<span class="hljs-number">1e-2</span>)]<br><br>    <span class="hljs-comment"># 拆分inputs</span><br>    x_train_scaled_wide = x_train_scaled[:, :<span class="hljs-number">5</span>]<br>    x_train_scaled_deep = x_train_scaled[:, <span class="hljs-number">2</span>:]<br>    x_valid_scaled_wide = x_valid_scaled[:, :<span class="hljs-number">5</span>]<br>    x_valid_scaled_deep = x_valid_scaled[:, <span class="hljs-number">2</span>:]<br>    x_test_scaled_wide = x_test_scaled[:, :<span class="hljs-number">5</span>]<br>    x_test_scaled_deep = x_test_scaled[:, <span class="hljs-number">2</span>:]<br><br>    history = model.fit([x_train_scaled_wide, x_train_scaled_deep],<br>                        y_train,<br>                        validation_data=(<br>                            [x_valid_scaled_wide, x_valid_scaled_deep],<br>                            y_valid),<br>                        epochs=<span class="hljs-number">100</span>,<br>                        callbacks=callbacks)<br></code></pre></td></tr></table></figure>

<br>

<br>

<h3 id="多输出"><a href="#多输出" class="headerlink" title="多输出"></a>多输出</h3><p>有时我们需要使用多输出(例如: 多任务学习问题)</p>
<p>例如: 对于一个房价问题, 我们既要预测当前的房价, 又要预测1年后的房价, 此时就有两个预测任务, 模型就要给出两个结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    housing = fetch_california_housing()<br>    x_train_all, x_test, y_train_all, y_test = train_test_split(<br>        housing.data, housing.target, random_state=<span class="hljs-number">7</span>)<br>    x_train, x_valid, y_train, y_valid = train_test_split(<br>        x_train_all, y_train_all, random_state=<span class="hljs-number">11</span>)<br><br>    scaler = StandardScaler()<br>    x_train_scaled = scaler.fit_transform(x_train)<br>    x_valid_scaled = scaler.transform(x_valid)<br>    x_test_scaled = scaler.transform(x_test)<br><br>    <span class="hljs-comment"># 多输出</span><br>    input_wide = keras.layers.Input(shape=[<span class="hljs-number">5</span>])<br>    input_deep = keras.layers.Input(shape=[<span class="hljs-number">6</span>])<br>    hidden1 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(input_deep)<br>    hidden2 = keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(hidden1)<br>    concat = keras.layers.concatenate([input_wide, hidden2])<br>    output = keras.layers.Dense(<span class="hljs-number">1</span>)(concat)<br>    output2 = keras.layers.Dense(<span class="hljs-number">1</span>)(hidden2)    <span class="hljs-comment"># 第二个输出</span><br>    model = keras.models.Model(inputs=[input_wide, input_deep],<br>                               outputs=[output, output2])<br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;mean_squared_error&quot;</span>, optimizer=<span class="hljs-string">&quot;sgd&quot;</span>)<br>    callbacks = [keras.callbacks.EarlyStopping(<br>        patience=<span class="hljs-number">5</span>, min_delta=<span class="hljs-number">1e-2</span>)]<br><br>    x_train_scaled_wide = x_train_scaled[:, :<span class="hljs-number">5</span>]<br>    x_train_scaled_deep = x_train_scaled[:, <span class="hljs-number">2</span>:]<br>    x_valid_scaled_wide = x_valid_scaled[:, :<span class="hljs-number">5</span>]<br>    x_valid_scaled_deep = x_valid_scaled[:, <span class="hljs-number">2</span>:]<br>    x_test_scaled_wide = x_test_scaled[:, :<span class="hljs-number">5</span>]<br>    x_test_scaled_deep = x_test_scaled[:, <span class="hljs-number">2</span>:]<br><br>    history = model.fit([x_train_scaled_wide, x_train_scaled_deep],<br>                        [y_train, y_train],<br>                        validation_data=(<br>                            [x_valid_scaled_wide, x_valid_scaled_deep],<br>                            [y_valid, y_valid]),<br>                        epochs=<span class="hljs-number">100</span>,<br>                        callbacks=callbacks)<br><br></code></pre></td></tr></table></figure>

<br>

<br>

<h2 id="超参数搜索"><a href="#超参数搜索" class="headerlink" title="超参数搜索"></a>超参数搜索</h2><p>超参数有很多种搜索策略</p>
<ul>
<li>网格搜索</li>
<li>随机搜索</li>
<li>遗传算法搜索</li>
<li>启发式搜索</li>
</ul>
<br>

<h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p><img src="/Blog/Blog/intro/tf/param_search.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>这个在机器学习的笔记中讲过</p>
<p>可以用并行化的方式进行计算. 例如一台机器计算第一行的参数, 另一台机器计算第二行的参数</p>
<br>

<h3 id="随机搜索"><a href="#随机搜索" class="headerlink" title="随机搜索"></a>随机搜索</h3><p>如果数据是连续的, 那么有可能网格搜索会跳过某个最优参数. 而使用随机搜索就可以使得最优解有概率被访问到.</p>
<p><img src="/Blog/Blog/intro/tf/param_search_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h3 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h3><p>遗传算法是模拟自然界的遗传.</p>
<p>它的思想是这样:</p>
<ol>
<li>初始化候选参数集合 -&gt; 训练 -&gt; 得到模型指标(把这个指标当作生存概率)</li>
<li>基于指标(生存概率)进行随机选择<ul>
<li>把参数进行交叉. (选择一个参数集合中的一些参数, 再选择另外一个参数集合中的一些参数组合成一个新的参数集合). </li>
<li>变异: 对某几个参数进行微小的调整 (DNA除了来自父母, 还会有变异的一部分)</li>
<li>产生下一代集合</li>
</ul>
</li>
<li>回到 (1) 进行下一轮</li>
</ol>
<br>

<h3 id="启发式搜索"><a href="#启发式搜索" class="headerlink" title="启发式搜索"></a>启发式搜索</h3><p>使用循环神经网络来生成参数</p>
<p>根据这组参数形成网络结构, 进行训练.</p>
<p>使用强化学习来进行反馈, 使用模型来训练生成参数</p>
<br>

<br>

<p>tensorflow 有 KerasClassifier 和 KerasRegressor. 这两个类是对tf的封装, 使得它们支持 scikit-learn 的 API. </p>
<p>这样就可以使用 scikit-learn 的 参数搜索.</p>
<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>(<span class="hljs-params">hidden_layers=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">                layer_size=<span class="hljs-number">30</span>,</span><br><span class="hljs-params">                learning_rate=<span class="hljs-number">3e-3</span></span>):<br>    model = keras.models.Sequential()<br>    model.add(keras.layers.Dense(layer_size, activation=<span class="hljs-string">&quot;relu&quot;</span>,<br>                                 input_shape=x_train.shape[<span class="hljs-number">1</span>:]))<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(hidden_layers - <span class="hljs-number">1</span>):<br>        model.add(keras.layers.Dense(layer_size, activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.Dense(<span class="hljs-number">1</span>))<br>    optimizer = keras.optimizers.SGD(learning_rate)<br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;mse&quot;</span>, optimizer=optimizer)<br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-comment"># 封装模型</span><br>sklearn_model = keras.wrappers.scikit_learn.KerasRegressor(build_model)  <span class="hljs-comment"># 传入函数</span><br><br><span class="hljs-comment"># 定义搜索范围</span><br><span class="hljs-comment"># f(x) = 1/(x*log(b/a)),  a &lt;= x &lt;= b</span><br>param_distribution = &#123;<br>    <span class="hljs-string">&quot;hidden_layers&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],<br>    <span class="hljs-string">&quot;layer_size&quot;</span>: np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>),<br>    <span class="hljs-string">&quot;learning_rate&quot;</span>: reciprocal(<span class="hljs-number">1e-4</span>, <span class="hljs-number">1e-2</span>)<br>&#125;<br><br><span class="hljs-comment"># 随机搜索</span><br>random_search_cv = RandomizedSearchCV(sklearn_model, param_distribution, n_iter=<span class="hljs-number">10</span>, n_jobs=<span class="hljs-number">1</span>)<br>random_search_cv.fit(x_train_scaled, y_train, epochs=<span class="hljs-number">100</span>, validation_data=(x_valid_scaled, y_valid))<br><br></code></pre></td></tr></table></figure>

<br>

<br>

<h2 id="TensorFlow-基础API"><a href="#TensorFlow-基础API" class="headerlink" title="TensorFlow 基础API"></a>TensorFlow 基础API</h2><p><code>@tf.function</code></p>
<p>将 python 函数编译成 tensorflow 的图结构</p>
<p>这样可以把模型导出称为 <code>GraphDef+checkpoint</code> 或者<code>SavedModel</code></p>
<p>使得eager execution可以默认打开</p>
<p>tf 1.0 的代码可以通过 <code>tf.function</code> 来继续在2.0中使用, 可以代替session</p>
<br>

<h3 id="tf-constant"><a href="#tf-constant" class="headerlink" title="tf.constant"></a>tf.constant</h3><p>使用起来和 numpy 很像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    t = tf.constant([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])<br>    <span class="hljs-built_in">print</span>(t)<br>    <span class="hljs-built_in">print</span>(t[:, <span class="hljs-number">1</span>:])<br>    <span class="hljs-built_in">print</span>(t[..., <span class="hljs-number">1</span>])<br>    <span class="hljs-built_in">print</span>(tf.reshape(t[:, <span class="hljs-number">1</span>], (-<span class="hljs-number">1</span>)))<br></code></pre></td></tr></table></figure>

<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua">tf.Tensor(<br><span class="hljs-string">[[1. 2. 3.]</span><br><span class="hljs-string"> [4. 5. 6.]]</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=float32)<br>tf.Tensor(<br><span class="hljs-string">[[2. 3.]</span><br><span class="hljs-string"> [5. 6.]]</span>, shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)<br>tf.Tensor([<span class="hljs-number">2.</span> <span class="hljs-number">5.</span>], shape=(<span class="hljs-number">2</span>,), dtype=float32)<br>tf.Tensor([<span class="hljs-number">2.</span> <span class="hljs-number">5.</span>], shape=(<span class="hljs-number">2</span>,), dtype=float32)<br></code></pre></td></tr></table></figure>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    t = tf.constant([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>], [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])<br>    <span class="hljs-built_in">print</span>(t+<span class="hljs-number">10</span>)<br>    <span class="hljs-built_in">print</span>(tf.square(t))     <span class="hljs-comment"># 每个值都平方</span><br>    <span class="hljs-built_in">print</span>(t ** <span class="hljs-number">3</span>)           <span class="hljs-comment"># 每个值都立方</span><br>    <span class="hljs-built_in">print</span>(t @ tf.transpose(t))  <span class="hljs-comment"># 点乘</span><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.Tensor(<br>[[<span class="hljs-number">11.</span> <span class="hljs-number">12.</span> <span class="hljs-number">13.</span>]<br> [<span class="hljs-number">14.</span> <span class="hljs-number">15.</span> <span class="hljs-number">16.</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=float32)<br>tf.Tensor(<br>[[ <span class="hljs-number">1.</span>  <span class="hljs-number">4.</span>  <span class="hljs-number">9.</span>]<br> [<span class="hljs-number">16.</span> <span class="hljs-number">25.</span> <span class="hljs-number">36.</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=float32)<br>tf.Tensor(<br>[[  <span class="hljs-number">1.</span>   <span class="hljs-number">8.</span>  <span class="hljs-number">27.</span>]<br> [ <span class="hljs-number">64.</span> <span class="hljs-number">125.</span> <span class="hljs-number">216.</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), dtype=float32)<br>tf.Tensor(<br>[[<span class="hljs-number">14.</span> <span class="hljs-number">32.</span>]<br> [<span class="hljs-number">32.</span> <span class="hljs-number">77.</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)<br></code></pre></td></tr></table></figure>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np_t = t.numpy()    <span class="hljs-comment"># 把 tf constant 转为 numpy 数组</span><br>tf_t = tf.constant(np_t)    <span class="hljs-comment"># 把 numpy 数组转为 tf constant</span><br></code></pre></td></tr></table></figure>

<br>

<br>

<h3 id="tf-strings"><a href="#tf-strings" class="headerlink" title="tf.strings"></a>tf.strings</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    t = tf.constant(<span class="hljs-string">&quot;cafe&quot;</span>)<br>    <span class="hljs-built_in">print</span>(t)<br>    <span class="hljs-built_in">print</span>(tf.strings.length(t))<br>    <span class="hljs-built_in">print</span>(tf.strings.length(t, unit=<span class="hljs-string">&quot;UTF8_CHAR&quot;</span>))<br>    <span class="hljs-built_in">print</span>(tf.strings.unicode_decode(t, <span class="hljs-string">&quot;UTF8&quot;</span>))<br></code></pre></td></tr></table></figure>

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">tf.<span class="hljs-constructor">Tensor(<span class="hljs-params">b</span>&#x27;<span class="hljs-params">cafe</span>&#x27;, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">string</span>)<br>tf.<span class="hljs-constructor">Tensor(4, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(4, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor([ 99  97 102 101], <span class="hljs-params">shape</span>=(4,)</span>, dtype=<span class="hljs-built_in">int32</span>)<br></code></pre></td></tr></table></figure>

<br>

<h3 id="Ragged-Tensor"><a href="#Ragged-Tensor" class="headerlink" title="Ragged Tensor"></a>Ragged Tensor</h3><p>r是一个不完整的矩阵(RaggedTensor), 因为每个字符串长度都不一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    t = tf.constant([<span class="hljs-string">&quot;cafe&quot;</span>, <span class="hljs-string">&quot;coffee&quot;</span>, <span class="hljs-string">&quot;咖啡&quot;</span>])<br>    <span class="hljs-built_in">print</span>(tf.strings.length(t, unit=<span class="hljs-string">&quot;UTF8_CHAR&quot;</span>))<br>    r = tf.strings.unicode_decode(t, <span class="hljs-string">&quot;UTF8&quot;</span>)<br>    <span class="hljs-built_in">print</span>(r)<br></code></pre></td></tr></table></figure>

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">tf<span class="hljs-selector-class">.Tensor</span>(<span class="hljs-selector-attr">[4 6 2]</span>, shape=(<span class="hljs-number">3</span>,), dtype=int32)<br>&lt;tf<span class="hljs-selector-class">.RaggedTensor</span> <span class="hljs-selector-attr">[[99, 97, 102, 101]</span>, <span class="hljs-selector-attr">[99, 111, 102, 102, 101, 101]</span>, <span class="hljs-selector-attr">[21654, 21857]</span>]&gt;<br></code></pre></td></tr></table></figure>

<br>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    r1 = tf.ragged.constant([[<span class="hljs-number">11</span>, <span class="hljs-number">12</span>], [<span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>], [], [<span class="hljs-number">41</span>]])<br>    r2 = tf.ragged.constant([[<span class="hljs-number">13</span>, <span class="hljs-number">14</span>], [<span class="hljs-number">15</span>], [], [<span class="hljs-number">42</span>, <span class="hljs-number">43</span>]])<br>    <span class="hljs-built_in">print</span>(tf.concat([r1, r2], axis=<span class="hljs-number">1</span>)) <span class="hljs-comment"># 拼接tensor</span><br>    <span class="hljs-built_in">print</span>(r1.to_tensor()) <span class="hljs-comment"># 转换为普通矩阵, 空白位置用0补齐</span><br></code></pre></td></tr></table></figure>

<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs inform7">&lt;tf.RaggedTensor <span class="hljs-comment">[<span class="hljs-comment">[11, 12, 13, 14]</span>, <span class="hljs-comment">[21, 22, 23, 15]</span>, <span class="hljs-comment">[]</span>, <span class="hljs-comment">[41, 42, 43]</span>]</span>&gt;<br>tf.Tensor(<br><span class="hljs-comment">[<span class="hljs-comment">[11 12  0]</span></span><br><span class="hljs-comment"> <span class="hljs-comment">[21 22 23]</span></span><br><span class="hljs-comment"> <span class="hljs-comment">[ 0  0  0]</span></span><br><span class="hljs-comment"> <span class="hljs-comment">[41  0  0]</span>]</span>, shape=(4, 3), dtype=int32)<br><br></code></pre></td></tr></table></figure>

<p>然而这样得到的矩阵0一定在值的右侧.</p>
<br>

<p>如何指定0的位置? 换句话说指定非零值的位置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># indices 必须是排好序的, 不然to_dense会报错</span><br>    s = tf.SparseTensor(indices=[[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]],<br>                        values=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                        dense_shape=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>    <span class="hljs-built_in">print</span>(s)<br>    <span class="hljs-built_in">print</span>(tf.sparse.to_dense(s))<br></code></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">SparseTensor(<span class="hljs-attribute">indices</span>=tf.Tensor(<br>[[0 1]<br> [1 0]<br> [2 3]], shape=(3, 2), <span class="hljs-attribute">dtype</span>=int64), <span class="hljs-attribute">values</span>=tf.Tensor([1 2 3], shape=(3,), <span class="hljs-attribute">dtype</span>=int32), <span class="hljs-attribute">dense_shape</span>=tf.Tensor([3 4], shape=(2,), <span class="hljs-attribute">dtype</span>=int64))<br>tf.Tensor(<br>[[0 1 0 0]<br> [2 0 0 0]<br> [0 0 0 3]], shape=(3, 4), <span class="hljs-attribute">dtype</span>=int32)<br></code></pre></td></tr></table></figure>

<br>

<p>如果indices是乱序的, 调用<code>to_dense</code> 之前要 <code>reorder</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    s = tf.SparseTensor(indices=[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]],<br>                        values=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                        dense_shape=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>    s_order = tf.sparse.reorder(s)<br>    <span class="hljs-built_in">print</span>(s)<br>    <span class="hljs-built_in">print</span>(tf.sparse.to_dense(s_order))<br></code></pre></td></tr></table></figure>

<br>

<br>

<h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><p>variable 和 constant 类似, 只不过 variable 是可以被重新赋值的.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    v = tf.Variable([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br>    v.assign(<span class="hljs-number">2</span>*v)           <span class="hljs-comment"># 对整个矩阵进行重新赋值</span><br>    <span class="hljs-built_in">print</span>(v)<br>    v[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>].assign(<span class="hljs-number">42</span>)      <span class="hljs-comment"># 对一个元素进行重新赋值</span><br>    <span class="hljs-built_in">print</span>(v)<br>    v[<span class="hljs-number">1</span>].assign([<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>])  <span class="hljs-comment"># 对一行进行重新赋值</span><br>    <span class="hljs-built_in">print</span>(v)<br>    v[:, <span class="hljs-number">1</span>].assign([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>])  <span class="hljs-comment"># 对一列进行重新赋值</span><br>    <span class="hljs-built_in">print</span>(v)<br></code></pre></td></tr></table></figure>

<br>

<br>

<h3 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    housing = fetch_california_housing()<br>    x_train_all, x_test, y_train_all, y_test = train_test_split(<br>        housing.data, housing.target, random_state=<span class="hljs-number">7</span>)<br>    x_train, x_valid, y_train, y_valid = train_test_split(<br>        x_train_all, y_train_all, random_state=<span class="hljs-number">11</span>)<br><br>    scaler = StandardScaler()<br>    x_train_scaled = scaler.fit_transform(x_train)<br>    x_valid_scaled = scaler.transform(x_valid)<br>    x_test_scaled = scaler.transform(x_test)<br><br>    <span class="hljs-comment"># 自定义mse</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">customized_mse</span>(<span class="hljs-params">y_true, y_pred</span>):<br>        <span class="hljs-keyword">return</span> tf.reduce_mean(tf.square(y_pred-y_true))<br><br>    model = keras.models.Sequential([<br>        keras.layers.Dense(<span class="hljs-number">30</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>, input_shape=x_train.shape[<span class="hljs-number">1</span>:]),<br>        keras.layers.Dense(<span class="hljs-number">1</span>)<br>    ])<br><br>    <span class="hljs-comment"># 把loss设置为自定义的损失函数即可</span><br>    model.<span class="hljs-built_in">compile</span>(loss=customized_mse, optimizer=<span class="hljs-string">&quot;sgd&quot;</span>, metrics=[<span class="hljs-string">&quot;mean_squared_error&quot;</span>])<br>    history = model.fit(x_train_scaled, y_train,<br>              validation_data=(x_valid_scaled, y_valid), epochs=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure>

<br>

<br>

<h3 id="Tf-data"><a href="#Tf-data" class="headerlink" title="Tf.data"></a>Tf.data</h3><p>from_tensor_slices 可以传入 列表, numpy数组, 字典…</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dataset = tf.data.Dataset.from_tensor_slices(np.arange(<span class="hljs-number">10</span>))<br>    <span class="hljs-built_in">print</span>(dataset)<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> dataset:<br>        <span class="hljs-built_in">print</span>(item)<br></code></pre></td></tr></table></figure>

<p>输出结果:</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">&lt;TensorSliceDataset shapes: <span class="hljs-literal">()</span>, types: tf.<span class="hljs-built_in">int32</span>&gt;<br>tf.<span class="hljs-constructor">Tensor(0, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(1, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(2, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(3, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(4, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(5, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(6, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(7, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(8, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(9, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br></code></pre></td></tr></table></figure>

<br>

<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haxe"><span class="hljs-keyword">new</span><span class="hljs-type">_dataset</span> = dataset.repeat(<span class="hljs-number">3</span>) <span class="hljs-meta"># 返回一个新的dataset, 元素重复3次</span><br></code></pre></td></tr></table></figure>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = dataset.repeat(<span class="hljs-number">3</span>).batch(<span class="hljs-number">7</span>)<br><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> dataset:<br>    <span class="hljs-built_in">print</span>(item)<br></code></pre></td></tr></table></figure>

<p>输出: 30个元素, 每7个分为一份, 可以分成4份, 余下的2个单独组成一份</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tf</span>.Tensor([<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span>], shape=(<span class="hljs-number">7</span>,), dtype=int32)<br><span class="hljs-attribute">tf</span>.Tensor([<span class="hljs-number">7</span> <span class="hljs-number">8</span> <span class="hljs-number">9</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>], shape=(<span class="hljs-number">7</span>,), dtype=int32)<br><span class="hljs-attribute">tf</span>.Tensor([<span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span> <span class="hljs-number">8</span> <span class="hljs-number">9</span> <span class="hljs-number">0</span>], shape=(<span class="hljs-number">7</span>,), dtype=int32)<br><span class="hljs-attribute">tf</span>.Tensor([<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span> <span class="hljs-number">7</span>], shape=(<span class="hljs-number">7</span>,), dtype=int32)<br><span class="hljs-attribute">tf</span>.Tensor([<span class="hljs-number">8</span> <span class="hljs-number">9</span>], shape=(<span class="hljs-number">2</span>,), dtype=int32)<br></code></pre></td></tr></table></figure>

<br>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dataset = tf.data.Dataset.from_tensor_slices(np.arange(<span class="hljs-number">10</span>))<br>    dataset = dataset.repeat(<span class="hljs-number">3</span>).batch(<span class="hljs-number">7</span>)<br>    <span class="hljs-comment"># interleave, 对dataset中每一个元素进行处理形成新的数据集</span><br><br>    dataset2 = dataset.interleave(<br>        map_func=<span class="hljs-keyword">lambda</span> v: tf.data.Dataset.from_tensor_slices(v),  <span class="hljs-comment"># map_func, 进行什么样的变化</span><br>        cycle_length=<span class="hljs-number">5</span>,      <span class="hljs-comment"># cycle_length 并行程度, 同时处理多少元素</span><br>        block_length=<span class="hljs-number">5</span>       <span class="hljs-comment"># block_length 从变换结果中每次取多少个元素出来</span><br>    )<br><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> dataset2:<br>        <span class="hljs-built_in">print</span>(item)<br></code></pre></td></tr></table></figure>

<br>

<p>结果:</p>
<p>0, 1, 2, 3, 4 取自第一个tensor中的前五个元素</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">tf.<span class="hljs-constructor">Tensor(0, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(1, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(2, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(3, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(4, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(7, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(8, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(9, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(0, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(1, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(4, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(5, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(6, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(7, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(8, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(1, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(2, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(3, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(4, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(5, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(8, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(9, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(5, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(6, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(2, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(3, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(9, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(0, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(6, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br>tf.<span class="hljs-constructor">Tensor(7, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">int32</span>)<br></code></pre></td></tr></table></figure>

<br>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    x = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br>    y = np.array([<span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;fox&#x27;</span>])<br>    dataset = tf.data.Dataset.from_tensor_slices((x, y))<br><br>    <span class="hljs-keyword">for</span> item_x, item_y <span class="hljs-keyword">in</span> dataset:<br>        <span class="hljs-built_in">print</span>(item_x, item_y)<br></code></pre></td></tr></table></figure>

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">tf.<span class="hljs-constructor">Tensor([1 2], <span class="hljs-params">shape</span>=(2,)</span>, dtype=<span class="hljs-built_in">int32</span>) tf.<span class="hljs-constructor">Tensor(<span class="hljs-params">b</span>&#x27;<span class="hljs-params">cat</span>&#x27;, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">string</span>)<br>tf.<span class="hljs-constructor">Tensor([3 4], <span class="hljs-params">shape</span>=(2,)</span>, dtype=<span class="hljs-built_in">int32</span>) tf.<span class="hljs-constructor">Tensor(<span class="hljs-params">b</span>&#x27;<span class="hljs-params">dog</span>&#x27;, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">string</span>)<br>tf.<span class="hljs-constructor">Tensor([5 6], <span class="hljs-params">shape</span>=(2,)</span>, dtype=<span class="hljs-built_in">int32</span>) tf.<span class="hljs-constructor">Tensor(<span class="hljs-params">b</span>&#x27;<span class="hljs-params">fox</span>&#x27;, <span class="hljs-params">shape</span>=()</span>, dtype=<span class="hljs-built_in">string</span>)<br></code></pre></td></tr></table></figure>

<br>

<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    x = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br>    y = np.array([<span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-string">&#x27;fox&#x27;</span>])<br>    dataset = tf.data.Dataset.from_tensor_slices(&#123;<span class="hljs-string">&quot;feature&quot;</span>: x, <span class="hljs-string">&quot;label&quot;</span>: y&#125;)<br><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> dataset:<br>        <span class="hljs-built_in">print</span>(item)<br></code></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">&#123;<span class="hljs-string">&#x27;feature&#x27;</span>: &lt;tf.Tensor: shape=(2,), <span class="hljs-attribute">dtype</span>=int32, <span class="hljs-attribute">numpy</span>=array([1, 2])&gt;, <span class="hljs-string">&#x27;label&#x27;</span>: &lt;tf.Tensor: shape=(), <span class="hljs-attribute">dtype</span>=string, <span class="hljs-attribute">numpy</span>=b&#x27;cat&#x27;&gt;&#125;<br>&#123;<span class="hljs-string">&#x27;feature&#x27;</span>: &lt;tf.Tensor: shape=(2,), <span class="hljs-attribute">dtype</span>=int32, <span class="hljs-attribute">numpy</span>=array([3, 4])&gt;, <span class="hljs-string">&#x27;label&#x27;</span>: &lt;tf.Tensor: shape=(), <span class="hljs-attribute">dtype</span>=string, <span class="hljs-attribute">numpy</span>=b&#x27;dog&#x27;&gt;&#125;<br>&#123;<span class="hljs-string">&#x27;feature&#x27;</span>: &lt;tf.Tensor: shape=(2,), <span class="hljs-attribute">dtype</span>=int32, <span class="hljs-attribute">numpy</span>=array([5, 6])&gt;, <span class="hljs-string">&#x27;label&#x27;</span>: &lt;tf.Tensor: shape=(), <span class="hljs-attribute">dtype</span>=string, <span class="hljs-attribute">numpy</span>=b&#x27;fox&#x27;&gt;&#125;<br></code></pre></td></tr></table></figure>

<br>

<br>











<br>

<br>

<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络的核心就是卷积. 通常卷积神经网络和卷积操作用于处理图像.</p>
<p>可以把卷积操作看作是对图像的预处理, 再通过神经网络分析卷积后的结果.</p>
<p>不过卷积核再网络中是可变的, 也就相当于由卷积神经网络自己判断要怎么对图像预处理.</p>
<br>

<h2 id="卷积神经网络的问题"><a href="#卷积神经网络的问题" class="headerlink" title="卷积神经网络的问题"></a>卷积神经网络的问题</h2><h3 id="参数过多"><a href="#参数过多" class="headerlink" title="参数过多"></a>参数过多</h3><p>如果图像是(1000, 1000)</p>
<p>假设下一层神经元的个数是$10^6$</p>
<p>那么全连接参数就是输入乘输出$1000<em>1000</em>10^6&#x3D;10^{12}$</p>
<p>这么大的数据量可能会导致计算资源不足, 且容易过拟合</p>
<p><img src="/Blog/Blog/intro/tf/lc.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>解决方法之一就是使用<strong>局部卷积</strong>. 一个单元只和图像中的一部分进行连接.</p>
<br>

<p>如果局部连接范围是(10, 10)</p>
<p>那么全连接参数为$10<em>10</em>10^6&#x3D;10^8$</p>
<p>虽然减少很多参数量, 但是参数量依然很大</p>
<br>

<p>另一个方法就是<strong>参数共享</strong></p>
<p>不同的局部连接的参数是一样的</p>
<p><img src="/Blog/Blog/intro/tf/lc.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>例如图中红色单元与(10, 10)的区域作连接, 有100个参数.</p>
<p>蓝色单元同样与(10, 10) 的区域作连接, 有100个参数. 但是我们让它与红色单元是同一组100个参数</p>
<p>此时全连接参数为$10*10 &#x3D; 100$</p>
<br>

<br>

<p>为什么参数共享起作用?</p>
<p>这是因为图像的区域性, 图像特征和位置无关</p>
<p>例如一个脸无论是在图片的左上方还是在图片的右下方, 它都应该是一张脸.</p>
<br>

<br>

<h2 id="卷积神经网络实现"><a href="#卷积神经网络实现" class="headerlink" title="卷积神经网络实现"></a>卷积神经网络实现</h2><p>无论原图有多少channel, 在使用同样channel数量的卷积核卷积后channel都变为1. 因此我们多设置几个卷积核, filter&#x3D;32就说明有32个卷积核, 卷积后的结果就有32个channel.</p>
<p><img src="/Blog/Blog/intro/tf/lc_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/tf/lc_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 准备数据</span><br>    fashion_mnist = keras.datasets.fashion_mnist<br>    (x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()<br>    x_valid, x_train = x_train_all[:<span class="hljs-number">5000</span>], x_train_all[<span class="hljs-number">5000</span>:]<br>    y_valid, y_train = y_train_all[:<span class="hljs-number">5000</span>], y_train_all[<span class="hljs-number">5000</span>:]<br>    scaler = StandardScaler()<br>    x_train_scaled = scaler.fit_transform(<br>        x_train.astype(np.float32).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)<br>    x_valid_scaled = scaler.transform(<br>        x_valid.astype(np.float32).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)<br>    x_test_scaled = scaler.transform(<br>        x_test.astype(np.float32).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型, 卷积神经网络</span><br>    <span class="hljs-comment"># 两层卷积, 一层池化</span><br>    model = keras.models.Sequential()<br>    model.add(keras.layers.Conv2D(filters=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                  padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                  activation=<span class="hljs-string">&quot;relu&quot;</span>,<br>                                  input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)))<br>    model.add(keras.layers.Conv2D(filters=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                  padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                  activation=<span class="hljs-string">&quot;relu&quot;</span>))<br><br>    model.add(keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>))<br>    model.add(keras.layers.Conv2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                  padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                  activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.Conv2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                  padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                  activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>))<br>    model.add(keras.layers.Conv2D(filters=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                  padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                  activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.Conv2D(filters=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                  padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                  activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>))<br>    model.add(keras.layers.Flatten())<br>    model.add(keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>))<br>    model.add(keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;softmax&quot;</span>))<br><br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>,<br>                  optimizer=<span class="hljs-string">&quot;sgd&quot;</span>,<br>                  metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])<br>    callbacks = [keras.callbacks.EarlyStopping(patience=<span class="hljs-number">5</span>, min_delta=<span class="hljs-number">1e-3</span>)]<br>    history = model.fit(x_train_scaled, y_train, epochs=<span class="hljs-number">10</span>,<br>                        validation_data=(x_valid_scaled, y_valid),<br>                        callbacks=callbacks)<br></code></pre></td></tr></table></figure>

<p>通常在池化后要把卷积层的filter翻倍.</p>
<p>因为池化会导致长宽缩小为原来的1&#x2F;2, 有一些信息损失了</p>
<p>为了缓解这种损失, 我们把filter翻倍</p>
<br>

<br>

<h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p>是普通卷积的变种, 可以替代普通卷积构成卷积神经网络</p>
<p><code>BN</code> 是批归一化</p>
<p><img src="/Blog/Blog/intro/tf/dc.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>左边是普通的卷积操作, 右边(Depthwise)就是深度可分离卷积操作</p>
<br>

<h3 id="Inception-V3"><a href="#Inception-V3" class="headerlink" title="Inception V3"></a>Inception V3</h3><p><img src="/Blog/Blog/intro/tf/dc_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>为什么要这样作分支?</p>
<p>这样可以有不同尺寸<strong>视野域</strong></p>
<blockquote>
<p>视野域:</p>
<ul>
<li>不同的视野域提取的图像特征尺度不同</li>
</ul>
<p><img src="/Blog/Blog/intro/tf/dc_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>例如最顶上的那个方块的视野域是(5, 5)的方阵, 而中间层的每一个方块的视野域是(3, 3)的区域</p>
<p>视野域可以叠加, 两层3x3的视野域就是5x5</p>
</blockquote>
<br>

<p><img src="/Blog/Blog/intro/tf/dc_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>而在这个结构中, 每个分支视野域是不一样的.</p>
<p>从左到右视野域分别是(1, 1), (3, 3), (5, 5)(average pool会是图像宽高变为原来的1&#x2F;2), (5, 5)</p>
<br>

<br>

<p>深度可分离卷积也是一种<strong>分支</strong>的网络结构, 只不过它的分支是建立在不同的网络通道之上.</p>
<p><img src="/Blog/Blog/intro/tf/dc_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>对于输入, 可以经过一个1x1的卷积得到一个多通道的输出. 我们把这个输出分为3份分别输入给3个不同的分支, 再把不同结果进行合并</p>
<p>如果分到极值: 把每个通道都分成一份</p>
<p><img src="/Blog/Blog/intro/tf/dc_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/tf/dc_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>而且深度可分离卷积的计算速度相对普通卷积快</p>
<br>

<p>普通卷积的计算量 $D_K<em>D_K</em>M<em>N</em>D_F*D_F$</p>
<p>$D_K$ 核的宽高(通常为正方形所以是$D_K^2$)</p>
<p>$D_F$ 是图的宽高</p>
<p>$M$是图像的channel数(输入的channel数)</p>
<p>$N$是输出的channel数</p>
<p>对单个像素进行卷积就是$D_K<em>D_K</em>M$, 对于一个卷积核来说, 就是 $D_K<em>D_K</em>M<em>D_F</em>D_F$ </p>
<p>如果有N个filter, 那么就是 $D_K<em>D_K</em>M<em>N</em>D_F*D_F$</p>
<br>

<p><img src="/Blog/Blog/intro/tf/dc_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>深度可分离卷积的计算量 $D_K<em>D_K</em>M<em>D_F</em>D_F$</p>
<br>

<p>1x1卷积的计算了则是 $M<em>N</em>D_F*D_F$ 因为 $D_K&#x3D;1$</p>
<br>

<p>优化比例就是:<br>$$<br>\frac {D_K<em>D_K</em>M<em>D_F</em>D_F+M<em>N</em>D_F<em>D_F} {D_K</em>D_K<em>M</em>N<em>D_F</em>D_F}<br>$$</p>
<p>$$<br>&#x3D;\frac {1}{N}+\frac {1}{D_K^2}<br>$$</p>
<p>这个值就是深度可分离卷积的计算量与普通卷积计算量的比值.</p>
<p>这个值越小, 说明优化越大</p>
<br>

<br>

<p>要在tensorflow中使用深度可分离卷积需要使用<code>SeparableConv2D</code> 而不是Conv2D</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">model = keras.models.Sequential()<br>model.add(keras.layers.SeparableConv2D(filters=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                       padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                       activation=<span class="hljs-string">&quot;relu&quot;</span>,<br>                                       input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)))<br>model.add(keras.layers.SeparableConv2D(filters=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>,<br>                                       padding=<span class="hljs-string">&quot;same&quot;</span>,<br>                                       activation=<span class="hljs-string">&quot;relu&quot;</span>))<br><br>model.add(keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure>

<p>深度卷积可分离网络的准确率降低了一些, 但计算量少, 可以在手机端运行</p>
<br>

<br>

<h3 id="经典卷积网络模型"><a href="#经典卷积网络模型" class="headerlink" title="经典卷积网络模型"></a>经典卷积网络模型</h3><h4 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h4><p><img src="/Blog/Blog/intro/tf/ccnm.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>论文: <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf</a></p>
<br>

<h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p>适用于识别较为复杂的彩色图, 可识别1000种类别</p>
<p>结构复杂, 使用relu作为激活函数.</p>
<p><img src="/Blog/Blog/intro/tf/ccnm_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<h4 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h4><p><img src="/Blog/Blog/intro/tf/ccnm_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<br>

<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p><img src="/Blog/Blog/intro/tf/embedding_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>先使用不同的神经网络对文本进行二分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_learning_curve</span>(<span class="hljs-params">history, label, epochs, min_values, max_values</span>):<br>    data = &#123;&#125;<br>    data[label] = history.history[label]<br>    data[<span class="hljs-string">&#x27;val_&#x27;</span>+label] = history.history[<span class="hljs-string">&#x27;val_&#x27;</span>+label]<br>    pd.DataFrame(data).plot(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>))<br>    plt.grid(<span class="hljs-literal">True</span>)<br>    plt.axis([<span class="hljs-number">0</span>, epochs, min_values, max_values])<br>    plt.show()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode_review</span>(<span class="hljs-params">reverse_word_index, text_ids</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(word_id, <span class="hljs-string">&quot;&lt;UNK&gt;&quot;</span>) <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> text_ids])<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    imdb = keras.datasets.imdb<br>    vocab_size = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 只处理10000个不同的字符, 其余的字符会变为 unk</span><br>    index_from = <span class="hljs-number">3</span><br>    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, index_from=index_from)<br>    <span class="hljs-built_in">print</span>(train_data.shape, train_labels.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(train_data[<span class="hljs-number">0</span>]), <span class="hljs-built_in">len</span>(train_data[<span class="hljs-number">1</span>]))  <span class="hljs-comment"># 样本是变长的</span><br><br>    word_index = imdb.get_word_index()<br>    <span class="hljs-comment"># index 改为从4开始, 这样流出4个位置放置特殊字符</span><br>    word_index = &#123;k: (v + <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word_index.items()&#125;<br><br>    <span class="hljs-comment"># 特殊字符</span><br>    word_index[<span class="hljs-string">&#x27;&lt;PAD&gt;&#x27;</span>] = <span class="hljs-number">0</span><br>    word_index[<span class="hljs-string">&#x27;&lt;START&gt;&#x27;</span>] = <span class="hljs-number">1</span><br>    word_index[<span class="hljs-string">&#x27;&lt;UNK&gt;&#x27;</span>] = <span class="hljs-number">2</span><br>    word_index[<span class="hljs-string">&#x27;&lt;END&gt;&#x27;</span>] = <span class="hljs-number">3</span><br><br>    reverse_word_index = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word_index.items()&#125;<br>    <span class="hljs-comment"># print(decode_review(reverse_word_index, train_data[0]))</span><br><br>    <span class="hljs-comment"># 长度小于500的句子被补全, 大于500的句子被截断</span><br>    max_length = <span class="hljs-number">500</span><br>    train_data = keras.preprocessing.sequence.pad_sequences(<br>        train_data,        <span class="hljs-comment"># list of list</span><br>        value=word_index[<span class="hljs-string">&#x27;&lt;PAD&gt;&#x27;</span>],<br>        padding=<span class="hljs-string">&#x27;post&#x27;</span>,    <span class="hljs-comment"># padding 放在句子后面</span><br>        maxlen=max_length<br>    )<br><br>    test_data = keras.preprocessing.sequence.pad_sequences(<br>        test_data,<br>        value=word_index[<span class="hljs-string">&#x27;&lt;PAD&gt;&#x27;</span>],<br>        padding=<span class="hljs-string">&#x27;post&#x27;</span>,<br>        maxlen=max_length<br>    )<br><br>    embedding_dim = <span class="hljs-number">16</span>  <span class="hljs-comment"># 每个 word 都 embedding 为16维向量</span><br>    batch_size = <span class="hljs-number">128</span><br><br>    model = keras.models.Sequential([<br>        <span class="hljs-comment"># 把每个word转为16维向量, 此时一个句子就是一个 max_length * embedding_dim 的矩阵</span><br>        <span class="hljs-comment"># 有 batch_size 个矩阵, 所以输出是三维矩阵(batch_size, max_length, embedding_dim)</span><br>        keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),<br><br>        <span class="hljs-comment"># 通过pooling, 把三维矩阵(batch_size, max_length, embedding_dim)转为</span><br>        <span class="hljs-comment"># 二维矩阵(batch_size, embedding_dim)</span><br>        keras.layers.GlobalAveragePooling1D(),<br>        keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>),<br>        keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&quot;sigmoid&quot;</span>)     <span class="hljs-comment"># 二分类</span><br>    ])<br><br>    model.summary()<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;binary_crossentropy&quot;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>    history = model.fit(train_data, train_labels,<br>                        epochs=<span class="hljs-number">30</span>,<br>                        batch_size=batch_size,<br>                        validation_split=<span class="hljs-number">0.2</span>)<br><br>    plot_learning_curve(history, <span class="hljs-string">&quot;accuracy&quot;</span>, <span class="hljs-number">30</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>    plot_learning_curve(history, <span class="hljs-string">&quot;loss&quot;</span>, <span class="hljs-number">30</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<p><img src="/Blog/Blog/intro/tf/acc.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/tf/loss.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>这种合并 + padding有一些缺点:</p>
<ol>
<li>信息丢失<ul>
<li>多个embedding合并</li>
<li>Pad噪音, 无主次</li>
</ul>
</li>
<li>无效计算太多<ul>
<li>有太多padding</li>
</ul>
</li>
</ol>
<br>

<h2 id="序列式问题"><a href="#序列式问题" class="headerlink" title="序列式问题"></a>序列式问题</h2><p>普通的神经网络是一对一. </p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_1.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>而有时候我们需要一对多的输出, 例如通过图片生成描述</p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_2.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>多对一的输出: 文本分类, 或文本感情分析</p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_3.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>多对多: encoding-decoding, 机器翻译</p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_4.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>实时多对多</p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_5.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<br>

<p>什么是循环神经网络?</p>
<p>rnn每一步会维护一个状态作为下一步的额外输入</p>
<br>

<p>举个例子:</p>
<p>字符语言模型, 预测下一个字符</p>
<p>词典 [j, e, p]</p>
<p>样本 jeep</p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_6.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>第二次的输入就是第一次的状态和e, 第三次的输入是第二次的状态和e</p>
<br>

<p><img src="/Blog/Blog/intro/tf/rnn_s_7.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>序列式预测时, 我们不知道ground truth, 因此要把前一层的输出作为下一层的输入</p>
<p>而这种情况下, 如果开头预测错误, 那么后面整体就全错了. 这也是序列式问题的一个难点.</p>
<br>

<br>

<h2 id="使用循环神经网络完成二分类"><a href="#使用循环神经网络完成二分类" class="headerlink" title="使用循环神经网络完成二分类"></a>使用循环神经网络完成二分类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_learning_curve</span>(<span class="hljs-params">history, label, epochs, min_values, max_values</span>):<br>    data = &#123;&#125;<br>    data[label] = history.history[label]<br>    data[<span class="hljs-string">&#x27;val_&#x27;</span>+label] = history.history[<span class="hljs-string">&#x27;val_&#x27;</span>+label]<br>    pd.DataFrame(data).plot(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>))<br>    plt.grid(<span class="hljs-literal">True</span>)<br>    plt.axis([<span class="hljs-number">0</span>, epochs, min_values, max_values])<br>    plt.show()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode_review</span>(<span class="hljs-params">reverse_word_index, text_ids</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27; &#x27;</span>.join([reverse_word_index.get(word_id, <span class="hljs-string">&quot;&lt;UNK&gt;&quot;</span>) <span class="hljs-keyword">for</span> word_id <span class="hljs-keyword">in</span> text_ids])<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    imdb = keras.datasets.imdb<br>    vocab_size = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 只处理10000个不同的字符, 其余的字符会变为 unk</span><br>    index_from = <span class="hljs-number">3</span><br>    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, index_from=index_from)<br>    <span class="hljs-built_in">print</span>(train_data.shape, train_labels.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(train_data[<span class="hljs-number">0</span>]), <span class="hljs-built_in">len</span>(train_data[<span class="hljs-number">1</span>]))  <span class="hljs-comment"># 样本是变长的</span><br>    <span class="hljs-built_in">print</span>(train_data[<span class="hljs-number">0</span>])<br><br>    word_index = imdb.get_word_index()<br>    <span class="hljs-comment"># index 改为从4开始, 这样流出4个位置放置特殊字符</span><br>    word_index = &#123;k: (v + <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word_index.items()&#125;<br><br>    <span class="hljs-comment"># 特殊字符</span><br>    word_index[<span class="hljs-string">&#x27;&lt;PAD&gt;&#x27;</span>] = <span class="hljs-number">0</span><br>    word_index[<span class="hljs-string">&#x27;&lt;START&gt;&#x27;</span>] = <span class="hljs-number">1</span><br>    word_index[<span class="hljs-string">&#x27;&lt;UNK&gt;&#x27;</span>] = <span class="hljs-number">2</span><br>    word_index[<span class="hljs-string">&#x27;&lt;END&gt;&#x27;</span>] = <span class="hljs-number">3</span><br><br>    reverse_word_index = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word_index.items()&#125;<br>    <span class="hljs-comment"># print(decode_review(reverse_word_index, train_data[0]))</span><br><br>    <span class="hljs-comment"># 长度小于500的句子被补全, 大于500的句子被截断</span><br>    max_length = <span class="hljs-number">500</span><br>    train_data = keras.preprocessing.sequence.pad_sequences(<br>        train_data,        <span class="hljs-comment"># list of list</span><br>        value=word_index[<span class="hljs-string">&#x27;&lt;PAD&gt;&#x27;</span>],<br>        padding=<span class="hljs-string">&#x27;post&#x27;</span>,    <span class="hljs-comment"># padding 放在句子后面</span><br>        maxlen=max_length<br>    )<br><br>    test_data = keras.preprocessing.sequence.pad_sequences(<br>        test_data,<br>        value=word_index[<span class="hljs-string">&#x27;&lt;PAD&gt;&#x27;</span>],<br>        padding=<span class="hljs-string">&#x27;post&#x27;</span>,<br>        maxlen=max_length<br>    )<br><br>    embedding_dim = <span class="hljs-number">16</span>  <span class="hljs-comment"># 每个 word 都 embedding 为16维向量</span><br>    batch_size = <span class="hljs-number">128</span><br><br>    <span class="hljs-comment"># 模型依然需要 embedding</span><br>    model = keras.models.Sequential([<br>        keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),<br>        <span class="hljs-comment"># 双向RNN结构, 只使用最后的输出</span><br>        keras.layers.Bidirectional(keras.layers.SimpleRNN(units=<span class="hljs-number">32</span>, return_sequences=<span class="hljs-literal">False</span>)),   <br>        keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>),<br>        keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">&quot;sigmoid&quot;</span>)<br>    ])<br><br>    model.summary()<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;binary_crossentropy&quot;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>    history = model.fit(train_data, train_labels,<br>                        epochs=<span class="hljs-number">30</span>,<br>                        batch_size=batch_size,<br>                        validation_split=<span class="hljs-number">0.2</span>)<br><br>    plot_learning_curve(history, <span class="hljs-string">&quot;accuracy&quot;</span>, <span class="hljs-number">30</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>    plot_learning_curve(history, <span class="hljs-string">&quot;loss&quot;</span>, <span class="hljs-number">30</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<p>和上一个例子相比, 只对模型部分进行更改</p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_8.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p><img src="/Blog/Blog/intro/tf/rnn_s_9.PNG" srcset="/Blog/img/loading.gif" lazyload></p>
<p>可以看出有明显的过拟合</p>
<p>然而如果使用多层RNN, 多层双向RNN, 则过拟合倾向更为严重.</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/Blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/Blog/tags/python3/">python3</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/Blog/2021/01/02/taylor/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">泰勒 Taylor</span>
                        <span class="visible-mobile">Antaŭa afiŝo</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/Blog/2020/12/20/fast-and-sift/">
                        <span class="hidden-mobile">FAST and SIFT</span>
                        <span class="visible-mobile">Sekva afiŝo</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Enhavtabelo</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Serĉi</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">ŝlosivorto</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/Blog/js/events.js" ></script>
<script  src="/Blog/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/Blog/js/local-search.js" ></script>



  
    <script  src="/Blog/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/Blog/js/boot.js" ></script>


</body>
</html>
