

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/Blog/img/favicon.ico">
  <link rel="icon" href="/Blog/img/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Daolin">
  <meta name="keywords" content="">
  
    <meta name="description" content="PCA算法: 降维, 降噪">
<meta property="og:type" content="article">
<meta property="og:title" content="python3入门机器学习(5)-PCA与梯度上升法">
<meta property="og:url" content="https://daolinzhou.github.io/Blog/2020/04/28/machine-learning-5/">
<meta property="og:site_name" content="Daolin&#39;s Repository">
<meta property="og:description" content="PCA算法: 降维, 降噪">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/pca_p_4.PNG">
<meta property="article:published_time" content="2020-04-28T22:27:29.000Z">
<meta property="article:modified_time" content="2020-05-24T00:58:32.716Z">
<meta property="article:author" content="Daolin">
<meta property="article:tag" content="python3">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daolinzhou.github.io/Blog/intro/pca_p_4.PNG">
  
  
  <title>python3入门机器学习(5)-PCA与梯度上升法 - Daolin&#39;s Repository</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/Blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"daolinzhou.github.io","root":"/Blog/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/Blog/local-search.xml"};
  </script>
  <script  src="/Blog/js/utils.js" ></script>
  <script  src="/Blog/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/Blog/">
      <strong>Daolin&#39;s Repo</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/Blog/intro/longyin.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="python3入门机器学习(5)-PCA与梯度上升法">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-04-28 15:27" pubdate>
        2020年4月28日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      19k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      163 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">python3入门机器学习(5)-PCA与梯度上升法</h1>
            
            <div class="markdown-body">
              <link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><p>PCA算法: 降维, 降噪</p>
<span id="more"></span>
<h1 id="主成分分析-Principal-Component-Analysis"><a href="#主成分分析-Principal-Component-Analysis" class="headerlink" title="主成分分析(Principal Component Analysis)"></a>主成分分析(Principal Component Analysis)</h1><p>不仅仅应用在机器学习领域, 也是统计学领域非常重要的方法(数理统计) </p>
<ul>
<li>一个非监督的机器学习算法</li>
<li>主要用于数据的降维</li>
<li>通过降维, 可以发现更便于人类理解的特征</li>
<li>其他应用: 可视化, 去噪</li>
</ul>
<p><br></p>
<h2 id="思路推导"><a href="#思路推导" class="headerlink" title="思路推导"></a>思路推导</h2><p><img src="/Blog/intro/PCA_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>横轴和纵轴代表两个特征, 每一个样本对应平面上的一个点</p>
<p>这是一个二维的特征样本, 如果为它降维的话显然可以降到一维</p>
<p>如何降到一维?</p>
<p>一个显而易见的方法就是将两个特征中的一个给丢掉, 把数据映射到x轴或y轴</p>
<p><img src="/Blog/intro/PCA_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>那么那个方案是更好的降维方案? 右边的</p>
<p>因为点和点之间的距离是相对比较大的, 就是说点和点之间有更高的可区分度. 同时它们之间的距离比较大也更高的保持了原来点和点之间的距离</p>
<p>因此如果选一种方案进行降维, 显然应该选择右边的方案</p>
<p>但是这种方案是最好的方案吗? 有没有比这种方案更好的方案?</p>
<p><br></p>
<p><img src="/Blog/intro/PCA_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>原来的样本是在二维平面, 现在要把它降维到一维平面, 是不是有一条直线, 这条直线是斜着的</p>
<p>数据点就变成这个样子</p>
<p><img src="/Blog/intro/PCA_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这个样子就感觉整体和原来样本的分布并没有更大的差距, 于此同时所有的点都在一个轴上(尽管轴是斜着的)</p>
<p>使用这种方式, 所有点更加趋近于原来的情况, 点和点之间的距离比之前的两种方式都大, 区分度都更高</p>
<p>问题就是: <strong>如何找到这个让样本间间距最大的轴</strong>?</p>
<p><br></p>
<p>为了找到这个轴, 首先要<strong>定义样本间的间距</strong></p>
<p>使用什么样的指标来表示样本间的间距? </p>
<p>事实上, 在统计学有一个指标可以直接来定义样本间的间距: <strong>方差(Variance)</strong></p>
<p>方差是<strong>描述样本整体分布的疏密的指标</strong>, 方差越大, 代表样本越稀疏. 方差越小, 代表样本越紧密</p>
<script type="math/tex; mode=display">
Var(x) = \frac 1m \sum_{i=1}^m(x_i-\bar x)^2</script><p><img src="/Blog/intro/PCA_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>问题就变成了找到一个轴, 使得样本空间所有点映射到这个轴后方差最大</p>
<p><br></p>
<p>第一步: 为了解决这个问题, 在进行主成分分析之前, 首先要将所有样本的均值归0 (通常叫做demean)</p>
<p>所谓的均值归0, 就是样本的所有元素都减去均值</p>
<p>这样就把样本变成了这个样子</p>
<p><img src="/Blog/intro/PCA_6.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>样本的分布是没有改变的, 只是把坐标轴进行了移动, 使得样本在每一个维度的均值都是0</p>
<p>再看一眼方差的公式</p>
<script type="math/tex; mode=display">
Var(x) = \frac 1m \sum_{i=1}^m(x_i-\bar x)^2</script><p>现在样本的均值已经为0了, 式子就可以化简为</p>
<script type="math/tex; mode=display">
Var(x) = \frac 1m \sum_{i=1}^m x_i^2</script><p>注意: 因为希望是<strong>映射后方差最大</strong>, 所以x<sub>i</sub>是所有的样本点已经映射到新的坐标轴上之后, 得到的新的样本</p>
<p>由于这个轴在图示中只有两个维度, 所以就可以叫做(w1, w2)</p>
<p><img src="/Blog/intro/PCA_7.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>第一步就是对所有样本进行demean处理</p>
<p>之后想求一个轴的方向 w=(w1, w2) (二维空间中)</p>
<p>使得所有的样本映射到w以后, 使得下述公式最大</p>
<p> <img src="/Blog/intro/PCA_8.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于X, 可能有n个维度, 所以是包含有n个数的向量, 所以更为准确地说, 应该写成这样的形式</p>
<p><img src="/Blog/intro/PCA_9.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>应该是向量的模的平方.</p>
<p>而且由于进行过demean处理所以X的均值的投影就为0</p>
<p>(X的均值也是一个向量, 是一个点, 而通过demean处理, X的均值为0, 就是说X的均值在原点上, 而我们要找的那个低维空间肯定是过原点的, 也就是说X的均值的投影还是在原点)</p>
<p> 所以最终我们要最大化这样一个式子</p>
<p><img src="/Blog/intro/PCA_10.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>就是把每个样本都映射到w这个轴上之后, 新的样本点的模的平方和最大</p>
</blockquote>
<p><br></p>
<p>如何最大化这个式子</p>
<p><img src="/Blog/intro/PCA_10.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>主要处理一下X<sup>(i)</sup><sub>project</sub> , 如何用原来的X<sup>(i)</sup>来代表它</p>
<p>其实就是正交映射</p>
<script type="math/tex; mode=display">
X_{project}^{(i)}=\frac {\vec w\cdot X^{(i)}} {\vec w\cdot\vec w}\cdot \vec w</script><script type="math/tex; mode=display">
\frac {\vec w\cdot X^{(i)}} {\vec w\cdot\vec w}</script><p>而这个式子将得到一个数, 这个数代表把w伸缩多少倍才能得到投影</p>
<p>而如果w的模为1, 即w是标准基的话(w只表示方向), 那么距离(模)就表示为: </p>
<script type="math/tex; mode=display">
||X_{project}^{(i)}|| = \frac {\vec w\cdot X^{(i)}} {\vec w\cdot\vec w}</script><p>而如果w的模为1, 且分子就是w的模的平方, 因此就可以化简为</p>
<script type="math/tex; mode=display">
||X_{project}^{(i)}||=\vec w\cdot X^{(i)}</script><blockquote>
<p>也可以使用点乘的定义推导</p>
<p><img src="/Blog/intro/PCA_11.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
</blockquote>
<p>此时我们的目标就变成:</p>
<script type="math/tex; mode=display">
Var(X_{project})=\frac 1 m\sum_{i=1}^m( X^{(i)} \cdot \vec w )^2</script><script type="math/tex; mode=display">
\ \ \ \ \ \ \ \ \ \ \ \ =\frac 1 m ||X\cdot \vec w||^2</script><p>求方向向量w(模为1), 让这个式子最大</p>
<p>而把点乘展开以后就是这样的式子:</p>
<script type="math/tex; mode=display">
Var(X_{project})=\frac 1 m\sum_{i=1}^m(X_1^{(i)}w_1+X_2^{(i)}w_2+\cdots+X_n^{(i)}w_n)^2</script><p>就是X<sup>(i)</sup>的每一个维度和w的每一个维度相乘再相加</p>
<p>现在主成分分析法就变成了一个<strong>求目标函数的最优化问题, 使用梯度上升法解决</strong></p>
<blockquote>
<p>当然, 对于这个问题也可以直接用数学的方法求解出公式, 事实上, 主成分分析法背后有非常强的数学原理作为支撑</p>
<p>主要关注用搜索的策略来求主成分分析法</p>
</blockquote>
<p><br></p>
<p class="note note-primary">和线性回归的不同</p>

<blockquote>
<p>注意: 和线性回归不同, 线性回归中横轴是特征, 纵轴是输出标记, 而PCA算法中, 横纵轴都是特征</p>
<p>PCA:</p>
<p><img src="/Blog/intro/PCA_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>线性回归:</p>
<p><img src="/Blog/intro/PCA_12.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这些线是垂直于x轴, 而不是垂直与线的</p>
</blockquote>
<p><br></p>
<p><br></p>
<h2 id="梯度上升法求PCA"><a href="#梯度上升法求PCA" class="headerlink" title="梯度上升法求PCA"></a>梯度上升法求PCA</h2><p><img src="/Blog/intro/PCA_13.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这里用 f(x) 而不是 J(x), 因为一般情况下J是用来描述损失函数的</p>
<p>如果想要用梯度上升法, 关键就是求f(x)的梯度</p>
<blockquote>
<p>注意X已知, 是<strong>非监督学习</strong>(没有y)提供的样本信息, 而w才是未知数</p>
</blockquote>
<p><img src="/Blog/intro/PCA_14.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/PCA_15.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/PCA_16.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>由于得到的是一个行向量, 所以对整体进行转置来得到列向量</p>
<script type="math/tex; mode=display">
=\frac 2m X^TXw</script><p><img src="/Blog/intro/PCA_17.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">demean</span>(<span class="hljs-params">X</span>):<br>    <span class="hljs-keyword">return</span> X - np.mean(X, axis=<span class="hljs-number">0</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">w, X</span>):<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(X.dot(w)**<span class="hljs-number">2</span>) / <span class="hljs-built_in">len</span>(X)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">df_math</span>(<span class="hljs-params">w, X</span>):<br>    <span class="hljs-keyword">return</span> X.T.dot(X).dot(w)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">df_debug</span>(<span class="hljs-params">w, X, epsilon=<span class="hljs-number">0.0001</span></span>):<br>    res = np.empty(<span class="hljs-built_in">len</span>(w))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(w)):<br>        w_1 = w.copy()<br>        w_2 = w.copy()<br>        w_1[i] += epsilon<br>        w_2[i] -= epsilon<br>        res[i] = (f(w_1, X) - f(w_2, X)) / (<span class="hljs-number">2</span>*epsilon)<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">direction</span>(<span class="hljs-params">w</span>):<br>    <span class="hljs-keyword">return</span> w / np.linalg.norm(w)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_ascent</span>(<span class="hljs-params">df, X, w, eta, n_iters=<span class="hljs-built_in">int</span>(<span class="hljs-params"><span class="hljs-number">1e4</span></span>), epsilon=<span class="hljs-number">1e-8</span></span>):<br><br>    w = direction(w)<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_iters):<br><br>        gradient = df(w, X)<br>        last_w = w<br>        w = w + eta * gradient<br>        w = direction(w)    <span class="hljs-comment"># 注意1: 每次求一个单位方向</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(f(w, X) - f(last_w, X)) &lt; epsilon:<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> w<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    X = np.empty((<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>    <span class="hljs-comment"># [0, 100)之间随机分布的100个样本</span><br>    X[:, <span class="hljs-number">0</span>] = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, size=<span class="hljs-number">100</span>)<br>    X[:, <span class="hljs-number">1</span>] = <span class="hljs-number">0.75</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3.</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)<br><br>    <span class="hljs-comment"># demean</span><br>    X_demean = demean(X)<br><br>    <span class="hljs-comment"># 梯度上升法</span><br>    <span class="hljs-comment"># 初始化的值不能是零向量, w=0本身就是极值点(最小值), 梯度为0</span><br>    initial_w = np.random.random(X.shape[<span class="hljs-number">1</span>])    <span class="hljs-comment"># 注意2: 不能从0向量开始</span><br>    eta = <span class="hljs-number">0.001</span><br><br>    <span class="hljs-comment"># 注意3: 不能只用StandardScaler标准化数据</span><br>    <span class="hljs-comment"># 我们希望找到一个轴, 使得方差最大</span><br>    <span class="hljs-comment"># 但一旦归一化了, 方差就为1, 就不存在最大的方差了</span><br><br>    res = gradient_ascent(df_debug, X_demean, initial_w, eta)<br>    <span class="hljs-built_in">print</span>(res)<br>    res = gradient_ascent(df_math, X_demean, initial_w, eta)<br>    <span class="hljs-built_in">print</span>(res)<br></code></pre></td></tr></table></figure>
<p>得到的主成分是我们第一个得到的, 所以也叫第一主成分</p>
<p><img src="/Blog/intro/PCA_18.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>如果生成的数据都在一条直线上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">X2 = np.empty((<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>X2[:,<span class="hljs-number">0</span>] = np.random.uniform(<span class="hljs-number">0.</span>, <span class="hljs-number">100.</span>, size=<span class="hljs-number">100</span>)<br>X2[:,<span class="hljs-number">1</span>] = <span class="hljs-number">0.75</span> * X2[:,<span class="hljs-number">0</span>] + <span class="hljs-number">3.</span><br><br>plt.scatter(X2[:,<span class="hljs-number">0</span>], X2[:,<span class="hljs-number">1</span>])<br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/PCA_19.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">X2_demean = demean(X2)<br>w2 = gradient_ascent(df_math, X2_demean, initial_w, eta)<br><br>plt.scatter(X2_demean[:,<span class="hljs-number">0</span>], X2_demean[:,<span class="hljs-number">1</span>])<br>plt.plot([<span class="hljs-number">0</span>, w2[<span class="hljs-number">0</span>]*<span class="hljs-number">30</span>], [<span class="hljs-number">0</span>, w2[<span class="hljs-number">1</span>]*<span class="hljs-number">30</span>], color=<span class="hljs-string">&#x27;r&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/PCA_20.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p class="note note-danger">
三个注意事项
<ol>
    <li>把方向向量的模归一</li>
    <li>不能从零向量开始梯度上升</li>
    <li>不可以对数据进行归一化</li>
</ol></p>

<p><br></p>
<p><br></p>
<h2 id="求数据的前n个主成分"><a href="#求数据的前n个主成分" class="headerlink" title="求数据的前n个主成分"></a>求数据的前n个主成分</h2><p><img src="/Blog/intro/PCA_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>将样本点都映射到这个轴上, 这个轴就是第一主成分</p>
<p>得到的这些样本点的方差是最大的</p>
<p><img src="/Blog/intro/PCA_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>不过样本点本身都是二维的坐标点, 将它映射到轴上之后并没有变成一维的数据</p>
<p>只不过将这些坐标点相应的一个维度从原先的这样两个轴其中的一个轴变成这样斜着的轴</p>
<p>相应由于它还是一个二维的数据, 它还应该有另外一个轴(对二维数据来说)</p>
<p>如果对于一个n维数据来说, 对应的还应该有n个轴, 只不过这n个轴通过主成分分析法重新进行了排列, 使得第一个轴保持样本相应的方差是最大的, 第二个轴次之, 第三个轴再次之</p>
<p>换言之, 主成分分析法本质是<strong>从一组坐标系, 转移到另外一组坐标系</strong> 这样一个过程</p>
<p>之前只求出第一个轴所在的方向, 下一个问题就是如何求出下一个轴所在的方向?</p>
<p><br></p>
<p>也就是说, 在求出第一主成分以后, 如何求出下一个主成分?</p>
<p>由于已经求出第一主成分了, 所以可以将数据进行改变, 将数据在第一个主成分上的分量去掉</p>
<script type="math/tex; mode=display">
X_{project}^{(i)}=\frac {\vec w\cdot X^{(i)}} {\vec w\cdot\vec w}\cdot \vec w</script><p>由于w的模为1, 所以分子为1</p>
<script type="math/tex; mode=display">
X_{project}^{(i)}={\vec w\cdot X^{(i)}}\cdot \vec w=||X_{project}^{(i)}||\cdot \vec w</script><p><img src="/Blog/intro/PCA_21.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>那么如何把 X 这个样本在 X_proj 上的分量去掉: 非常简单</p>
<script type="math/tex; mode=display">
X^{(i)}-X_{proj}^{(i)}</script><p>就是把X映射到与X_proj相垂直的轴上</p>
<p>这个轴就是数据去除掉在第一主成分上的分量相应最终得到的结果</p>
<p><img src="/Blog/intro/PCA_22.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>X’ 是 X 中所有的样本都去除了在第一主成分上的分量, 最终的结果</p>
<p>现在想要求第二主成分就非常简单了</p>
<p>只要在新的数据上(X’) 重新求第一主成分, 此时在X’上求出的第一主成分就是原来数据的第二主成分</p>
<p>以此类推就可以求出第三主成分, 第四主成分</p>
<p><br></p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">demean</span>(<span class="hljs-params">X</span>):<br>    <span class="hljs-keyword">return</span> X - np.mean(X, axis=<span class="hljs-number">0</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">w, X</span>):<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(X.dot(w)**<span class="hljs-number">2</span>) / <span class="hljs-built_in">len</span>(X)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">df_math</span>(<span class="hljs-params">w, X</span>):<br>    <span class="hljs-keyword">return</span> X.T.dot(X).dot(w)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">df_debug</span>(<span class="hljs-params">w, X, epsilon=<span class="hljs-number">0.0001</span></span>):<br>    res = np.empty(<span class="hljs-built_in">len</span>(w))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(w)):<br>        w_1 = w.copy()<br>        w_2 = w.copy()<br>        w_1[i] += epsilon<br>        w_2[i] -= epsilon<br>        res[i] = (f(w_1, X) - f(w_2, X)) / (<span class="hljs-number">2</span>*epsilon)<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">direction</span>(<span class="hljs-params">w</span>):<br>    <span class="hljs-keyword">return</span> w / np.linalg.norm(w)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">first_component</span>(<span class="hljs-params">X, w, eta, n_iters=<span class="hljs-built_in">int</span>(<span class="hljs-params"><span class="hljs-number">1e4</span></span>), epsilon=<span class="hljs-number">1e-8</span></span>):<br><br>    w = direction(w)<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_iters):<br><br>        gradient = df_math(w, X)<br>        last_w = w<br>        w = w + eta * gradient<br>        w = direction(w)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(f(w, X) - f(last_w, X)) &lt; epsilon:<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> w<br><br><br><span class="hljs-comment"># 获取前n个主成分</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">first_n_components</span>(<span class="hljs-params">n, X, eta=<span class="hljs-number">0.01</span>, n_iters=<span class="hljs-built_in">int</span>(<span class="hljs-params"><span class="hljs-number">1e4</span></span>), epsilon=<span class="hljs-number">1e-8</span></span>):<br>    X_pca = X.copy()<br>    X_pca = demean(X_pca)<br><br>    res = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        <span class="hljs-comment"># 得到第一主成分</span><br>        initial_w = np.random.random(X_pca.shape[<span class="hljs-number">1</span>])<br>        w = first_component(X_pca, initial_w, eta)<br>        res.append(w)<br><br>        <span class="hljs-comment"># 把第一主成分从原本的样本中去除</span><br>        X_pca = X_pca - X_pca.dot(w).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) * w<br><br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    X = np.empty((<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>    <span class="hljs-comment"># [0, 100)之间随机分布的100个样本</span><br>    X[:, <span class="hljs-number">0</span>] = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, size=<span class="hljs-number">100</span>)<br>    X[:, <span class="hljs-number">1</span>] = <span class="hljs-number">0.75</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3.</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)<br><br>    res = first_n_components(<span class="hljs-number">2</span>, X)<br>    <span class="hljs-built_in">print</span>(res)<br></code></pre></td></tr></table></figure>
<p>绿色是第一主成分的方向放大30倍的线</p>
<p>红色是第二主成分的方向放大30倍的线</p>
<p>二者垂直</p>
<p><img src="/Blog/intro/PCA_24.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>把数据映射到第一主成分上</p>
<p><img src="/Blog/intro/PCA_19.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>映射到第二主成分上</p>
<p><img src="/Blog/intro/PCA_23.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>但PCA的主要作用是用于降维, 现在为止, 只不过是将原本所处的坐标系转换到了一个新的坐标系中, 如何把原来的样本数据映射到低维空间?</p>
<p><br></p>
<h2 id="高维数据向低维数据映射"><a href="#高维数据向低维数据映射" class="headerlink" title="高维数据向低维数据映射"></a>高维数据向低维数据映射</h2><p>虽然求出来主成分所代表的方向, 但是数据集本身依然是n维的, 并没有进行降维</p>
<p>那么如何具体用PCA进行降维?</p>
<p><br></p>
<p><img src="/Blog/intro/PCA_25.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于本身样本数据X来说有m行n列, 代表有m个样本, n个特征</p>
<p>通过主成分分析法, 假设已经求出来对于这个数据集来说, 前k个主成分, 每一个主成分是一个单位方向, 用w来表示</p>
<p><img src="/Blog/intro/PCA_26.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>它有k行代表求出来的前k个主成分, 每一行有n个元素, 代表每一个主成分的坐标轴是有n个元素的</p>
<p>毕竟主成分分析法的本质是<strong>从一个坐标系转换到另外一个坐标系</strong></p>
<p>原来的坐标系有n个维度的话, 转换后的坐标系也有n个维度</p>
<p>一个样本和一个w (W的某一行) 进行点乘其实是将这个样本映射到w这个轴上得到的模(大小)</p>
<script type="math/tex; mode=display">
||X_{project}^{(i)}||=X^{(i)}\cdot \vec w</script><p>而如果将这一个样本和这k个w分别作点乘的话, 得到的就是这一个样本在k个方向上相应的每一个方向上的大小</p>
<p>这k个元素合在一起就能表示这一个样本映射到k个轴所代表的坐标系上相应的样本的大小</p>
<p>X<sup>(1)</sup>乘以W<sup>(1)</sup>, … W<sup>(k)</sup>, 得到k个数, 这k个数组成的向量就是样本1映射到W<sub>k</sub>这个坐标系上的k维的向量</p>
<p>由于k &lt; n 因此就完成了一个样本从n维向k维的映射</p>
<p>其实整体就是做了一个矩阵的乘法</p>
<p><img src="/Blog/intro/PCA_25.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/PCA_26.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<script type="math/tex; mode=display">
X\cdot W_k^T</script><p>如果想不明白就看一下矩阵的维度</p>
<p><img src="/Blog/intro/PCA_27.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>X是m乘n维的矩阵, W<sub>k</sub>的转置是n乘k维的矩阵, 得到的结果是m乘k维的矩阵, 就是说有m个样本, 每个样本有k个维度</p>
<blockquote>
<p>个人从数学的角度推理:</p>
<p>假设有X和W, X的每一行是一个样本, 而W的每一行是一个方向向量</p>
<p><img src="/Blog/intro/PCA_25.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/PCA_26.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>在线性代数中, 对于两个坐标系来说</p>
<p><img src="/Blog/intro/trans_coord_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>X的每一行都是一个样本, 换句话说是坐标,</p>
<p>X的转置的每一列就是一个坐标, 且这些坐标的基是标准正交基, 因此</p>
<script type="math/tex; mode=display">
I\cdot X^T = W_n^T\cdot V</script><p>假设求出了所有的主成分, W_n就是一个n乘n的方阵, W_n的每一行都是一个方向向量, 然而作为坐标转换矩阵, 每一列应该是一个方向向量, 所以取W_n的转置</p>
<p>V是我们要求的经过坐标转换后的坐标, 因此:</p>
<script type="math/tex; mode=display">
(W_n^T)^{-1}\cdot X^T = V</script><p>又因为W_n是一个标准正交矩阵, 每一列是一个标准正交基, 所以W_n的逆就是W_n的转置</p>
<script type="math/tex; mode=display">
W_n\cdot X^T = V</script><p>得到的V就是一个n乘m的矩阵, V的每一列都对应了一个转换坐标后的样本, 同时对于V第i列来说, 就是这样得来的</p>
<script type="math/tex; mode=display">
W_n\cdot X_i^T = V_i</script><p>V_i的第一个元素是样本映射到W_1(第1个主成分, 与W的第一行点乘)的模(长度)</p>
<p>V_i的第二个元素是样本映射到W_2(第2个主成分, 与W的第二行点乘)的模(长度)</p>
<p>然而现在我们想要降维, 也就是说只取前k个主成分, 自然就是把W_n降到W_k, 也就是说原本n乘n的方阵, 最后的n-k行(最后n-k个主成分)不要了只保留前k个主成分, 就是k行</p>
<script type="math/tex; mode=display">
W_k\cdot X^T = V</script><p>此时的V就是我们降维好的X^T, 而V是列向量, 如果把它转化成行向量就是</p>
<script type="math/tex; mode=display">
X_k=V^T = X \cdot W_k^T</script><p>得到了和之前一样的结果</p>
</blockquote>
<p><br></p>
<p><img src="/Blog/intro/PCA_28.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/PCA_26.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>一旦获得了X<sub>k</sub>, m个k维数据组成的矩阵</p>
<p>对于W<sub>k</sub>来说, 我们依然可以反过来给它恢复成原来n维的数据</p>
<p>其实现在X<sub>k</sub>每行数据有k个元素, 这k个元素和W<sub>k</sub>的每一列作乘法</p>
<p>此时把X<sub>k</sub>每行映射到W<sub>k</sub>的每一列对应的方向中, 一共有n列, 最终就又回复成m乘n的矩阵</p>
<blockquote>
<p>数学的角度: 上面已经推导出</p>
<script type="math/tex; mode=display">
X_k = X \cdot W_k^T</script><script type="math/tex; mode=display">
X_k\cdot W_k = X \cdot W_k^T \cdot W_k</script><p>由于W<sub>k</sub><sup>T</sup> 的列向量两两正交, 且模为1, 所以</p>
<script type="math/tex; mode=display">
W_k^T \cdot W_k</script><p>得到的是一个<strong>不是</strong>单位矩阵的矩阵</p>
<p>而这个矩阵的矩阵乘以X肯定得到的不是X, 暂时称为X’</p>
<script type="math/tex; mode=display">
X_k\cdot W_k = X'</script><p>其实由于W<sub>k</sub>不是方阵, 且W<sub>k</sub><sup>T</sup> 的列向量两两正交, 所以W<sub>k</sub>是W<sub>k</sub>的转置的伪逆</p>
<p>满足:</p>
<script type="math/tex; mode=display">
W_k^T \cdot W_k \cdot W_k^T = W_k^T</script><script type="math/tex; mode=display">
W_k \cdot W_k^T \cdot W_k = W_k</script><p><br></p>
<p>而如果k=n, 就是说W<sub>k</sub>是一个标准正交矩阵, 说明它可逆, 则</p>
<script type="math/tex; mode=display">
X_k\cdot W_k = X</script><p>此时X’ 就是 X, 没有任何损失</p>
</blockquote>
<p>当然, 恢复回来的结果已经不是原来的结果了, 因为在降维的过程中丢失了一些信息, 即使恢复回来, 丢失的这些信息也是恢复不回来的, 但是反向的操作在数学的角度是成立的</p>
<p><img src="/Blog/intro/PCA_29.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>X<sub>m</sub>和原来的X是不一样的</p>
<p><br></p>
<p><br></p>
<p>编程实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PCA</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_components</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;初始化PCA&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> n_components &gt;= <span class="hljs-number">1</span>, <span class="hljs-string">&quot;n_components must be valid&quot;</span><br>        self.n_components = n_components<br>        self.components_ = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, X, eta=<span class="hljs-number">0.01</span>, n_iters=<span class="hljs-built_in">int</span>(<span class="hljs-params"><span class="hljs-number">1e4</span></span>)</span>):<br>        <span class="hljs-keyword">assert</span> self.n_components &lt;= X.shape[<span class="hljs-number">1</span>], \<br>            <span class="hljs-string">&quot;n_components must not be greater than feature number of X&quot;</span><br><br>        <span class="hljs-comment"># 均值归0</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">demean</span>(<span class="hljs-params">X</span>):<br>            <span class="hljs-keyword">return</span> X - np.mean(X, axis=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 目标函数</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">w, X</span>):<br>            <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(X.dot(w)**<span class="hljs-number">2</span>) / <span class="hljs-built_in">len</span>(X)<br>		<br>        <span class="hljs-comment"># 梯度</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">df</span>(<span class="hljs-params">w, X</span>):<br>            <span class="hljs-keyword">return</span> X.T.dot(X.dot(w)) * <span class="hljs-number">2.</span> / <span class="hljs-built_in">len</span>(X)<br>		<br>        <span class="hljs-comment"># 向量归一化</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">direction</span>(<span class="hljs-params">w</span>):<br>            <span class="hljs-keyword">return</span> w / np.linalg.norm(w)<br>	<br>    	<span class="hljs-comment"># 求第一主成分</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">first_component</span>(<span class="hljs-params">X, initial_w, eta=<span class="hljs-number">0.01</span>, n_iters=<span class="hljs-built_in">int</span>(<span class="hljs-params"><span class="hljs-number">1e4</span></span>), epsilon=<span class="hljs-number">1e-8</span></span>):<br>            w = direction(initial_w)<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_iters):<br>                gradient = df(w, X)<br>                last_w = w<br>                w = w + eta * gradient<br>                w = direction(w)<br><br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(f(w, X) - f(last_w, X)) &lt; epsilon:<br>                    <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">return</span> w<br><br>        X_pca = demean(X)<br>        initial_w = np.random.random(X.shape[<span class="hljs-number">1</span>])<br>        self.components_ = np.empty(shape=(self.n_components, X.shape[<span class="hljs-number">1</span>]))<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.n_components):<br>            w = first_component(X_pca, initial_w, eta, n_iters)<br>            self.components_[i, :] = w<br>            <span class="hljs-comment"># self.components_[i] = w</span><br><br>            X_pca = X_pca - X_pca.dot(w).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) * w<br><br>        <span class="hljs-keyword">return</span> self<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">transform</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;将给定X映射到各个主成分分量中&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> X.shape[<span class="hljs-number">1</span>] == self.components_.shape[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">return</span> X.dot(self.components_.T)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inverse_transform</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">assert</span> X.shape[<span class="hljs-number">1</span>] == self.components_.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> X.dot(self.components_)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;PCA(n_components=%d)&quot;</span> % self.n_components<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    X = np.empty((<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>    X[:, <span class="hljs-number">0</span>] = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, size=<span class="hljs-number">100</span>)<br>    X[:, <span class="hljs-number">1</span>] = <span class="hljs-number">0.75</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, size=<span class="hljs-number">100</span>)<br><br>    pca = PCA(n_components=<span class="hljs-number">2</span>)<br>    pca.fit(X)<br>    <span class="hljs-built_in">print</span>(pca.components_)<br><br>    pca = PCA(n_components=<span class="hljs-number">1</span>)<br>    pca.fit(X)<br>    <span class="hljs-built_in">print</span>(pca.components_)<br>    <br>    <span class="hljs-comment"># 降维</span><br>    X_reduction = pca.transform(X)<br>    <span class="hljs-comment"># 恢复</span><br>    X_restore = pca.inverse_transform(X_reduction)<br><br>    <span class="hljs-comment"># 蓝色点是原始数据</span><br>    plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;b&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>    <span class="hljs-comment"># 红色点是降维后有恢复成高维数据的点</span><br>    plt.scatter(X_restore[:, <span class="hljs-number">0</span>], X_restore[:, <span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;r&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/pca_p_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><span style="color:blue;">蓝色</span>点是原始数据, <span style="color:red;">红色</span>点是降维后有恢复成高维数据的点</p>
<p>可以看到inverse_trasform的过程中是丢失信息的</p>
<p>失去的信息并不能restore回来, restore所做的只不过是<strong>在高维的空间中表达这些低维的样本</strong></p>
<blockquote>
<p>PCA首先做的事情是寻找另外一个坐标系, 这个坐标系中, 每一个轴依次可以表达原来样本的重要程度, 也就是所谓的所有的主成分</p>
<p>取出前k个最重要的主成分, 就可以把样本映射到这k个轴上, 获得一个低维的数据信息</p>
</blockquote>
<p> <br></p>
<h2 id="scikit-learn中的PCA的使用"><a href="#scikit-learn中的PCA的使用" class="headerlink" title="scikit-learn中的PCA的使用"></a>scikit-learn中的PCA的使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    X = np.empty((<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>    X[:, <span class="hljs-number">0</span>] = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, size=<span class="hljs-number">100</span>)<br>    X[:, <span class="hljs-number">1</span>] = <span class="hljs-number">0.75</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, size=<span class="hljs-number">100</span>)<br><br>    pca = PCA(n_components=<span class="hljs-number">1</span>)<br>    pca.fit(X)<br>    <span class="hljs-comment"># 不是使用梯度上升法解出来的, 和我们解的结果是相反的</span><br>    <span class="hljs-comment"># 方向是完全相反的并不影响降维结果</span><br>    <span class="hljs-built_in">print</span>(pca.components_)<br><br>    X_reduction = pca.transform(X)<br>    X_restore = pca.inverse_transform(X_reduction)<br><br>    <span class="hljs-comment"># 蓝色点是原始数据</span><br>    plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;b&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>    <span class="hljs-comment"># 红色点是降维后有恢复成高维数据的点</span><br>    plt.scatter(X_restore[:, <span class="hljs-number">0</span>], X_restore[:, <span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;r&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/pca_p_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>使用真实数据集进行测试: 手写识别</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    digits = datasets.load_digits()<br>    X = digits.data<br>    y = digits.target<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="hljs-number">666</span>)<br><br>    knn_clf = KNeighborsClassifier()<br>    <span class="hljs-comment"># 耗时69ms</span><br>    knn_clf.fit(X_train, y_train)<br>    <span class="hljs-comment"># 0.98666</span><br>    <span class="hljs-built_in">print</span>(knn_clf.score(X_test, y_test))<br><br>    <span class="hljs-comment"># 使用降维, 降维2维数据</span><br>    pca = PCA(n_components=<span class="hljs-number">2</span>)<br>    pca.fit(X_train)<br>    X_train_reduction = pca.transform(X_train)<br>    X_test_reduction = pca.transform(X_test)<br><br>    <span class="hljs-comment"># 耗时2ms</span><br>    knn_clf.fit(X_train_reduction, y_train)<br>    <span class="hljs-comment"># 0.60666</span><br>    <span class="hljs-built_in">print</span>(knn_clf.score(X_test_reduction, y_test))<br></code></pre></td></tr></table></figure>
<p>可以看到降维后fit的时间大大减少, 但是由于从64维降到2维, 所以损失一些数据, predict的score也减少了, 但是具体应该降到几维?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># [0.14566817 0.13735469], 现在PCA中有2个轴</span><br>  <span class="hljs-comment"># 第一个轴能解释14.5%的方差, 第二个轴能解释13.7%的方差</span><br>  <span class="hljs-built_in">print</span>(pca.explained_variance_ratio_)<br><br>  <span class="hljs-comment"># 可以通过上面这个变量找到应该把数据降到多少维</span><br>  pca = PCA(n_components=X_train.shape[<span class="hljs-number">1</span>])<br>  pca.fit(X_train)<br><br>  plt.plot([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X_train.shape[<span class="hljs-number">1</span>])],<br>           [np.<span class="hljs-built_in">sum</span>(pca.explained_variance_ratio_[:i+<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X_train.shape[<span class="hljs-number">1</span>])])<br>  plt.show()<br></code></pre></td></tr></table></figure>
<p>pca内置一个属性 <b>explained<em>variance_ratio</em></b> , 记录了每个主要成分对原始数据方差的描述程度 (这个对应着PCA背后深刻的数学原理, 这里不进行深入的探索)</p>
<p>因此求出所有主成分以后就可以画出这样的图:</p>
<p><img src="/Blog/intro/pca_p_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这个曲线就代表了随着取前k个主成分的变化(横轴), 对总体原来的方程的解释率(大概信息保留的比例)(纵轴)</p>
<p>我们就可以通过这个曲线找到需要把数据降到几维</p>
<p>比如说我们希望数据能保持95%以上的信息, 就在纵轴找到0.95, 再看横轴对应的数就好了</p>
<p>而这个功能已经被scikit-learn封装好了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">pca = PCA(<span class="hljs-number">0.95</span>)<br>pca.fit(X_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;n_components_=&quot;</span>, pca.n_components_)     <span class="hljs-comment"># 28</span><br></code></pre></td></tr></table></figure>
<p>这样初始化PCA就代表, 我不知道要取多少个主成分, 但是我取的主成分的个数应该能解释95%以上的方差</p>
<p>fit后n_components=28, 说明对于原来64维的数据, 使用28维就能解释原本数据95%以上的方差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train_reduction = pca.transform(X_train)<br>   X_test_reduction = pca.transform(X_test)<br>   <span class="hljs-comment"># 耗时7.52ms</span><br>   knn_clf.fit(X_train_reduction, y_train)<br>   <span class="hljs-built_in">print</span>(knn_clf.score(X_test_reduction, y_test))      <span class="hljs-comment"># 0.98</span><br></code></pre></td></tr></table></figure>
<p>降到28维时, fit耗时就只有7.52ms, 远远快于使用全样本进行训练, 而且score也足够的好, 比原来的score=0.98666没下降多少</p>
<p>这就是使用PCA进行降维的威力</p>
<p><br></p>
<p>但这也不是说, 用PCA将数据降到2维是完全没有意义的, 把数据降到2维有非常重要的意义: <strong>可视化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">pca = PCA(n_components=<span class="hljs-number">2</span>)<br>pca.fit(X)<br>X_reduction = pca.transform(X)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    plt.scatter(X_reduction[y==i, <span class="hljs-number">0</span>], X_reduction[y==i, <span class="hljs-number">1</span>], alpha=<span class="hljs-number">0.8</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/pca_p_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>在二维空间中, 其实很多数字的区分度是很明显的</p>
<p>如果我们只想区分蓝色的数字和紫色的数字, 很有可能降维到2维就足够了</p>
<p><br></p>
<p><br></p>
<h2 id="尝试MNIST数据集-手写数字"><a href="#尝试MNIST数据集-手写数字" class="headerlink" title="尝试MNIST数据集(手写数字)"></a>尝试MNIST数据集(手写数字)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br><br>    <span class="hljs-comment"># 下载mnist</span><br>    mnist = fetch_openml(<span class="hljs-string">&quot;mnist_784&quot;</span>)<br><br>    X, y = mnist[<span class="hljs-string">&#x27;data&#x27;</span>], mnist[<span class="hljs-string">&#x27;target&#x27;</span>]<br><br>    <span class="hljs-comment"># 一共有70000个数据, 每个数据有784个维度, 前60000是训练数据</span><br>    <span class="hljs-comment"># 这里取前6000作为训练, 后1000作为测试</span><br>    X_train = np.array(X[:<span class="hljs-number">6000</span>], dtype=<span class="hljs-built_in">float</span>)<br>    y_train = np.array(y[:<span class="hljs-number">6000</span>], dtype=<span class="hljs-built_in">float</span>)<br>    X_test = np.array(X[<span class="hljs-number">69000</span>:], dtype=<span class="hljs-built_in">float</span>)<br>    y_test = np.array(y[<span class="hljs-number">69000</span>:], dtype=<span class="hljs-built_in">float</span>)<br><br>    <span class="hljs-comment"># 训练原本的数据</span><br>    knn_clf = KNeighborsClassifier()<br>    <span class="hljs-comment"># scikit-learn中的kNN在数据量非常大的时候会用树结构进行存储, 而不是简单的拷贝</span><br>    knn_clf.fit(X_train, y_train)<br>    <span class="hljs-comment"># score = 0.945</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;before PCA, score =&quot;</span>, knn_clf.score(X_test, y_test))<br><br>    <span class="hljs-comment"># PCA降维</span><br>    pca = PCA(<span class="hljs-number">0.9</span>)<br>    pca.fit(X_train)<br>    X_train_reduction = pca.transform(X_train)<br>    X_test_reduction = pca.transform(X_test)<br>    <br>    <span class="hljs-comment"># 训练降维后的数据</span><br>    knn_clf = KNeighborsClassifier()<br>    knn_clf.fit(X_train_reduction, y_train)<br>    <span class="hljs-comment"># score = 0.954</span><br>    <span class="hljs-comment"># 降维之后, 不仅运行速度变快了, 准确率也变高了</span><br>    <span class="hljs-comment"># 这就是PCA另外一个应用: 降噪, 将原有数据所包含的噪音消除了</span><br>    <span class="hljs-comment"># 这使得可以更加准确地得到特征</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;after PCA, score =&quot;</span>, knn_clf.score(X_test_reduction, y_test))<br></code></pre></td></tr></table></figure>
<p>这就体现出PCA的另外一个重要功能: 降噪</p>
<p>因此通过PCA降维处理后的数据, 不仅速度变快, 准确率反而更高了. 这是非常好的结果.</p>
<blockquote>
<p>kNN没有对数据进行归一化, 这是因为这784个维度本身代表的就是28*28方阵中, 每一个像素的灰度. 也就是说每一个维度的尺度是一样的, 因此不用归一化</p>
</blockquote>
<p><br></p>
<p><br></p>
<h2 id="使用PCA对数据进行降噪"><a href="#使用PCA对数据进行降噪" class="headerlink" title="使用PCA对数据进行降噪"></a>使用PCA对数据进行降噪</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.empty((<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>X[:, <span class="hljs-number">0</span>] = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, size=<span class="hljs-number">100</span>)<br>X[:, <span class="hljs-number">1</span>] = <span class="hljs-number">0.75</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, size=<span class="hljs-number">100</span>)<br><br>plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>])<br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/pca_p_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>其实有一个问题, 这个数据集展现出这样一个结果</p>
<p>有没有可能实际数据集就是一根直线? 换句话说这个数据集展现出是在一个直线上下进行抖动分布. 这种抖动和这个直线的距离是噪音, 可能是测量仪器本身有误差, 测量手段有问题…</p>
<p>在这个过程中使用PCA, 使用transform降维成1维数据, 再用inverse_transform将1维数据转回二维数据</p>
<p>然后再绘制这些数据, 此时数据就成为了一条直线</p>
<p><img src="/Blog/intro/PCA_19.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>我们完全可以把这个过程理解为: <strong>将原有的数据的噪音给去除了</strong></p>
<p>然而在实际情况下, 我们不好说这样的数据就是一点噪音都没有了, 也不好说所有的抖动都是噪音, 所以总体而言我们还是倾向于说: <strong>丢失了信息</strong></p>
<p>不过我们丢失的信息很<strong>有可能有大部分是噪音</strong>, 这也造成了对于mnist数据集来说进行降维处理后, 识别准确率反而提高了</p>
<blockquote>
<p>当然现在判断识别准确率的方式还是非常简陋的, 对如何判断识别准确率后续还会强调</p>
</blockquote>
<p><br></p>
<p>再举一个例子, 依然是手写识别</p>
<p>这次故意给数据添加一些噪音</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><br><span class="hljs-comment"># 展示数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_digits</span>(<span class="hljs-params">data</span>):<br>    fig, axes = plt.subplots(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>),<br>                             subplot_kw=&#123;<span class="hljs-string">&#x27;xticks&#x27;</span>: [], <span class="hljs-string">&#x27;yticks&#x27;</span>: []&#125;,<br>                             gridspec_kw=<span class="hljs-built_in">dict</span>(hspace=<span class="hljs-number">0.1</span>, wspace=<span class="hljs-number">0.1</span>))<br>    <span class="hljs-keyword">for</span> i, ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(axes.flat):<br>        ax.imshow(data[i].reshape(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>),<br>                  cmap=<span class="hljs-string">&#x27;binary&#x27;</span>, interpolation=<span class="hljs-string">&#x27;nearest&#x27;</span>,<br>                  clim=(<span class="hljs-number">0</span>, <span class="hljs-number">16</span>))<br>    plt.show()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    digits = datasets.load_digits()<br>    X = digits.data<br>    y = digits.target<br><br>    <span class="hljs-comment"># 创建一个有噪音的数据集</span><br>    noisy_digits = X + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">4</span>, size=X.shape)<br><br>    <span class="hljs-comment"># 100个样本, 每10个样本是一个类别</span><br>    <span class="hljs-comment"># 1-10为0, 11-20为1....每个元素对应10个样本</span><br>    example_digits = noisy_digits[y == <span class="hljs-number">0</span>][:<span class="hljs-number">10</span>]<br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>):<br>        X_num = noisy_digits[y==num, :][:<span class="hljs-number">10</span>]<br>        example_digits = np.vstack([example_digits, X_num])<br><br>    <span class="hljs-comment"># 展示去噪前的数据</span><br>    plot_digits(example_digits)<br><br>    <span class="hljs-comment"># 感觉噪音比较大, 所以只保留50%的信息</span><br>    pca = PCA(<span class="hljs-number">0.5</span>)<br>    pca.fit(noisy_digits)<br>    <span class="hljs-built_in">print</span>(pca.n_components_) <span class="hljs-comment"># 12维</span><br><br>    <span class="hljs-comment"># 降维在升维, 降噪</span><br>    components = pca.transform(example_digits)<br>    filtered_digits = pca.inverse_transform(components)<br>    <br>    <span class="hljs-comment"># 展示去噪后的数据</span><br>    plot_digits(filtered_digits)<br></code></pre></td></tr></table></figure>
<p>降噪前的数据</p>
<p><img src="/Blog/intro/pca_p_6.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>降噪后的数据</p>
<p><img src="/Blog/intro/pca_p_7.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h2 id="人脸识别与特征脸"><a href="#人脸识别与特征脸" class="headerlink" title="人脸识别与特征脸"></a>人脸识别与特征脸</h2><p>PCA在人脸识别领域的应用: <strong>特征脸</strong></p>
<p><img src="/Blog/intro/PCA_25.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/PCA_26.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<script type="math/tex; mode=display">
X_k=X \cdot W_k^T</script><p>之前说过, 对于样本数据 X, 从n维空间映射到 k 维空间, 只需要使用PCA算法求出前k个主成分</p>
<p>对于X中的每一个样本(行向量) 都和 W中的每一行进行点乘, 得到的结果相当于<strong>这个样本映射到W这一行表示的轴中所得到的映射向量的模(大小)</strong></p>
<p>但是这里可以换一个思路来想, 对于W矩阵来说, 它也有n列</p>
<p>也就是说<strong>它的维度和X的维度是一样的</strong></p>
<blockquote>
<p>PCA算法本质实在空间中找一组基, 所以原本空间有几个维度, 这组基就有几个维度.</p>
</blockquote>
<p>如果将W矩阵的每一行想象成是一个样本的话, 它代表是什么?</p>
<p>W的每一行都代表一个方向, 第一个方向是最重要的方向, 第二个方向是次重要的方向, 以此类推</p>
<p>如果把W的每一行都看作是一个样本的话, 我们也可以说: <strong>第一行所代表的样本是最重要的样本, 最能表示X这个矩阵原来样本特征的样本</strong>, 第二行所表示的样本是次重要的样本, 它也能够非常好地反应原来X的样本的特征</p>
<p>将W的每一行当作样本看待, 在人脸识别领域中, X中的每一行都是一个人脸的图形, W这个样本的每一行也可以理解成是一个人脸, 这个人脸就称之为是<strong>特征脸</strong>(eigen face)</p>
<blockquote>
<p>之所以叫它特征量, 是因为每一个特征脸其实对应的是一个主成分, 它相当于表达了一部分原来样本中这些人脸数据对应的特征</p>
<p>特征脸的英文是: eigen face, eigen 是和矩阵中的 eigen value对应的. 即: <strong>特征值</strong></p>
</blockquote>
<p><br></p>
<p>编程实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_lfw_people<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><br><span class="hljs-comment"># 画图</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_faces</span>(<span class="hljs-params">faces</span>):<br>    fig, axes = plt.subplots(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>),<br>                             subplot_kw=&#123;<span class="hljs-string">&#x27;xticks&#x27;</span>: [], <span class="hljs-string">&#x27;yticks&#x27;</span>: []&#125;,<br>                             gridspec_kw=<span class="hljs-built_in">dict</span>(hspace=<span class="hljs-number">0.1</span>, wspace=<span class="hljs-number">0.1</span>))<br>    <span class="hljs-keyword">for</span> i, ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(axes.flat):<br>        ax.imshow(faces[i].reshape(<span class="hljs-number">62</span>, <span class="hljs-number">47</span>), cmap=<span class="hljs-string">&#x27;bone&#x27;</span>)<br>    plt.show()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 要安装pillow包才能使用</span><br>    <span class="hljs-comment"># 可以传入min_faces_per_person参数, 来过滤出最少要有多少照片的人</span><br>    faces = fetch_lfw_people()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;data.shape =&quot;</span>, faces.data.shape)         <span class="hljs-comment"># (13233, 2914)     图像一维展现出来</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;images.shape =&quot;</span>, faces.images.shape)     <span class="hljs-comment"># (13233, 62, 47)   图像二维展现出来</span><br><br>    <span class="hljs-comment"># 打乱X的顺序</span><br>    random_indexes = np.random.permutation(<span class="hljs-built_in">len</span>(faces.data))<br>    X = faces.data[random_indexes]<br><br>    <span class="hljs-comment"># 取前36张脸</span><br>    example_faces = X[:<span class="hljs-number">36</span>, :]<br><br>    <span class="hljs-comment"># 展示随机的36张脸</span><br>    plot_faces(example_faces)<br><br>    <span class="hljs-comment"># 随机的方式求解PCA</span><br>    <span class="hljs-comment"># 没有指定n_components, 说明所有的主成分都求出来</span><br>    pca = PCA(svd_solver=<span class="hljs-string">&#x27;randomized&#x27;</span>)<br>    pca.fit(X)<br><br>    <span class="hljs-comment"># 把主成分当作样本来看, 绘制前36张脸</span><br>    plot_faces(pca.components_[:<span class="hljs-number">36</span>, :])<br></code></pre></td></tr></table></figure>
<p>取出随机36张脸</p>
<p><img src="/Blog/intro/pca_p_8.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>取出前36张特征量</p>
<p><img src="/Blog/intro/pca_p_9.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于特征脸来说, 前面的脸比较笼统, 尤其是第一张, 只告诉我们人脸大概是什么样, 越往后鼻子眼睛的信息就清晰起来. 也就是说对于描述一张脸, 一张脸的大概轮廓是最重要的, 之后是鼻子眼镜…</p>
<p>通过求出特征脸, 一方面我们可以直观看到在人脸识别的时候我们是怎么看待人脸的特征的, 另外一方面, 也可以根据之前的式子看出来, 每一个人脸都是这些<strong>特征脸的线性组合</strong>, 而特征脸依据重要程度顺序排在这里</p>
<p><br></p>
<blockquote>
<p>思考:</p>
<ol>
<li>特征脸和特征值的关系</li>
<li>PCA是如何求出每个主成分所表述原本数据集方差的比率的</li>
<li>是否可以反过来从最不重要的成分开始解析, 即最小化投影后的方差来反向进行主成分分析. 这样就可以使用最小二乘法? 不可以, 因为w会渐渐靠近0, 梯度也渐渐归0</li>
</ol>
</blockquote>
<p><br></p>
<h1 id="补充2-关于PCA的explainedvariance-ratio的思考"><a href="#补充2-关于PCA的explainedvariance-ratio的思考" class="headerlink" title="补充2: 关于PCA的explainedvariance_ratio的思考"></a>补充2: 关于PCA的explained<em>variance_ratio</em>的思考</h1><p>scikit-learn中的PCA在fit之后可以通过explained<em>variance_ratio</em>查看各个主成分所表示原本数据集的比率</p>
<p>而这个功能是如何实现的? 在没有足够的相关知识下尝试进行反推</p>
<p><br></p>
<p>首先, 一个非常自然的想法就是: <strong>求出所有的主成分所能表示的方差和, 在求每个方差和总方差的比例</strong></p>
<p>尝试:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    X = np.empty((<span class="hljs-number">100</span>, <span class="hljs-number">2</span>))<br>    X[:, <span class="hljs-number">0</span>] = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, size=<span class="hljs-number">100</span>)<br>    X[:, <span class="hljs-number">1</span>] = <span class="hljs-number">0.75</span> * X[:, <span class="hljs-number">0</span>] + <span class="hljs-number">3</span> + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, size=<span class="hljs-number">100</span>)<br><br>    <span class="hljs-comment"># 求原本数据的主成分</span><br>    pca = PCA(n_components=<span class="hljs-number">2</span>)<br>    pca.fit(X)<br>    new_X = pca.transform(X)<br>    <span class="hljs-comment"># [0.96314273 0.03685727]</span><br>    <span class="hljs-built_in">print</span>(pca.explained_variance_ratio_)<br><br>    <span class="hljs-comment"># ratio是通过每个维度的方差除以总方差得来的</span><br>    var_X_1 = np.var(new_X[:, <span class="hljs-number">0</span>])<br>    var_X_2 = np.var(new_X[:, <span class="hljs-number">1</span>])<br>    var_total = var_X_1 + var_X_2<br>    <span class="hljs-comment"># 0.9631427325012338  ,  0.036857267498766076</span><br>    <span class="hljs-built_in">print</span>(var_X_1 / var_total, <span class="hljs-string">&quot; , &quot;</span>, var_X_2/var_total)<br>    <span class="hljs-comment"># 1459.7675646638604</span><br>    <span class="hljs-built_in">print</span>(var_total)<br></code></pre></td></tr></table></figure>
<p>new_X是新的坐标系下的X, 它的维度和X一样, 因此它的第0列就是把X投影到第一主成分后的模(长度), 它的第一列就是投影到第二主成分后的模</p>
<p>得到的结果的确如我所猜测, 这种方法确实可以求出来每个主成分的方差在总方差上的占比</p>
<p>也就是说如果我们要想求方差的比例, 必须<strong>先求出总体的方差和</strong></p>
<p><br></p>
<p>然而这未免有一些大材小用了, 为了求比例就要把所有的主成分都求出来.</p>
<p>因此再分析一下上面到底求的是什么</p>
<p>首先new_X的所有维度的均值都为0, 这是因为再PCA内部进行了demean操作</p>
<script type="math/tex; mode=display">
Var(x) = \frac 1m \sum_{i=1}^m(x_i-\bar x)^2</script><p>当x的均值为0时, 方差就是求各项的平方和在除以总数</p>
<script type="math/tex; mode=display">
Var(x) = \frac 1m \sum_{i=1}^mx_i^2</script><p>m是一个常数, 也就是说我们对new_X的第0列求方差就是<strong>把X投影到第一主成分后, 所有样本模的平方和</strong>再除以m</p>
<p>或者说对所有样本以第一主成分进行降维, 降成一维数据后, 所表示的值就是模</p>
<p><img src="/Blog/intro/pca_thinking.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>同理对第二列求方差就是<strong>把X投影到第二主成分后, 所有样本模的平方和</strong>再除以m</p>
<p>因此推广到n维: 求<strong>所有主成分方差和</strong>就是求<strong>demean后的样本所有维度的平方和</strong>, <strong>也就是demean后的样本数据的模的平方</strong>再除以m</p>
<p><img src="/Blog/intro/pca_thinking_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>而这个demean后的样本数据的模的平方不仅可以在主成分轴上表示, 在原始的数据轴上也可以表示, 计算方法也是所有维度的平方和 (根据向量模的定义) 再除以m</p>
<p><img src="/Blog/intro/pca_thinking_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>也就是说, 无论在哪个坐标系下, demean后的数据的总方差就等于所有样本的各个维度的值的平方和再除以m,</p>
<p>因此对于矩阵来说就是<strong>所有项的平方的和除以样本数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X = demean(X)<br><span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">sum</span>(X**<span class="hljs-number">2</span>)/<span class="hljs-built_in">len</span>(X))<br></code></pre></td></tr></table></figure>
<p>此时就可以通过编程实现了, 把demean后的矩阵中X所有项的平方和再除以样本个数就是总方差</p>
<p>因为PCA本身就有demean的操作</p>
<p>求出总方差之后就可以非常简单的对各个主成分的方差求比例了</p>
<p><br></p>
<p>然而可以进一步思考: 如果不进行demean操作, 如何求出总方差?</p>
<p>因为demean只是偏移了一下坐标系, 点与点之间的距离没有变, 而方差是<strong>描述样本整体分布的疏密的指标</strong></p>
<p>其实上面的步骤的核心是<strong>对样本数据各个维度求方差在加起来</strong>, 只不过由于样本各个维度的均值都为0, 所以表现出来就是对每个样本模的平方进行加和</p>
<p>而如果不进行demean的话, 核心思想也是对各个维度求方差在加和.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 转换坐标前的总方差和转换坐标后的中放出一样</span><br>var_X_1 = np.var(X[:, <span class="hljs-number">0</span>])<br>var_X_2 = np.var(X[:, <span class="hljs-number">1</span>])<br>var_total = var_X_1 + var_X_2<br><span class="hljs-built_in">print</span>(var_total)<br></code></pre></td></tr></table></figure>
<p>此时就不能使用样本的模来求总方差了, 因为模改变了, 但点与点之间各个维度的方差和没有改变. 如果一定要用样本的模来求总方差, 则一定要先demean</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 测试, 总体方差=均值归一化后点的模的平方和</span><br>X = demean(X)<br>norms = np.array([np.linalg.norm(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> X])**<span class="hljs-number">2</span><br>var_total = np.<span class="hljs-built_in">sum</span>(norms) / <span class="hljs-built_in">len</span>(X)<br><span class="hljs-built_in">print</span>(var_total)<br></code></pre></td></tr></table></figure>
<p><br></p>
<p>但总而言之, 由于PCA本身就要进行一步demean, 所以最方便最快捷的操作还是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X = demean(X)<br><span class="hljs-built_in">print</span>(np.<span class="hljs-built_in">sum</span>(X**<span class="hljs-number">2</span>)/<span class="hljs-built_in">len</span>(X))<br></code></pre></td></tr></table></figure>
<p><br></p>
<p><br></p>
<p>补充3: 主成分分析的另一种可能求法</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/Blog/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/Blog/tags/python3/">python3</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/Blog/2020/05/01/machine-learning-6/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">python3入门机器学习(6)-多项式回归与模型泛化</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/Blog/2020/04/27/machine-learning-4/">
                        <span class="hidden-mobile">python3入门机器学习(4)-梯度下降法</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/Blog/js/events.js" ></script>
<script  src="/Blog/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/Blog/js/local-search.js" ></script>



  
    <script  src="/Blog/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/Blog/js/boot.js" ></script>


</body>
</html>
