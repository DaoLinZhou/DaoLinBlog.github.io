

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2808425287808210"
     crossorigin="anonymous"></script>
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.ico">
  <link rel="icon" href="/img/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Daolin">
  <meta name="keywords" content="">
  
    <meta name="description" content="&lt;专为程序员设计的线性代数&gt;学习笔记(3)">
<meta property="og:type" content="article">
<meta property="og:title" content="&lt;专为程序员设计的线性代数&gt;学习笔记(3)">
<meta property="og:url" content="https://daolinzhou.github.io/2020/01/24/play-with-linear-algebra-3/">
<meta property="og:site_name" content="Daolin&#39;s Repository">
<meta property="og:description" content="&lt;专为程序员设计的线性代数&gt;学习笔记(3)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/base.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/base_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/orthogonality.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/dot_product_orthogonality.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/proof_linear_no_relate.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/proof_linear_no_relate_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/proof_linear_no_relate_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/proof_linear_no_relate_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/proof_linear_no_relate_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/unionlize.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/two_vector.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/projection.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/projection_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/projection_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/projection_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/projection_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/projection_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/3d_projection_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/projection_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/unionlize.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/def_standard_or_ma.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_ma_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/matrix_qr.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/matrix_qr_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/matrix_qr_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/matrix_qr_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_15.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_16.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_17.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/standard_or_mat.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/relationship_a_p_q_17.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/ex_use_sub_space.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_space_and_e_1_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/trans_coord_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/proof_linear_tr_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/proof_linear_tr_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/linear_transform_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_dif_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_dif_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_dif_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/basis_det_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/4_properity_det_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cal_Det_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_pro_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_15.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_16.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_17.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/dia_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/left_dia_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/dia_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/det_row_col_16.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Cramer_proof.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Cramer_proof_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/cross_product.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Eigenvalues.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Eigenvalues_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Eigenvalues_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Eigenvalues_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Eigenvalues_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Eigenvalues_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/Eigenvalues_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/infinity_eigenvec.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/infinity_eigenvec_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/infinity_eigenvec_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/infinity_eigenvec_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/infinity_eigenvec_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/infinity_eigenvec_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/infinity_eigenvec_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/sove_eigenvalue_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/property_eigenvalue_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/fundamental_theorem.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/fundamental_theorem_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_15.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_16.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/visuallize_elgenvalue_17.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/eigen_vec_linear_indep.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_eigenvalue_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similar_triangle.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/similer_matrix_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/matrix_sim.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/d_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonlization_matrix_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonalization.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonalization_power.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonalization_power_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonalization_power_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonalization_power_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonalization_power_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/diagonalization_power_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/symmetric_matrix.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/org_dia_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/singular_value_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_2.jpg">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/meaning_of_singular_value_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_10.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_11.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_12.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_13.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_14.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_15.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_16.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/SVD_proof_17.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/virtual_number_9.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_7.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_8.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_5.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/complex_number_6.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/state_of_system.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/markov_chain.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/markov_chain_1.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/markov_chain_2.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/markov_chain_3.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/markov_chain_4.PNG">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/markov_chain_5.PNG">
<meta property="article:published_time" content="2020-01-25T07:14:13.000Z">
<meta property="article:modified_time" content="2020-05-23T21:29:32.257Z">
<meta property="article:author" content="Daolin">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="线性代数">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daolinzhou.github.io/Blog/intro/base.PNG">
  
  
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2808425287808210" crossorigin="anonymous"></script>
  
  <title>&lt;专为程序员设计的线性代数&gt;学习笔记(3) - Daolin&#39;s Repository</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"daolinzhou.github.io","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Daolin&#39;s Repo</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/intro/eye.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="&lt;专为程序员设计的线性代数&gt;学习笔记(3)">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-01-24 23:14" pubdate>
        2020年1月24日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      39k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      322 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">&lt;专为程序员设计的线性代数&gt;学习笔记(3)</h1>
            
            <div class="markdown-body">
              <link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><p>&lt;专为程序员设计的线性代数&gt;学习笔记(3)</p>
<span id="more"></span>
<h1 id="正交性-标准正交矩阵和投影"><a href="#正交性-标准正交矩阵和投影" class="headerlink" title="正交性, 标准正交矩阵和投影"></a>正交性, 标准正交矩阵和投影</h1><h2 id="正交基与标准正交基"><a href="#正交基与标准正交基" class="headerlink" title="正交基与标准正交基"></a>正交基与标准正交基</h2><p>描述空间的重要的方式, 除了<strong>维度</strong>以外, 就是<strong>基</strong></p>
<p>实际上, 维度和基这两个概念是有联系的: <strong>基是一组向量, 这组向量的个数就是这个空间的维度</strong></p>
<p>回忆: 一个n维空间任何一组线性无关的向量, 都是这个n维空间的一组基</p>
<p>因此对于n维空间来说, 它有无数组基, 每组基都可以生成二维空间中任何一个向量</p>
<p><img src="/Blog/intro/base.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>然而我们更习惯用标准正交基, 无论三维二维n维, 它们两两互相垂直, 我们称这种两两互相垂直的基为<strong>正交基</strong></p>
<p>我们可以很容易用正交基来描述一个点的位置(只需看一下横纵坐标的值即可), 而在一般的基上寻找点的位置相对比较困难</p>
<p><img src="/Blog/intro/base_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>所以, 正交基非常方便我们研究这个空间中所有的<strong>向量</strong>, 或者说所有的点</p>
<p><br></p>
<h3 id="什么是正交"><a href="#什么是正交" class="headerlink" title="什么是正交"></a>什么是正交</h3><p>正交只是垂直的另外一种说法</p>
<p><img src="/Blog/intro/orthogonality.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>当u, v相垂直时会发生什么</p>
<p><img src="/Blog/intro/dot_product_orthogonality.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此在线性代数领域, 我们这样定义正交:</p>
<p><strong>如果两个向量的点乘结果为0, 我们就可以说这两个向量互相垂直, 或者说这两个向量正交, 反之也是成立的</strong></p>
<p><br></p>
<h3 id="正交向量组"><a href="#正交向量组" class="headerlink" title="正交向量组"></a>正交向量组</h3><p>定义: 一组向量, 如果两两正交, 则称为正交向量组</p>
<p><strong>正交非零向量组</strong>一定线性无关  (由于零向量和任意向量点乘都为0, 所以零向量和任意向量都是垂直的, 而包含零向量意味着一定线性相关, 因此刨去)</p>
<p><br></p>
<h4 id="证明线性无关"><a href="#证明线性无关" class="headerlink" title="证明线性无关"></a>证明线性无关</h4><p>证明: 正交非零向量组一定线性无关</p>
<p><img src="/Blog/intro/proof_linear_no_relate.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意, 我们已知这组向量两两正交, 就说说任意两个向量点乘的结果为0</p>
<p>为了用上这个已知条件, 我们就要让等式以向量点乘的方式来表示, 因此等式左右同乘vi</p>
<p><img src="/Blog/intro/proof_linear_no_relate_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而这就意味着</p>
<p><img src="/Blog/intro/proof_linear_no_relate_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>由于没有零向量, 所以ki=0</p>
<p><img src="/Blog/intro/proof_linear_no_relate_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于任取的vi, 都有ki=0, 因此如果把v1到vn取个便, k1 = k2 = … = kn = 0</p>
<p>k只有零解, 得证</p>
<p><br></p>
<p>回忆: n维空间中, 任意n个线性无关的向量, 一定是这个n维空间的基</p>
<p>结合这个结论: 正交非零向量组一定线性无关</p>
<p>这意味着: <strong>n个非零正交向量一定是n维空间的基</strong></p>
<p>这相当于我们把<strong>线性无关</strong>这样的问题, 转换成了<strong>正交</strong>这样的问题</p>
<p><br></p>
<h3 id="正交基和标准正交基"><a href="#正交基和标准正交基" class="headerlink" title="正交基和标准正交基"></a>正交基和标准正交基</h3><h4 id="正交基"><a href="#正交基" class="headerlink" title="正交基"></a>正交基</h4><p>这样就引出了正交基这个概念:</p>
<p><strong>如果一个空间的一组基两两正交, 则称这组基为一组正交基</strong></p>
<p>这里不要求非零, 因为正交基首先要是一组基, 而一组基若要线性无关肯定不能有零向量, 因此我们不用限定非零这个条件, 这个条件隐含在<strong>它是一组基</strong>这个概念里了</p>
<p><br></p>
<h4 id="标准正交基"><a href="#标准正交基" class="headerlink" title="标准正交基"></a>标准正交基</h4><p>在此之上我们再引出标准正交基就非常简单了:</p>
<p><strong>如果一个空间的一组正交基, 模均为1, 则称这组基是一组标准正交基</strong></p>
<p><br></p>
<p>正交基是一种特殊的基, 标准正交基是一种特殊的标准正交基. 范围是逐渐缩小的关系.</p>
<p>尽管如此一个空间的标准正交基依然有<strong>无数组</strong></p>
<p><img src="/Blog/intro/proof_linear_no_relate_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h2 id="一维投影"><a href="#一维投影" class="headerlink" title="一维投影"></a>一维投影</h2><p>看起来和正交一点关系都没有, 但其实他们的关系是巨大的</p>
<p>更重要的是, 这里我们其实把一个非常复杂的过程进行拆解, 先看这个复杂过程的一小部分</p>
<p><br></p>
<p>这个复杂的过程指的是: </p>
<p>通常对于一个空间来说, 我们可能只能得到随便的一组基. 而我们希望进一步找到这个空间的一组<strong>正交基</strong></p>
<p>这是我们接下来三个小节要一直处理的问题</p>
<p>如果找到一组正交基, 更进一步, 找到一组<strong>标准正交基</strong>就特别容易了, 只要把找到的正交基进行<strong>归一化</strong>处理就好了</p>
<p><img src="/Blog/intro/unionlize.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>这一小节将从最简单的二维情况出发, 来看看这个过程是怎样做的</p>
<blockquote>
<p>本小结虽然叫做<strong>投影</strong></p>
<p>投影本身是在线性代数领域不小的一个话题</p>
<p>但我们的关注点并不在投影, 只是借助投影来帮我们求出一组正交基</p>
</blockquote>
<p><br></p>
<p>如果给出任意两个不共线的向量(因为我们假设的是给出一组基, 因此一定不共线), 如何根据这两个向量求出二维平面的一组正交基?</p>
<p><img src="/Blog/intro/two_vector.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们要借助投影, 但实际上我们只需要改变一个向量, 让它和另一个向量垂直, 这里假设向量u是不变的, 我们改变向量v让它们互相垂直</p>
<p>很简单, 如果我们先求出 v向量 在 u向量 上的投影</p>
<p><img src="/Blog/intro/projection.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>所谓投影, 就是把 <strong>v向量</strong> 理解为线段的话, 从线段末端向u所在直线做垂线, 把垂足的位置当作是另外一个向量的末端.</p>
<p>这个蓝色向量p 就是v向量在u向量上的投影</p>
<p>如果我们已知p向量, 那么相应的我们求出和u向量垂直的向量就非常容易</p>
<p><img src="/Blog/intro/projection_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>绿色的向量(v-p) 就和u垂直, 因为绿色向量加蓝色向量等于向量v</p>
<p>u和(v-p) 就是这个二维空间的一组正交基</p>
<blockquote>
<p>为什么叫做一维投影, 核心就是我们要找到一组蓝色向量p, 即投影向量</p>
</blockquote>
<p>下面的关键就是求出p</p>
<p><br></p>
<h3 id="求投影向量p"><a href="#求投影向量p" class="headerlink" title="求投影向量p"></a>求投影向量p</h3><p>非常容易, 在之前讲向量时已经涉及过了</p>
<p>点乘的结果就是将一个向量投影到另一个向量上在做向量乘法</p>
<p><img src="/Blog/intro/projection_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这样就即得到了p的模, 以及p的方向. 合在一起就得到了p向量的表达</p>
<p><img src="/Blog/intro/projection_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>注意: 向量不能连续点乘, u <em> v 是一个数, 所以 (u </em> v) <em> u 不等价于 (u </em> u *) v</p>
<p>个人理解:</p>
<p>u<em>v 相当于用u的长度乘以把v<strong>投影</strong>到u上的长度 ||v|| </em> cos(θ), 而u乘以u则是u的长度的平方</p>
<p>因此 uv/uu 的结果可以看作是 v到u的投影是u的多少倍, 假设为k倍, 就说明把u伸缩k倍就得到v到u的投影</p>
<p>因此把k乘以u得到的就是v到u的投影 </p>
</blockquote>
<p>这就是一维投影. 我们把一个向量投影到另外一个向量(可以理解为是一维空间)上</p>
<p>在有的教材可能会换一种写法</p>
<p><img src="/Blog/intro/projection_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>不过, 写成这个样子是不能约分的, 这是向量的点乘, 不是两个数字相乘</p>
<p><br></p>
<p>此时就在<strong>二维空间</strong>找到了一组正交基, 但如果三维乃至更高维度怎么办?</p>
<p><br></p>
<p><br></p>
<h2 id="高维投影-和-Gram-Schmidt过程"><a href="#高维投影-和-Gram-Schmidt过程" class="headerlink" title="高维投影 和 Gram-Schmidt过程"></a>高维投影 和 Gram-Schmidt过程</h2><p>接下来处理更高维度的投影问题, 进而把求正交基的过程总结成一个严谨的算法过程(<strong>Gram-Schmidt过程</strong>). </p>
<p>二维空间中:</p>
<p><img src="/Blog/intro/projection_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="求三维空间中的投影"><a href="#求三维空间中的投影" class="headerlink" title="求三维空间中的投影"></a>求三维空间中的投影</h3><p><img src="/Blog/intro/3d_projection.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这三个向量是这个三维空间中的一组基, 但不是一组正交基, 不满足两两正交的条件</p>
<p>首先我们可以使用<strong>一维投影</strong>的问题, 处理其中的两个向量</p>
<p>这里先把u和v两个向量处理成正交的形状</p>
<p>u 和 v 变为 p1, p2</p>
<p><img src="/Blog/intro/3d_projection_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时p1, p2已经相互垂直. 但第三个向量w还没有和p1, p2垂直</p>
<p>下面只需要让w和p1, p2相垂直就好了</p>
<p>即: 找出w的一个分量, 它和p1, p2所在的平面垂直</p>
<p><img src="/Blog/intro/3d_projection_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这和我们之前讲的一维投影非常像, 只不过此时w向量从向一个<strong>一维子空间</strong>投影变成了向<strong>二维子空间(平面)投影</strong></p>
<p>依然, 在求出p之后用w-p就是我们所求的向量</p>
<p>问题的核心又变成了: 求w在p1, p2平面上相应的投影向量p</p>
<p><br></p>
<p>注意: 此时p1, p2本身已经是相互垂直的, 所以p1, p2已经是p1, p2所组成的二维屏面的一组<strong>正交基</strong>了</p>
<p>因此, p这个向量就可以写作是p1 和 p2 的一个线性组合, (<strong>因为p1, p2是一组基, 因此这组基所代表的平面上的任意向量都可以表示成这组基的线性组合</strong>)</p>
<p><img src="/Blog/intro/3d_projection_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>或者理解成p向p1, p2作垂线, 相应的落在p1, p2方向上就有两个分向量a和b, p = a+b</p>
<p><img src="/Blog/intro/3d_projection_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时, 已知条件除了p1, p2以外, 还有向量w</p>
<p>w的末端可以和a, b进行连接, aw垂直于p1, bw垂直于p2</p>
<p><img src="/Blog/intro/3d_projection_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>可以很明显看出:</p>
<p>a 是 w 在 p1 的投影, b 是 w 在 p2 的投影</p>
<p>即: <strong>w向量对p1和p2这两个正交基上的投影(a和b), 它们的加和 (肯定在p1, p2组成的子空间中) 就是 w向量在这个子空间 (p1, p2生成的子空间) 的投影</strong></p>
<blockquote>
<p>整理一下过程: </p>
<p>我们先求出二维平面的一组正交基 p1, p2</p>
<p>之后第三个向量w分布向p1, p2作投影, 得到分量a, b</p>
<p>把a, b加和起来就是w在p1, p2构成的子空间上的投影p</p>
<p>之后就可以求出第三个正交基: w-p</p>
</blockquote>
<p>其实我们把这个问题转换成了一个向量向另外 <strong>一个</strong> 向量上的投影</p>
<p><img src="/Blog/intro/3d_projection_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>通过a和b就可以构造p, 通过p就可以构造第三个正交向量: w-p</p>
<p><img src="/Blog/intro/3d_projection_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/3d_projection_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>如果是一个四维向量, 虽然我们无法可视化, 但是同理我们依然可以先用同样的方式求出p1, p2, p3</p>
<p>假设第四个向量为x, 根据x求出p4, 使得p4和p1, p2, p3全都垂直</p>
<p><strong>其实我们要做的就是让x减去x对p1, p2, p3 三个向量组成的子空间(三维空间)上的投影</strong></p>
<p><strong>即: p4 = x减去x对p1, p2, p3 三个向量的三个一维投影的分量</strong></p>
<p><img src="/Blog/intro/3d_projection_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>逻辑和之前完全是一样的, 只不过维度越堆越高</p>
<blockquote>
<p>思想: 从低维空间出发, 把低维空间搞明白后, 用同样的逻辑向高维空间推导</p>
</blockquote>
<p><br></p>
<h3 id="拓展到高维空间"><a href="#拓展到高维空间" class="headerlink" title="拓展到高维空间"></a>拓展到高维空间</h3><p><img src="/Blog/intro/3d_projection_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>以此类推, 这个过程称为Gram-Schmidt过程</p>
<p><br></p>
<p>注意: Gram-Schmidt过程算法的输入必须是一组基, 即<strong>线性无关</strong></p>
<p><br></p>
<p>如果线性相关:</p>
<p><img src="/Blog/intro/projection_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>如果u, v共线, 则v的投影p就是自身, v-p = 0</p>
<p>此时求出来的结果就会包含零向量, 而正交基中是不可能有零向量的, 虽然它和任意向量正交, 但零向量会使得这组向量<strong>线性相关</strong>, 不满足基的定义, 不可能是一组基</p>
<p>因此Gram-Schmidt过程是给定一组基, 求另外一组基的过程</p>
<p><br></p>
<p>进一步找到标准正交基就是把正交基进行归一化就好了</p>
<p><img src="/Blog/intro/unionlize.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>最终就得到了一组标准正交基</p>
<p><br></p>
<blockquote>
<p>为什么要用Gram-Schmidt过程的个人理解:</p>
<p>我突然想到一个问题, 如果我们有一组基, 为什么不可以把它们表示为行空间, 化为rref后的每一行不就是标准单位向量吗, 即rref的所有行向量组成一个标准正交基</p>
<p>后来我想到, 如果这组基是在高维空间中的子空间的基呢?</p>
<p>例如: u = (1, 1, 1), v = (0, 2, 3) 这样rref的方式求得的基就不是正交的, 而同样的例子, 使用Gram-Schmidt过程得到的结果确实正交的</p>
</blockquote>
<p><br></p>
<p><br></p>
<h2 id="标准正交基的性质"><a href="#标准正交基的性质" class="headerlink" title="标准正交基的性质"></a>标准正交基的性质</h2><h3 id="标准正交矩阵的重要性质"><a href="#标准正交矩阵的重要性质" class="headerlink" title="标准正交矩阵的重要性质"></a>标准正交矩阵的重要性质</h3><p>之前我们就说过可以把矩阵看作是空间</p>
<p><img src="/Blog/intro/standard_or_ma.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而现在我们就明白, 所谓把矩阵看作空间, 其实是把空间的基一列一列地排列起来就形成了矩阵</p>
<p>这样一个矩阵乘以一个点坐标, 得到的结果实际上相当于把<strong>那个矩阵表示的空间中所对应的的点</strong>转化到<strong>标准欧几里得空间中的位置</strong></p>
<p><br></p>
<p>这里头, 关键就是在于矩阵中的每一列都是空间中的一组基</p>
<p>现在我们已经直到怎样求标准正交基, 那么相应地, 就可以把标准正交基按照列的方式排成矩阵, 这个矩阵就叫<strong>标准正交矩阵</strong></p>
<p><strong>更为严谨的定义:</strong></p>
<p><img src="/Blog/intro/def_standard_or_ma.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="重要性质及其证明"><a href="#重要性质及其证明" class="headerlink" title="重要性质及其证明"></a>重要性质及其证明</h3><p><img src="/Blog/intro/standard_or_ma_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>证明: </p>
<p>假设方阵Q长这样</p>
<p><img src="/Blog/intro/standard_or_ma_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/standard_or_ma_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此Q的转置乘以Q可以写成这样</p>
<p><img src="/Blog/intro/standard_or_ma_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时Q的转置中每一行就是标准正交基的一个向量, 而Q的每一列就是标准正交基的一个向量</p>
<p>所以结果矩阵就可以表示为这样</p>
<p><img src="/Blog/intro/standard_or_ma_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意, 结果矩阵中每一个进行点乘的向量都是从标准正交基中取出来的向量, 所以他们是两两相垂直的</p>
<p>而由于每个向量都是标准单位向量, 即模为1, 自己点乘自己为1</p>
<p>所以这个矩阵主对角线上的元素都为1, 其余位置元素都为0, 即为单位矩阵 I</p>
<p><br></p>
<p>进一步可以通过这个性质推导出更加优美的性质</p>
<p>标准正交矩阵的每一列都是标准正交基中的一个向量, 说明Q的各列线性无关</p>
<p>再看一下这些等价命题</p>
<p><img src="/Blog/intro/standard_or_ma_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>其中有一条就是方阵A的列向量线性无关 / 是n维空间的基</p>
<p>因此这个<strong>标准正交矩阵Q是可逆的</strong></p>
<p>而Q的转置乘以Q等于单位矩阵, 这就说明 Q的转置是Q的<strong>左逆</strong></p>
<p>而之前也证明过, 如果一个矩阵可逆, 那么它的左逆就等于它的右逆就等于这个<strong>矩阵的逆</strong></p>
<blockquote>
<p><strong>对于标准正交矩阵Q来说, Q的逆就是Q的转置</strong> </p>
</blockquote>
<p><img src="/Blog/intro/standard_or_ma_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>这个性质有很多的应用</p>
<p>机器学习算法中, 有一个非常重要的非监督学习算法: PCA算法</p>
<p>它的作用是降维, 在降维的过程中, 我们把一个高维空间降成了低维空间</p>
<p>反过来我们要把低维空间再转乘高维空间, 我们就要用到这一条性质, 这是因为我们描述低维空间的方式就是使用标准正交矩阵</p>
<p>反过去的过程我们就看这个矩阵的逆就好了, 而这个逆可以直接通过转置得到</p>
<p>在后序讲矩阵对角化时, 也会看到Q逆等于QT这个性质的应用</p>
</blockquote>
<p><br></p>
<p><br></p>
<h2 id="矩阵QR分解"><a href="#矩阵QR分解" class="headerlink" title="矩阵QR分解"></a>矩阵QR分解</h2><p>前文中介绍过一种矩阵的分解 LU分解, 把一个矩阵分为上三角矩阵和下三角矩阵</p>
<p>类似的我们可以把一个矩阵A 分解成Q和R两部分</p>
<p><img src="/Blog/intro/matrix_qr.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>某种意义上可以可以把QR分解的R理解为LU分解的U</p>
<p>这里的R是reduce的缩写, 其实行最简形式 (rref, 第一个r也是reduce的缩写) 也可以理解为一个上三角矩阵</p>
<p><br></p>
<h3 id="为什么要把矩阵分解为QR这样的形式"><a href="#为什么要把矩阵分解为QR这样的形式" class="headerlink" title="为什么要把矩阵分解为QR这样的形式?"></a>为什么要把矩阵分解为QR这样的形式?</h3><p>之前也说过, 真实世界的很多问题就是 <strong>Ax = b</strong> 这种形式</p>
<p>我们在线性代数所学习的很多内容都是为了更方便地解决这个线性系统</p>
<p>如果A可以分解为Q和R两部分</p>
<p><img src="/Blog/intro/matrix_qr_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而R是一个上三角矩阵, 这就意味着我们已经做完了高斯消元法的过程</p>
<p>进而就可以倒着进行Jordan消元, 快速求解这个方程组</p>
<p><br></p>
<p>很快我们就会看到: 如果要想求标准正交矩阵Q, 我们就要运行Gram-Schmidt过程, 而要想运行这个过程, 必须要保证: <strong>A的各个列向量线性无关</strong></p>
<blockquote>
<p>实际上QR分解不局限于方阵, 但这里只讲述QR分解是怎么作用在方阵上的</p>
</blockquote>
<p><br></p>
<h3 id="怎样进行矩阵的QR分解"><a href="#怎样进行矩阵的QR分解" class="headerlink" title="怎样进行矩阵的QR分解"></a>怎样进行矩阵的QR分解</h3><p><img src="/Blog/intro/matrix_qr_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>把A的列向量看作是一组基, 这组基所对应的一组正交基</p>
<p>而每一个p的相应求法就是这样</p>
<p><img src="/Blog/intro/matrix_qr_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<h4 id="a-p-q之间的关系"><a href="#a-p-q之间的关系" class="headerlink" title="a, p, q之间的关系"></a>a, p, q之间的关系</h4><p>p1 很简单, 就等于a1, 而回忆p和q的关系 ( 归一化, p除以p的模等于q )又可以有下面的式子</p>
<p><img src="/Blog/intro/relationship_a_p_q.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里我们主要关注a和q之间的关系, 由于a这个向量是矩阵A中的一个<strong>列向量</strong>, 而q是标准正交矩阵中的一个<strong>列向量</strong></p>
<p>A = QR 这个等式描述的就是A这个矩阵和标准正交矩阵Q之间有这样的一个关系</p>
<p>p1的模就是一个系数, 我们把它称为r11, 因此我们可以把a1这个向量写成是 某一个系数(r11) 乘以向量q1</p>
<p><img src="/Blog/intro/relationship_a_p_q_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>接下来看p2这个向量</p>
<p><img src="/Blog/intro/relationship_a_p_q_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们依然可以将其看成: a2 减去一个系数乘以向量p1, 或者p2的模乘以q2</p>
<p>我们关注的是后面两个式子, 尝试寻找a2和q2之间的关系</p>
<p><img src="/Blog/intro/relationship_a_p_q_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们把上面的等式变化为下面的等式</p>
<p>而我们已经可以用 <strong>p1的模乘以q1</strong> 来表示p1了</p>
<p><img src="/Blog/intro/relationship_a_p_q.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>所以</p>
<p><img src="/Blog/intro/relationship_a_p_q_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而相应的, 最后这个式子中q1前面的那部分都是一个系数, q2前面那一部分也是一个系数. 同样我们把这两个系数进行简化</p>
<p><img src="/Blog/intro/relationship_a_p_q_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>再看p3</p>
<p><img src="/Blog/intro/relationship_a_p_q_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同理也可以推导出:</p>
<p><img src="/Blog/intro/relationship_a_p_q_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><img src="/Blog/intro/relationship_a_p_q_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们根据Gram-Schmidt过程, 一旦把a化成了一组正交基, 我们再把这组正交基化为标准正交基, 那么反推回来A的各个列向量就可以写为标准正交基各个列向量的线性组合</p>
<p><img src="/Blog/intro/relationship_a_p_q_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>而 我们可以把这个式子代入A, 把a1, a2… an都替换掉, 就得到</p>
<p><img src="/Blog/intro/relationship_a_p_q_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于这个矩阵, 我们可以进一步把它化成n个矩阵的和</p>
<p>这个矩阵中, 每一个向量都是一个列向量</p>
<p>我们把这些列向量中包含<strong>q1的那一项</strong>拿出来作为第一个矩阵</p>
<p><img src="/Blog/intro/relationship_a_p_q_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>把每一项含q2的矩阵拿出来组成第二个矩阵, 注意A矩阵的第一项没有q2, 所以为0</p>
<p><img src="/Blog/intro/relationship_a_p_q_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>以此类推</p>
<p>于是矩阵A就可以写成这n个矩阵相加的形式</p>
<p><img src="/Blog/intro/relationship_a_p_q_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>回忆: 我们可以通过列视角来看待矩阵的乘法</p>
<p><img src="/Blog/intro/relationship_a_p_q_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>把前一个矩阵A看作是一列一列的, 后一个矩阵B看作是一行一行的</p>
<p>而现在, 我们已经把A化作是n个矩阵相加的结果</p>
<p><img src="/Blog/intro/relationship_a_p_q_15.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这n个矩阵相加的结果, 就可以反过来想成是这样两个矩阵点乘的结果</p>
<p><img src="/Blog/intro/relationship_a_p_q_16.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>第一个矩阵, 就是由每一个q向量组成的标准正交矩阵, 每一个q向量作为列向量组成矩阵, 再乘以后面这个矩阵</p>
<p>其实, 从矩阵的加法推导出矩阵的乘法是比较难的, 因为这是反向的推导</p>
<p><br></p>
<p>其实所有的r在我们的推导的过程中, 都知道了具体该怎么求</p>
<p>但其实我们具体实现的时候, 根本不需要把这些r给求出来</p>
<p>因为刚才的整个过程可以理解为是一个<strong>证明过程</strong>. </p>
<p>我们证明了, 只要A这个方阵的各个列是线性无关的, 它就能分解为QR这样的形式, R就可以用Q的逆点乘A求出来</p>
<p><img src="/Blog/intro/relationship_a_p_q_17.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里就用到了上一节的那个优美的性质, 对于标准正交矩阵Q, Q的转置就是Q的逆</p>
<p>这样我们就把矩阵A分解为了QR两部分.</p>
<p><br></p>
<p><br></p>
<h2 id="本章小结以及更多和投影相关的话题"><a href="#本章小结以及更多和投影相关的话题" class="headerlink" title="本章小结以及更多和投影相关的话题"></a>本章小结以及更多和投影相关的话题</h2><p>这一章主要关注的是<strong>正交基</strong>, 这个基中的向量两两正交</p>
<p>而把每一个向量进行一下标准化就是<strong>标准正交基</strong></p>
<p>一个非常重要的结论: <strong>正交的非零向量组一定线性无关</strong></p>
<p>n个非零正交向量一定是n维空间的基</p>
<p><br></p>
<p>如果给定一个空间的一组基, 我们希望进一步找到这个空间的一组正交基, 需要进行<strong>Gram-Schmidt过程</strong></p>
<p>进一步可以对每一个向量进行归一化处理, 得到标准正交基</p>
<p>在Gram-Schmidt过程中, 我们一直在利用投影, 从最基础的一维投影出发, 推导出一个向量向一个高维空间投影的求法</p>
<p><br></p>
<p>把标准正交基以列向量排成矩阵就是<strong>标准正交矩阵</strong></p>
<p><img src="/Blog/intro/standard_or_mat.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>如果矩阵A (本文只讲述方阵的QR分解) 的各个列是线性无关的, 就可以进行QR分解</p>
<p>矩阵Q是标准正交矩阵, R是上三角矩阵</p>
<p><img src="/Blog/intro/relationship_a_p_q_17.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="投影还能怎样使用"><a href="#投影还能怎样使用" class="headerlink" title="投影还能怎样使用?"></a>投影还能怎样使用?</h3><p>在上一章总结各种子空间时提到子空间的应用时, 举了一个例子</p>
<p>当线性系统大概率无解时, 我们就可以找一个近似解. 我们需要修改b, 让它处于A的列空间中, 那么线性系统就有解了, 我们可以把这个解当作原来线性系统 Ax=b 中的一个近似解</p>
<p><img src="/Blog/intro/ex_use_sub_space.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>问题是如何找到距离b最近的b’, <strong>b’就是A的列空间的投影</strong></p>
<p><br></p>
<p>如果给出A的列空间, 如何找到b在A的列空间的投影呢?</p>
<p><strong>只要求出A的列空间的一组正交基, 之后求出向量b到这个正交基中各个分量的一维投影, 每一个投影都是一个分量, 把这些分量加在一起就是向量b到 A的列空间(高维空间)的投影</strong></p>
<blockquote>
<p>这个思路就是大名鼎鼎的<strong>最小二乘法</strong>的思路</p>
</blockquote>
<p><br></p>
<p><br></p>
<h1 id="坐标转换和线性变换"><a href="#坐标转换和线性变换" class="headerlink" title="坐标转换和线性变换"></a>坐标转换和线性变换</h1><p>对于一个空间来说, 基是非常重要的一个属性. 对于一个空间来说, 其实是有无数组基的</p>
<p>我们通常对正交基和标准正交基非常感兴趣, 但在不同的领域中, 我们可能会对不同的基感兴趣</p>
<p>既然一个空间中有这么多不同的基, 就涉及到一个问题:</p>
<p><strong>一个空间的一组基和另外一组基之间是怎样变换的</strong></p>
<p>因此本章的重点就是坐标转换和线性变换</p>
<p><br></p>
<h2 id="空间的基和坐标系"><a href="#空间的基和坐标系" class="headerlink" title="空间的基和坐标系"></a>空间的基和坐标系</h2><p>坐标系和空间的基是一种一一对应的关系, 有了空间的基我们就可以说有了这个空间的坐标系, 有了这个空间的一个坐标系我们就可以说有了这个空间的一个基</p>
<p>可以把<strong>坐标系</strong>看作是<strong>理解空间的基的一个视角</strong>, 反之也是成立的</p>
<p><img src="/Blog/intro/basis_space_and_e.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>更加严谨的来说, 在之前讲空间的基的时候, 证明过这样的结论:</p>
<p>在n维空间中, 如果给定一组基. 任何一个向量(或者点) 都可以表示成这组基的<strong>线性组合</strong>! <strong>且表示方法唯一</strong></p>
<p><img src="/Blog/intro/basis_space_and_e_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/basis_space_and_e_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>可以看出坐标的值就是: 把这个向量表示成这组基所对应的线性组合中每一个基向量前面的<strong>系数</strong></p>
<p>所以我们就可以给出这样一个定义</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><img src="/Blog/intro/basis_space_and_e_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>例子:</p>
<p><img src="/Blog/intro/basis_space_and_e_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>对于同样的空间V, 由于使用不同的基, 也就有了不同的坐标. 但是这些不同的坐标对应的都是同一个点x</p>
<p>所以为了区分起见</p>
<p><img src="/Blog/intro/basis_space_and_e_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>表示x这个点在B这组基下相应的坐标</p>
<p>例如:</p>
<p><img src="/Blog/intro/basis_space_and_e_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>然而通常我们提到向量并不会显示地标上它的基, 这是因为我们所用的坐标都是在 ε坐标系下来看的</p>
<p>即使我们说这个空间的基是u, v这两个向量, 但我们也不会把点的坐标表达成(2, 2), 我们还是把它表达成(12, 8)</p>
<p>但是有些情况下, 可能我们真的需要知道某一点在另一套坐标系下的坐标是多少, 此时我们就需要使用这种表示法进行区分</p>
<p><br></p>
<p><img src="/Blog/intro/basis_space_and_e_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>然而这种ε坐标系表示是我们最常用的方法, 它太常用了, 以至于我们说一个向量(a, b), 如果我们没有指定, 默认就是在这个ε坐标系下, 我们称这种坐标系所对应的基为<strong>标准基(Standard Basis)</strong>, 标准基所对应的坐标系就称<strong>标准坐标系</strong></p>
<blockquote>
<p>标准基这个中文翻译很容易和标准正交基混淆</p>
<p>标准基 Standard Basis</p>
<p>标准正交基 Orthonormal Basis</p>
<p>正交基 Orthogonal Basis</p>
<p>标准基(一个)的范围比标准正交基(无数组)的范围小</p>
</blockquote>
<p><img src="/Blog/intro/basis_space_and_e_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同一个点, 在不同的基中有不同的表示, 这些不同的表示之间有没有什么关系? 是否可以相互转换?</p>
<p><br></p>
<p><br></p>
<h2 id="其他坐标系与标准坐标系的转换"><a href="#其他坐标系与标准坐标系的转换" class="headerlink" title="其他坐标系与标准坐标系的转换"></a>其他坐标系与标准坐标系的转换</h2><p>一个空间有无数组基, 每一组基都对应着一个坐标系</p>
<p>标准基就是欧几里得空间中最常用的坐标系对应的基就是标准基, 相应的坐标系就叫做标准坐标系</p>
<p>我们先看其他坐标系和标准坐标系的转换, 之后再来看任意两个坐标系之间的转换就会非常容易</p>
<p><strong>如何进行其他坐标系与标准坐标系之间的转换?</strong></p>
<p>还是这个例子:</p>
<p><img src="/Blog/intro/basis_space_and_e_1_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>(12, 8) 和 (2, 2) 之间有什么联系?</p>
<p>在讲矩阵的视角时就已经提到过</p>
<p><img src="/Blog/intro/basis_space_and_e_1_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>就相当于在(4, 1)的方向上走两步, 再向(2, 3)的方向走两步, 最终所得到的点的位置</p>
<p>由于(4, 1) 和 (2, 3) 都是在标准坐标系下衡量的, 所以我们最终得到的点坐标就是标准坐标系下的点坐标</p>
<p>总结一下:</p>
<p><img src="/Blog/intro/basis_space_and_e_1_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意: B 和 PB 是两个东西, B是一个集合, 因为它是一组基, 包含n个向量</p>
<p>PB是一个矩阵</p>
<p>我们称矩阵PB为: <strong>坐标转换矩阵</strong></p>
<p><br></p>
<h3 id="更加严谨的证明"><a href="#更加严谨的证明" class="headerlink" title="更加严谨的证明"></a>更加严谨的证明</h3><p><img src="/Blog/intro/basis_space_and_e_1_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="反过来转换"><a href="#反过来转换" class="headerlink" title="反过来转换"></a>反过来转换</h3><p>现在我们已知这个式子</p>
<p><img src="/Blog/intro/basis_space_and_e_1_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>要想让标准坐标系的坐标转换为其他坐标系的坐标非常简单, 只要同时左乘一下PB的逆</p>
<p><img src="/Blog/intro/basis_space_and_e_1_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>为什么我们可以确信矩阵PB可逆?</p>
<p>因为PB的列向量是线性无关的, 同时也是生成空间的, 因为他们是一组基, 这等价于PB矩阵可逆</p>
<p><br></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>对于给定的任意一个坐标系, 就可以用这种方式, 和标准坐标系中的坐标进行转换</p>
<p><img src="/Blog/intro/basis_space_and_e_1_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h2 id="任意坐标系转换"><a href="#任意坐标系转换" class="headerlink" title="任意坐标系转换"></a>任意坐标系转换</h2><p><img src="/Blog/intro/trans_coord_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>就是这个公式</p>
<p><img src="/Blog/intro/trans_coord_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>Pc的逆乘以Pb的结果也是一个矩阵, 我们便可以把这个矩阵称为P从b到c</p>
<p><img src="/Blog/intro/trans_coord_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>其实我更喜欢  Pc <em> [x]c = Pb </em> [x]b  这种写法</p>
<p><img src="/Blog/intro/trans_coord_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
</blockquote>
<p><img src="/Blog/intro/trans_coord_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里关注一下这两个矩阵</p>
<p><img src="/Blog/intro/trans_coord_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这两个矩阵 P从b到c, 和P从c到b 互为逆</p>
<p><br></p>
<p>我们可以用矩阵逆的定义来证明 (A乘A的逆等于单位矩阵), 但是这里我们使用另一种方式</p>
<p><img src="/Blog/intro/trans_coord_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>标准坐标系也是坐标系, 所以也满足上面的性质</p>
<p><img src="/Blog/intro/trans_coord_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>它只是任意坐标系转换的一个特例</p>
<p><br></p>
<p>在处理任意坐标系向标准坐标系的转换时, B = {b1, b2…bn} 这些基向量本质是, 我们知道这些<strong>基向量在我们要转到的坐标系 (标准坐标系) 的转换方式</strong></p>
<p><img src="/Blog/intro/trans_coord_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此</p>
<p><img src="/Blog/intro/trans_coord_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>或者用另一种思考/证明方法</p>
<p><img src="/Blog/intro/trans_coord_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>例如:</p>
<p><img src="/Blog/intro/trans_coord_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>如果我们在处理两个基的转换时, 已经知道一组基在另外一个坐标系的表示的话, 就可以不以单位坐标系作为桥梁了, 它们之间已经有桥梁了, 直接使用就好了</p>
<p><br></p>
<p><br></p>
<h2 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h2><p>前面几个小节一直在关注坐标系的<strong>转换</strong>, 而在线性代数领域, 关于变换这个话题, 我们会关注一个更加宏观, 更加庞大的定义: <strong>线性变换</strong></p>
<p>而我们之前提到的坐标系转换, 也只是线性变换的一种.</p>
<p><br></p>
<h3 id="什么是线性变换"><a href="#什么是线性变换" class="headerlink" title="什么是线性变换?"></a>什么是线性变换?</h3><p>在研究线性变换之前, 先看看什么是变换. </p>
<p>通常在数学中, <strong>变换是一个函数</strong></p>
<p>而在线性代数领域中, 我们对 “<strong>线性</strong>“ 更为关注, 因此我们主要研究的就是<strong>线性变换</strong></p>
<p><img src="/Blog/intro/linear_transform.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意: u和v都是向量, 因为在线性代数中, 我们只对向量空间感兴趣</p>
<p>再一次呈现了向量的两种基本运算, 向量的加法, 和向量的数量乘法</p>
<p><br></p>
<blockquote>
<p>线性变换是一个动作, 它本身就是非常抽象的. 且线性代数领域研究的是向量空间, 向量空间是非常广阔的, 欧几里得空间只是向量空间中的一种, 配合不同的向量空间 “什么是线性变换” 就变得更加抽象, 而在这里我们只探讨欧几里得空间的线性变换</p>
</blockquote>
<p>在欧几里得空间中, 我们可以简单地把矩阵所表示的变换, 理解为是线性变换. 我们看待矩阵的视角中, 有一个就是把<strong>矩阵看作是向量的函数</strong></p>
<p>其实在线性代数领域中, 我们所研究的线性变换总能够找到一个矩阵, 和这个变换相对应</p>
<p>我们研究清楚了矩阵, 其实就研究清楚了线性代数领域的变化</p>
<blockquote>
<p>线性变换可能发生维度变化, 例如把一个三维向量线性变换维二维向量</p>
<p>我们称一个把n维向量变换到n维向量的线性变换为: 线性算子(linear operator)</p>
<p>T:R<sup>n</sup> -&gt; R<sup>n</sup></p>
</blockquote>
<p><br></p>
<h3 id="证明-矩阵乘以向量这样的变换为线性变换"><a href="#证明-矩阵乘以向量这样的变换为线性变换" class="headerlink" title="证明: 矩阵乘以向量这样的变换为线性变换"></a>证明: 矩阵乘以向量这样的变换为线性变换</h3><p><img src="/Blog/intro/linear_transform_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="证明-一个线性变换对应一个矩阵"><a href="#证明-一个线性变换对应一个矩阵" class="headerlink" title="证明: 一个线性变换对应一个矩阵"></a>证明: 一个线性变换对应一个矩阵</h3><p><img src="/Blog/intro/proof_linear_tr_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>也就是说对于任意对于向量x的线性变换都可以写作Ax这样一个矩阵乘以x的形式.</p>
<p>矩阵A就代表了这个线性变换, 且矩阵A的每一个列向量都是标准正交基进行线性变换后的结果</p>
<p><img src="/Blog/intro/proof_linear_tr_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>换句话说, 如果知道这个空间的基是怎么进行线性变换的, 我们就知道这个空间任意一个向量的变换方式</p>
<p><br></p>
<h3 id="空间就是变换-矩阵可以表示变换"><a href="#空间就是变换-矩阵可以表示变换" class="headerlink" title="空间就是变换, 矩阵可以表示变换"></a>空间就是变换, 矩阵可以表示变换</h3><p>一个矩阵, 既可以看作是空间, 又可以看作是变换, 这两个视角本质是一样的</p>
<p><img src="/Blog/intro/linear_transform_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>当我们把矩阵看作是空间, 其实描述的是这个空间的坐标系</p>
<p>而这个矩阵本身又是P从B到ε的坐标变换矩阵, 这个矩阵本身就表示一个变换</p>
<p>而在课程之前就是使用这个矩阵, 和一个向量相乘得到另外一个向量</p>
<p>我们说这个过程是把同样一个向量从一个坐标系的表示, 转换成另外一个坐标系的表示, 我们也可以把这个过程看作是一个变换</p>
<p><br></p>
<p>空间就是变换, 不同的空间对应着不同的基, 对应着不同的坐标系, 进行矩阵乘法就可以看作是不同的基之间的变换</p>
<p>反过来一个变换肯定对应着不同的基和不同的坐标系</p>
<p><br></p>
<p><img src="/Blog/intro/linear_transform_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>T的列向量就是另外一组基, 这个坐标系下的(x, y) 对应到标准坐标系下就是(-x, -y)</p>
<p>同理:</p>
<p><img src="/Blog/intro/linear_transform_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们说这个变换本质就是对应另外一组基, 另外一个坐标系</p>
<p><strong>就是看 x, y 在T的列向量的坐标系下对应在标准坐标系下的位置</strong></p>
<p><br></p>
<p><strong>所有的矩阵都可以用来表示一个线性变换</strong></p>
<p><strong>用矩阵表示空间的视角和用矩阵表示变换的视角是等价的</strong></p>
<p><br></p>
<p><br></p>
<h2 id="更多和坐标转换和线性变换相关的话题"><a href="#更多和坐标转换和线性变换相关的话题" class="headerlink" title="更多和坐标转换和线性变换相关的话题"></a>更多和坐标转换和线性变换相关的话题</h2><p>本章我们首先讲解了<strong>坐标转换</strong>, 理解坐标系和空间的基之间成一一对应的关系. 空间的不同基之间的转换就是坐标转换</p>
<p>在此基础上我们又了解了什么是<strong>线性变换</strong></p>
<p>而坐标转换和线性变换都可以看作是理解矩阵的一个视角, 这两个视角本质是一致的</p>
<p>注意: 本章中给出的例子其实都是在同等空间中(一个空间中)进行转换</p>
<p>都是在二维平面中, 从一点转化到另外一点</p>
<p><br></p>
<p>这种转换有非常多的应用, 最简单的例子就是动画</p>
<p><img src="/Blog/intro/linear_transform_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>动画之所以能动起来, 是动画中的每一个物体都有着自己的一个坐标系, 这个坐标系在不断地转换</p>
<p>不过在这里我们说的动画相应的坐标转换还是在一个同维度的坐标中转换, 二维动画所有点都在二维空间中, 所有点都在二维空间中进行变换. 三维动画, 所有点都在三维空间中进行变换.</p>
<p>然而在实际应用中, 我们也需要不同维度的空间中进行转换:</p>
<p>最简单的, 当人看 3D动画 / 或者三维东西, 这个过程就是3D空间转换到2D空间的过程</p>
<p><img src="/Blog/intro/linear_transform_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>反过来, 从低维空间向高维空间转换也是有意义的, 对此最典型的应用领域就是计算机视觉. 就是机器看到我们的世界从而提取出信息, 计算机通过二维图像恢复三维世界中的信息</p>
<p><img src="/Blog/intro/linear_transform_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>同维度的转换也是有意义的, 其中一个应用就是压缩</p>
<p><img src="/Blog/intro/linear_transform_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>假设我们用x, y 来表示每个点的位置, 这两个坐标轴都生成了这个空间, 都是这个二维空间的一组基, 但是红色的坐标轴来表示点的话, 这组数据就产生了一个特点:</p>
<p>这组数据在红色坐标系y轴的位置所对应的值都特别小, 换句话说, 这些数据在这个红色的坐标系下, 在y轴方向的信息量是非常低的, 他们主要的信息都在x轴方向上</p>
<p>如果我们求出来红色坐标轴, 每一个数据点就可以只用一个数字(红色坐标轴的x)来表示, 而y的信息我们直接扔掉, 这组数据我们近乎把它压缩了一倍</p>
<p><strong>很多压缩算法的本质, 就是找一组基,</strong> 用这组基来表示数据很多信息可以扔掉, 而不影响我们对数据的理解, JPEG就是使用这样的压缩算法</p>
<blockquote>
<p>傅里叶变换, 小波变换 </p>
<p>都可以用于进行压缩, 他们的区别就是在于所找的基不一样</p>
<p>这两种变换处理可以压缩, 还可以用来描述我们要分析的图像更多的特征, 这些特征是我们直接看这些图片看不出来的</p>
<p>这是因为我们直接看图片所使用的基/坐标系和进行傅里叶变换/小波变换所使用的坐标系不同了</p>
</blockquote>
<p><br></p>
<p><br></p>
<h1 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h1><p><br></p>
<h2 id="什么是行列式"><a href="#什么是行列式" class="headerlink" title="什么是行列式"></a>什么是行列式</h2><p>行列式是<strong>方阵</strong>的一个属性</p>
<p>什么是属性?</p>
<ul>
<li>研究数: 数的大小, 符号, 约数, 是否为素数?</li>
<li>研究角度: sin, cos, tan, ctg…</li>
<li>研究多项式:  ax^2 + bx + c 项数, 最高次的次数, 多项式为0的解</li>
</ul>
<p><br></p>
<p>矩阵可以表示一组向量, 方阵表示n个n维向量</p>
<p>矩阵不仅仅可以表示这个空间中的一组基, 还可以表示这个空间的这组基所对应的坐标转换</p>
<p>然而有一个问题我们始终忽略了, 下图有三组基, 都可以表示整个二维空间, 这三组基有什么不同?</p>
<p> <img src="/Blog/intro/basis_dif_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>显然, 这三组基的每一个基向量都不一样, <strong>我们可不可以只使用一个数就能一定程度的表示出它们的不同, 或者一定程度地刻画出它们的某一个特征?</strong> 可以的</p>
<p>每两个向量都组成了一个<strong>面积</strong>, 例如标准基组成了红色区域的面积, 绿色的基组成了绿色的面积, 蓝色的基组成的平行四边形也拥有面积</p>
<p><img src="/Blog/intro/basis_dif_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这些面积的大小就可以一定程度刻画出不同的基的特征</p>
<p><strong>行列式就是描述这n个n维向量所对应的面积的大小</strong>, 面积是基于二维平面空间说的, 对于三维平面空间三个三维向量组成的是一个体</p>
<p><img src="/Blog/intro/basis_dif_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="从二维空间入手看如何求行列式"><a href="#从二维空间入手看如何求行列式" class="headerlink" title="从二维空间入手看如何求行列式"></a>从二维空间入手看如何求行列式</h3><p>现在有两个二维向量(a, b), (c, d)</p>
<p><img src="/Blog/intro/basis_det_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<h4 id="表示法"><a href="#表示法" class="headerlink" title="表示法"></a>表示法</h4><p><img src="/Blog/intro/basis_det_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>det是行列式(Determinant)的缩写, 这个方阵对于这两个向量是<strong>按行排列的</strong></p>
<blockquote>
<p>在后面我们会发现, 对于行列式来说, 按行排列还是按列排列都是可以的, 不过按行排列可以方便我们讲解它的性质</p>
</blockquote>
<p>出了上面的表示法之外还可以这样表示</p>
<p><img src="/Blog/intro/basis_det_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>注意, 当看文献时, 遇见 |a|, 一定要分清a是一个矩阵还是一个数, 因为绝对值和行列式的值是不同的概念</p>
<p>行列式的结果也是一个数, 是这n个向量在高维体所对应的(有向的)体积, 我们得到的数有可能是负数</p>
</blockquote>
<p><br></p>
<h4 id="如何计算"><a href="#如何计算" class="headerlink" title="如何计算"></a>如何计算</h4><p><img src="/Blog/intro/basis_det_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>若想求这个面积, 我们可以作一些辅助线, 用大的面积减去几块不需要的小面积</p>
<p><img src="/Blog/intro/basis_det_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>而如果我们先排列(c, d), 在排列(a, b)</p>
<p><img src="/Blog/intro/basis_det_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>换句话说, 当我们交换行列式排列的顺序时, 行列式的值变符号了</p>
<p><img src="/Blog/intro/basis_det_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>行列式表示向量组在空间中形成的<strong>有向</strong>体积</p>
<p><img src="/Blog/intro/basis_det_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>如果把二维平面想象成纸片, 第二种情况就是把这张纸片翻过来放置</p>
<p>在三维及以上的空间, 体积的方向将变得极其复杂</p>
<p>简单来说, 在行列式中, 向量排列的顺序是有意义的, 交换两行, 则行列式的值取反, 这个结论在n阶行列式 </p>
<p><br></p>
<p><br></p>
<h2 id="行列式的四大基本性质"><a href="#行列式的四大基本性质" class="headerlink" title="行列式的四大基本性质"></a>行列式的四大基本性质</h2><h3 id="前两个性质"><a href="#前两个性质" class="headerlink" title="前两个性质"></a>前两个性质</h3><ul>
<li>单位矩阵对应的行列式的值为1, 即: detI = 1</li>
<li>交换行列式的两行, 则行列式的值取反</li>
</ul>
<p><br></p>
<h3 id="第三个性质"><a href="#第三个性质" class="headerlink" title="第三个性质"></a>第三个性质</h3><p>方阵的<strong>某一行</strong>乘以一个数k, 则其对应的行列式也缩放了k倍</p>
<p><img src="/Blog/intro/4_properity_det.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/4_properity_det_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意: 是某一行, 而不是所有行</p>
<p><img src="/Blog/intro/4_properity_det_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>n是A的行数</p>
<p><br></p>
<h3 id="第四个性质"><a href="#第四个性质" class="headerlink" title="第四个性质"></a>第四个性质</h3><p><img src="/Blog/intro/4_properity_det_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>图形化的表达方法如下</p>
<p><img src="/Blog/intro/4_properity_det_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>或者可以使用纯代数方法计算</p>
<p><img src="/Blog/intro/4_properity_det_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<figure class="highlight lisp"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs lisp">left = (<span class="hljs-name">a</span> + a&#x27;)d - (<span class="hljs-name">b</span> + b&#x27;)c<br>	 = ad + a&#x27;d - bc - b&#x27;c<br>	 = (<span class="hljs-name">ad</span> - bc) + (<span class="hljs-name">a</span>&#x27;d - b&#x27;c)<br>	 = right<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/4_properity_det_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>依然是加法和数量乘法, 不过是行列式的加法和数量乘法</p>
<p><br></p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>四条基本性质</p>
<p><img src="/Blog/intro/4_properity_det_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h2 id="行列式与矩阵的逆"><a href="#行列式与矩阵的逆" class="headerlink" title="行列式与矩阵的逆"></a>行列式与矩阵的逆</h2><p>下面就要用上面的四个基本性质得到一下有用的结论</p>
<p><br></p>
<h3 id="如果行列式的两行相同-则行列式的值为0"><a href="#如果行列式的两行相同-则行列式的值为0" class="headerlink" title="如果行列式的两行相同, 则行列式的值为0"></a>如果行列式的两行相同, 则行列式的值为0</h3><p><img src="/Blog/intro/4_properity_det_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>或者直观地想: </p>
<p>二维空间: 两个向量共线, 面积为0</p>
<p>三维空间, 两个向量一样, 三个向量只能形成一个平面, 体积为0</p>
<p>n维空间中, 两个向量一样, 只能形成n-1维的体, 这个体对于n维空间来说, 它的体积为0</p>
<p><br></p>
<p>基于此, 我们又可以得出一个结论:</p>
<h3 id="如果行列式的一行是另一行的k倍-则行列式的值为0"><a href="#如果行列式的一行是另一行的k倍-则行列式的值为0" class="headerlink" title="如果行列式的一行是另一行的k倍, 则行列式的值为0"></a>如果行列式的一行是另一行的k倍, 则行列式的值为0</h3><p>前一个证明过程对于这条性质同样成立</p>
<p><img src="/Blog/intro/4_properity_det_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="如果行列式有一行为0-则行列式的值为0"><a href="#如果行列式有一行为0-则行列式的值为0" class="headerlink" title="如果行列式有一行为0, 则行列式的值为0"></a>如果行列式有一行为0, 则行列式的值为0</h3><p>有一行为0是什么意思? 即有一个向量是0向量, 剩下的n-1个向量对多只有可能形成一个n-1维的体.</p>
<p>换句话说, 三个三维向量, 有一个向量是零向量, 其余两个只能组成一个二维平面</p>
<p>代数证明:</p>
<p><img src="/Blog/intro/4_properity_det_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>利用上一条性质证明: <strong>如果行列式的一行是另一行的k倍, 则行列式的值为0</strong></p>
<p><img src="/Blog/intro/4_properity_det_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>前面证明这么多性质, 其实都指向这个<strong>行列式的值为0</strong></p>
<p>这些性质不管它前面的条件是怎样的, 对应到行列式的几何解释中, 其实都是说<strong>这n个向量不能构成这n维空间中的一组基</strong></p>
<p>这n个向量, 至少有一个向量没有额外的信息, 使得这n个向量在n维空间中构成的体, 实际上退化为了一个n-1维的体, 这n-1维的体从n维空间的角度来看, 它的体积为0</p>
<p><br></p>
<p>由此我们可以得到:</p>
<h3 id="如果行列式的一行是其他行的线性组合-行列式的值为0"><a href="#如果行列式的一行是其他行的线性组合-行列式的值为0" class="headerlink" title="如果行列式的一行是其他行的线性组合, 行列式的值为0"></a>如果行列式的一行是其他行的线性组合, 行列式的值为0</h3><p><img src="/Blog/intro/4_properity_det_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>首先可以把这个行列式拆成两个行列式的加和, 而新拆成的两个行列式都为0.</p>
<p>对于第一个行列式, 第三行是第一行的k1倍, 对于第二个行列式, 第三行是第二行的k2倍. 所以两个行列式的值都为0</p>
<p><br></p>
<p>之前讲的众多性质, 本质上和这个性质是一样的, 上面三条性质都可以归为这一条性质</p>
<p><img src="/Blog/intro/4_properity_det_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="行列式和矩阵的逆的联系"><a href="#行列式和矩阵的逆的联系" class="headerlink" title="行列式和矩阵的逆的联系"></a>行列式和矩阵的逆的联系</h3><p>如果行列式的一行是其他行的线性组合, 行列式的值为0</p>
<p>它的本质就是一个向量组, n个n维向量一行一行排列起来</p>
<p>如果某一行是其他一行的线性组合, 则这个向量组是线性相关的, 如果线性相关矩阵是不可逆的</p>
<p><br></p>
<p><img src="/Blog/intro/4_properity_det_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>即:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">det</span><span class="hljs-params">(A)</span></span> = <span class="hljs-number">0</span>  iff A不可逆<br><br><span class="hljs-function"><span class="hljs-title">det</span><span class="hljs-params">(A)</span></span> != <span class="hljs-number">0</span> iff A可逆<br></code></pre></td></tr></table></figure>
<p>因此, 对于方阵A可逆的等价命题可以再添加一个:</p>
<p><img src="/Blog/intro/4_properity_det_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h2 id="计算行列式的值"><a href="#计算行列式的值" class="headerlink" title="计算行列式的值"></a>计算行列式的值</h2><p>在计算行列式的值之前, 先证明一个性质:</p>
<p><strong>如果一个行列式的一行加(减)另一行的k倍, 行列式的值不变</strong>, 即:</p>
<p><img src="/Blog/intro/cal_Det.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>首先根据加法的性质, 我们可以把行列式拆成两个部分:</p>
<p><img src="/Blog/intro/cal_Det_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而第二个行列式的值为0, 因为根据上一小结讲解的性质: <strong>如果行列式的一行是其他行的线性组合, 行列式的值为0.</strong></p>
<p><img src="/Blog/intro/cal_Det_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而某一行加减另一行的k倍不就是Gauss消元吗 (注: 高斯消元法得到的结果和原来的方阵得到的行列式的值是一样的), 之后又可以反向进行Gauss-Jordan消元</p>
<p>注意: 有两点和之前的Gauss-Jordan不同: </p>
<ol>
<li>这里个Gauss-Jordan消元<strong>不能进行归一化</strong>操作, 因为进行归一化操作其实行列式的值就改变了, 因为归一化的本质就是某一行乘以某一个常数k, 因此相应的行列式的值就应该乘以1/k</li>
<li>行置换/列置换操作要<strong>变符号</strong></li>
</ol>
<p>但如果单纯的让某一行加(减)另一行的k倍, 行列式的值不变</p>
<p>如果消元结果有零行, 行列式的值为0</p>
<p>Gauss消元后我们会得到一个<strong>上三角矩阵U</strong>, 在进行Gauss-Jordan消元法得到一个<strong>对角矩阵D</strong></p>
<p><br></p>
<h3 id="计算对角矩阵的行列式"><a href="#计算对角矩阵的行列式" class="headerlink" title="计算对角矩阵的行列式"></a>计算对角矩阵的行列式</h3><p>假设我们的对角矩阵是n阶的对角矩阵</p>
<p><img src="/Blog/intro/cal_Det_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们把d1提出来, 想象成d1乘以一个行列式:</p>
<p><img src="/Blog/intro/cal_Det_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同理, 这个操作我们也可以对第二行乃至第n行进行</p>
<p><img src="/Blog/intro/cal_Det_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>单位矩阵的行列式的值为1, 所以就等于d1一直乘到dn</p>
<p><img src="/Blog/intro/cal_Det_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>因此对于对角矩阵, 它的行列式的计算方式就是<strong>把对角线上的值进行相乘</strong></p>
<p><img src="/Blog/intro/cal_Det_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同时, 因为上三角矩阵进行Gauss-Jordan消元时把上三角矩阵化为单位矩阵时, 这个过程对<strong>主对角线没有任何影响</strong>, 因此上三角矩阵的行列式也为d1…dn</p>
<p><img src="/Blog/intro/cal_Det_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同理, 下三角矩阵也可以进行消元(某一行减去另外一行的k倍), 而且不影响对角线上的元素, 因此一个下三角矩阵对应的行列式的值也是<strong>主对角线上所有元素的乘积</strong></p>
<p><img src="/Blog/intro/cal_Det_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>因此对于三角矩阵, 行列式的值就是<strong>主对角线上所有元素的乘积</strong></p>
<p><img src="/Blog/intro/cal_Det_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>有了这个结论, 再看求一个方阵行列式的过程:</p>
<p><img src="/Blog/intro/cal_Det_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>只要进行Gauss消元就够了</p>
<p><br></p>
<h3 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结:"></a>算法总结:</h3><p>其实 有全零行行列式的值为0 也可以归为 对角线的乘积</p>
<p><img src="/Blog/intro/cal_Det_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="验证算法"><a href="#验证算法" class="headerlink" title="验证算法:"></a>验证算法:</h3><p><img src="/Blog/intro/cal_Det_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>和我们之前使用几何的方法求得行列式的值是一致的</p>
<p><br></p>
<p><br></p>
<h2 id="初等矩阵与行列式"><a href="#初等矩阵与行列式" class="headerlink" title="初等矩阵与行列式"></a>初等矩阵与行列式</h2><p>这一小节, 我们要探讨行列式的一个稍微有些不一样的性质:</p>
<p><img src="/Blog/intro/det_pro.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>为了探讨清楚这个性质, 我们要研究一下初等矩阵对应的行列式</p>
<p><br></p>
<p>为什么这个性质不太一样? 因为前面的几个性质都可以从行列式的<strong>几何</strong>属性出发, 相应得到一个直观的理解: <strong>行列式表示的就是在n维空间中n维多面体的有向体积</strong></p>
<p>但是对于这个性质, 这个几何理解稍微有些没用了, 因为等式的右边是两个行列式的乘积, 从几何的角度理解就是两个体积的乘积, 而两个体积的乘积的几何意义就不是特别明确了</p>
<p>因此, 为了证明这个性质, 我们要从纯代数的角度出发</p>
<p><br></p>
<h3 id="如果A或者B中的某一行和其他行线性相关"><a href="#如果A或者B中的某一行和其他行线性相关" class="headerlink" title="如果A或者B中的某一行和其他行线性相关"></a>如果A或者B中的某一行和其他行线性相关</h3><p><img src="/Blog/intro/det_pro.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>如果A或者B中的某一行和其他行线性相关, 此时</p>
<p><img src="/Blog/intro/det_pro_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因为如果A中的某一行和其他行线性相关, A中的这一行可以写为其他行的线性组合, 而在进行乘法的过程中, 这一行中的所有的元素会和B中的某一些元素进行相乘再相加. 这个过程下, A中这一行的所有元素一定还保持着是其他行的线性组合, 只不过前面的系数会产生一些变化, 但是这个线性组合的线性关系依然存在, 因此行列式的值为0, 同理如果B中有一行和其他行线性相关, 也是如此</p>
<blockquote>
<p>我更喜欢另一种理解方式:</p>
<p>如果A或B中某一行和其他行线性相关, 则它不能表达为一系列初等矩阵的乘积, 而是一系列初等矩阵乘以一个非初等矩阵, </p>
<p>因此将其代入, A乘B同样不能表达为一系列初等矩阵的乘积, 即AB不可逆, 即AB的行向量线性相关, 即det(AB) = 0</p>
</blockquote>
<p>此时等式的左右两侧都为0</p>
<p><br></p>
<h3 id="如果A和B中的所有行都线性无关"><a href="#如果A和B中的所有行都线性无关" class="headerlink" title="如果A和B中的所有行都线性无关"></a>如果A和B中的所有行都线性无关</h3><p>如果A和B中的所有行都线性无关, 而A和B都是方阵, 这就说明这个方阵可以表示成一系列初等矩阵的乘积</p>
<p><img src="/Blog/intro/det_pro_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>回忆: 初等矩阵: 对单位矩阵进行初等行操作</p>
<p>一个初等矩阵乘以某一个矩阵, 就相当于是对这个矩阵进行初等矩阵对应的初等行操作</p>
<p>因此我们可以拆解矩阵A</p>
<p><img src="/Blog/intro/det_pro_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此我们要研究一下初等矩阵的行列式</p>
<p><br></p>
<h4 id="初等矩阵的行列式"><a href="#初等矩阵的行列式" class="headerlink" title="初等矩阵的行列式"></a>初等矩阵的行列式</h4><p>初等矩阵的行列式非常简单, 近乎就是之前讲的一个一个性质</p>
<p><img src="/Blog/intro/det_pro_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="先证明初等矩阵乘以B的行列式"><a href="#先证明初等矩阵乘以B的行列式" class="headerlink" title="先证明初等矩阵乘以B的行列式"></a>先证明初等矩阵乘以B的行列式</h3><p>先证明这样一个命题, 一个初等矩阵E 和 另外一个矩阵B满足</p>
<p><img src="/Blog/intro/det_pro_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>证明这个结论就简单很多, 因为初等矩阵E乘以矩阵B本质就是对B进行一次行变换而已, 所以我们完全可以把E乘B看作一个矩阵</p>
<p><strong>情况1:</strong></p>
<p><img src="/Blog/intro/det_pro_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><strong>情况2:</strong></p>
<p><img src="/Blog/intro/det_pro_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><strong>情况3:</strong></p>
<p><img src="/Blog/intro/det_pro_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此: 这个命题是正确的</p>
<p><img src="/Blog/intro/det_pro_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>对于一个E乘以B我们有这个结论, 那么对于一串E乘以B我们也能得到类似的结论:</p>
<p><img src="/Blog/intro/det_pro_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>思路就是不断使用上面那个结论</p>
<p><img src="/Blog/intro/det_pro_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="再证明A乘以B的行列式"><a href="#再证明A乘以B的行列式" class="headerlink" title="再证明A乘以B的行列式"></a>再证明A乘以B的行列式</h3><p><img src="/Blog/intro/det_pro_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此</p>
<p><img src="/Blog/intro/det_pro_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>这样我们就通过研究初等矩阵的行列式证明出任意两个矩阵A, B; <strong>det(AB) = det(A) * det(B)</strong></p>
<p><br></p>
<h3 id="应用这个结论回顾前面的知识点"><a href="#应用这个结论回顾前面的知识点" class="headerlink" title="应用这个结论回顾前面的知识点"></a>应用这个结论回顾前面的知识点</h3><p><img src="/Blog/intro/det_pro.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于这个式子, 我们把B换做是A的逆</p>
<p><img src="/Blog/intro/det_pro_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个结论意味着什么? </p>
<p>首先给了我们一个计算A的逆的行列式的值的方式</p>
<p>更重要的一点是: det(A)的值放在分母的位置, 这就意味着如果我们想求A的逆的行列式的值, det(A)不能为0</p>
<p>换句话说, 如果det(A) = 0, A的逆的行列式的值不存在, 即: A的逆不存在</p>
<p>我们又从另外一个角度证明出来了行列式与矩阵的逆的关系得到的非常重要的结论:</p>
<p>如果矩阵A存在逆, det(A) != 0</p>
<p><br></p>
<p>等价命题列表:</p>
<p><img src="/Blog/intro/det_pro_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>行列式我们一直用行的角度讨论, 下一小节就将用这一小结证明的性质, 来说明用列的方式讨论行列式完全等价于用行的方式讨论行列式</p>
<p><br></p>
<p><br></p>
<h2 id="行式就是列式"><a href="#行式就是列式" class="headerlink" title="行式就是列式!"></a>行式就是列式!</h2><p>之前介绍行列式都是从行的角度来看方阵的, 例如: 交换两行, 某一行加减另外一行的k倍, 就连几何的意义也是按行码起来看的</p>
<p>但是对于行列式来说, 行式就是列式, 从无论从行的角度看, 还是从列的角度看, 最终归到行列式的值来说, 这个值是不变的 </p>
<p><img src="/Blog/intro/det_row_col.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>为了证明这个结论, 首先要用一下更早之前的关于矩阵分解的知识</p>
<p>QR分解和LU分解都有条件:</p>
<ul>
<li>QR分解的前提条件就是每一列所对应的向量线性无关, 显然矩阵A不能保证这个条件</li>
<li>LU分解的前提条件是在Gauss消元法的过程中不能进行行变换或者列变换</li>
</ul>
<p>不过在在将LU分解之后, 提出来LU分解的一个变形, 即把矩阵分解为<strong>PLUP’</strong> 这四个矩阵相乘的形式, 矩阵P就是行变换矩阵, 矩阵P’就是列变换矩阵</p>
<p>而对于任意矩阵, 都是可以分解为PLUP’形式的</p>
<p>而这就为我们证明行式是列式提供了帮助</p>
<p><br></p>
<h3 id="证明"><a href="#证明" class="headerlink" title="证明:"></a>证明:</h3><p><img src="/Blog/intro/det_row_col_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>又根据我们上一小结证明的性质, 可以拆开成</p>
<p><img src="/Blog/intro/det_row_col_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>反过来对于A的转置的行列式的值就是</p>
<p><img src="/Blog/intro/det_row_col_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>下面就来看看这两个形式是否相等</p>
<p>L的行列式和L的转置的行列式的值是相等的, 因为我们证明过<strong>一个下三角矩阵的行列式的值就等于主对角线上元素的乘积</strong></p>
<p>而一个下三角矩阵转置之后, 是一个上三角矩阵, 这个上三角矩阵的行列式的值也等于它主对角线上元素的乘积, 而转置的过程中, <strong>主对角线上的元素完全没有变</strong></p>
<p>同理U和U的转置的行列式的值也一样</p>
<p><br></p>
<p>再看P和P’两部分, 它们是两个从单位矩阵进行(行变换/列变换)后得到的矩阵</p>
<p>所以为了证明 det(P) = det(P的转置), det(P’) = det(P’的转置), 我们要证明一个辅助的结论</p>
<p><br></p>
<p>辅助证明的结论:</p>
<p><img src="/Blog/intro/det_row_col_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于这个辅助证明, 和上一小节一样, 分三种情况讨论就可以了</p>
<p>然而, P 和 P’ 所执行的只有行交换操作(P’的列交换也可以由行交换得到), 因此只要证明E表示两行交换位置的情况下, E的行列式等于E的转置的行列式</p>
<p>这个是显然相等的, 例如:</p>
<p><img src="/Blog/intro/det_row_col_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里交换了单位矩阵的两行, 这个行列式的值就为-1</p>
<p>而交换两行后在进行转置, 转置的结果还是两行交换了位置, 所以E的转置的行列式的值也是-1</p>
<p>因此:</p>
<p><img src="/Blog/intro/det_row_col_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>而P和P’在这个式子中, 表示的是多次行交换操作</p>
<p><img src="/Blog/intro/det_row_col_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此可以把P或P’分解为多个初等矩阵的乘积的结果, 而每个初等矩阵都等于它的转置, 所以合起来就是P或P’的转置, P或P’的行列式等于P或P’的转置的行列式</p>
<p><br></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论:"></a>结论:</h3><p>所以我们就得到了</p>
<p><img src="/Blog/intro/det_row_col_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>虽然证明过程很麻烦, 但最终得到的结论是非常简洁的</p>
<p>行式就是列式, 对于一组向量, 我们横着码求行列式或竖着码是完全一样的</p>
<p>这就意味着: 我们之前讲的所有性质, 都是基于行的. 换成列一样存在</p>
<p><img src="/Blog/intro/det_row_col_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/det_row_col_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们之前所讲和矩阵的基相关的所有的内容, 我们总是原意将基向量一列一列地排列, 现在我们便知道这样一列一列排列行列式的这些性质依然是保留的, 我们后面把空间的基和行列式结合起来就方便了很多</p>
<p><br></p>
<p><br></p>
<h2 id="华而不实的行列式的代数表达"><a href="#华而不实的行列式的代数表达" class="headerlink" title="华而不实的行列式的代数表达"></a>华而不实的行列式的代数表达</h2><p>几何含义不能帮我们求出三阶以上的行列式的值, 而在这一章介绍的计算行列式的<strong>算法</strong>, 这个算法是一个过程, 我们也没有用数学的方式直接写出来一个行列式的值到底是多少</p>
<p>实际上行列式是有一个代数表达的</p>
<p><img src="/Blog/intro/det_row_col_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而M又是这样定义的, 如果i取2, 则把对应的那一行一列删掉</p>
<p><img src="/Blog/intro/det_row_col_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>剩下n-1行, n-1列又可以构成一个行列式, M1i就是通过这个手段构造出来的n-1阶的行列式</p>
<p>这便是行列式的代数表达, 非常复杂, 但却非常合理.</p>
<p>因为行列式的集合含义是在n维空间中n维体的体积, 求这个体积的式子一定是一个非常复杂的式子</p>
<blockquote>
<p>若要严格地证明这个式子是怎么推导出来的是很复杂的一件事情, 甚至要想透彻地理解这个式子背后想表达的数学思想需要离散数学中的组合数学, 群论…</p>
<p>如果不考研, 了解即可, 使用这个式子求行列式反而有很多缺点</p>
<p>不过先说一下这个定义的优点: 这个定义是一个非常优美的递归定义</p>
<p>优美归优美, 但使用这个方式的时间复杂度为 O(n!), 近乎是时间复杂度最高的一种算法的, 比质数级的还高, 仅次于O(n^n)</p>
<p>但是对于这个定义, 由于它是非常优美的递归定义, 所以我们使用这个行列式的代数表达可以证明之前提到的所有性质, 并且很多证明本身是极其优美, 极其巧妙的</p>
</blockquote>
<p>A和M在数学中也有特有的称呼, M是余子式, A是代数余子式</p>
<p><img src="/Blog/intro/det_row_col_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>第一行的每个元素乘以它的代数余子式, 最后加和, 这就是行列式的代数表达</p>
<p>除了用第一行这么做, 也可以用第二行…, 甚至可以按列做, 用第一列, 第二列</p>
<p>选择一行或一列, 遍历依次取元素, 每个元素乘以对应的代数余子式, 再加起来</p>
<p>因此选择哪一行或是那一列就有技巧了, 选择合适的行就可以更好计算</p>
<p><br></p>
<p>使用行列式的代数表达, 可以得到几何表达得不到的性质</p>
<p>通过几何性质我们得到过这个性质</p>
<p><img src="/Blog/intro/det_row_col_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于这个式子, 我们其实只能求出A的逆的行列式的值, 但求不出A的逆</p>
<p>但是通过行列式的代数表达, 我们就能直接写出A的逆</p>
<p><img src="/Blog/intro/det_row_col_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>A的逆等于A的行列式的倒数, 乘以矩阵A<em>(称为伴随矩阵), 这个矩阵的每一个元素都是矩阵A的元素对应的 <em>*代数余子式</em></em>,</p>
<blockquote>
<p>还是一个看起来优美, 简单, 但求起来极其复杂的式子</p>
</blockquote>
<p><br></p>
<h3 id="Cramer法则"><a href="#Cramer法则" class="headerlink" title="Cramer法则"></a>Cramer法则</h3><p>一般教材在介绍行列式时会介绍Cramer法则, 它所干的事情就是用行列式的方式求解 Ax=b 这个(方程组 / 线性系统)</p>
<p><img src="/Blog/intro/det_row_col_15.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>Cramer法则告诉我们, 对于一个线性方程组, 它的解长这样</p>
<p><img src="/Blog/intro/det_row_col_16.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/det_row_col_17.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>即: Ai(b)的行列式除以A的行列式的值</p>
<blockquote>
<p>同样, Cramer法则为什么成立, 它的推导还是很复杂的</p>
</blockquote>
<p><br></p>
<p>在现代数学中, 行列式更多是一个理论工具</p>
<p><br></p>
<p><br></p>
<h3 id="补充-当系数来自其他行-列时-行列式的代数表达式会发生什么"><a href="#补充-当系数来自其他行-列时-行列式的代数表达式会发生什么" class="headerlink" title="补充: 当系数来自其他行/列时, 行列式的代数表达式会发生什么?"></a>补充: 当系数来自其他行/列时, 行列式的代数表达式会发生什么?</h3><p>假设我们有矩阵A</p>
<p><img src="/Blog/intro/det_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时, 代数余子式的系数为A的第三行的各个元素, 因为我们选择第三行来计算行列式</p>
<p>而问题就是如果对于这个式子, 我们把它的系数从A的第三行改为A的第一行或第二行, 会发生什么?</p>
<p> <br></p>
<p>在回答这个问题之前我们先看一下另一个矩阵A’</p>
<p><img src="/Blog/intro/det_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>首先我们可以一眼确定 det(A’) = 0, 因为第一行和第三行一样.</p>
<p>然而 det(A’) 同时又等于 det(A) = (1 <em> C31) + (2 </em> C32) + (3 * C33) </p>
<p><br></p>
<p>A’ 的 C31 是等于 A 的 C31 的, 这是为什么? 因为计算C31时需要把这个元素的<strong>所在行和所在列都删去</strong></p>
<p>换句话说, <strong>计算某一行的代数余子式, 是和这一行的元素无关的</strong></p>
<p>因此, 我们知道, 对于矩阵A来说 <strong>(1 <em> C31) + (2 </em> C32) + (3 * C33) = 0</strong>, 因为这就相当于计算A’</p>
<p>即: 当我们选择矩阵的某一行计算代数余子式, 但却用另一行来当系数, 最后的结果为0</p>
<p><br></p>
<h3 id="补充-通过伴随矩阵求矩阵的逆的证明"><a href="#补充-通过伴随矩阵求矩阵的逆的证明" class="headerlink" title="补充: 通过伴随矩阵求矩阵的逆的证明"></a>补充: 通过伴随矩阵求矩阵的逆的证明</h3><p>注意: 这里不进行推导, 而是对已有的公式进行证明</p>
<p>前文讲到, 对于一个矩阵, 我们可以这样来求它的逆</p>
<p><img src="/Blog/intro/det_row_col_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于A的伴随矩阵我们使用 Adj(A) 来表示, 为 Adjoint 的缩写</p>
<p>也就是说我们要证明:  A的逆 = Adj(A) / det(A)</p>
<p>如果同时左乘A, 则      I = A * Adj(A) / det(A)</p>
<p>这等价于证明:             A <em> Adj(A) = det(A) </em> I</p>
<p><br></p>
<p>首先看等式右边, det(A) 乘以单位矩阵, 结果就是一个对角矩阵, 对角线上每一个元素都是det(A)</p>
<p><img src="/Blog/intro/dia_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>再看左边</p>
<p><img src="/Blog/intro/left_dia_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>结果也是一个矩阵, 根据点乘, A的第一行乘以Adj(A)的第一列为矩阵的第一行第一列元素, A的第二行乘以Adj(A)的第一列为矩阵的第二行第一列元素</p>
<p>又跟据上一个补充的性质, <strong>当我们选择矩阵的某一行计算代数余子式, 但却用另一行来当系数, 最后的结果为0</strong></p>
<p>也就是说, 当C的下标与a的下标不匹配时, 点乘结果为0. 而只有对角线上的结果匹配, 如果匹配点乘结果就是det(A), 即:</p>
<p><img src="/Blog/intro/dia_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此等式左边等于等式右边, 得证</p>
<p><br></p>
<h3 id="补充-Cramer法则的解释"><a href="#补充-Cramer法则的解释" class="headerlink" title="补充: Cramer法则的解释"></a>补充: Cramer法则的解释</h3><p>上文的Cramer法则给出这样一个公式</p>
<p><img src="/Blog/intro/det_row_col_16.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里解释一下这个公式是怎么得来的</p>
<p>首先我们通过上一个补充中, 求矩阵的逆的公式: A的逆 = Adj(A) / det(A)</p>
<p> 而我们要求解的系统是 Ax = b 这样的系统</p>
<p>同时左乘A的逆就是</p>
<p><img src="/Blog/intro/Cramer_proof.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>可以看到, 和我们要证明的结果对比只有分母不同, 分子都为det(A)或|A|, 下一步就是如何解释Adj(A) * b = A<sub>i</sub>(b)</p>
<p><img src="/Blog/intro/Cramer_proof_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而根据补充1的性质, <strong>第一行</strong>点乘后的结果就想当与把原本矩阵的<strong>第一列</strong>换成b再计算行列式的结果, 同理第n行也是</p>
<blockquote>
<p>注意: 伴随矩阵的排列的角标, 这就是为什么第一行点乘的结果和列换成b计算行列式对应</p>
</blockquote>
<p>而A<sub>i</sub>(b)的定义就是把第i列换为b计算行列式的结果, 也就是说: Adj(A) * b = A<sub>i</sub>(b)</p>
<p><br></p>
<h3 id="补充-叉积-Cross-Product"><a href="#补充-叉积-Cross-Product" class="headerlink" title="补充: 叉积(Cross Product)"></a>补充: 叉积(Cross Product)</h3><p>叉积是对三维空间中的两个向量的二元运算</p>
<p><img src="/Blog/intro/cross_product.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意这里和普通的行列式不一样, 第一行的每一个元素是向量, 因此结果也是一个向量</p>
<p>例如: 当u = (1, -2, 1) v = (2, 0 -2)</p>
<p>u x v = e<sub>1</sub>(4 - 0) - e<sub>2</sub>(-2-2) + e<sub>3</sub>(0+4) = 4 <em> e<sub>1</sub> + 4 </em> e<sub>2</sub> + 4 * e<sub>3</sub> = (4, 4, 4)</p>
<p>这个向量既和u正交, 又和v正交, 即和uv生成的平面正交</p>
<p>更多内容直接看<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/叉积">wiki百科</a></p>
<p> <br></p>
<p><br></p>
<h1 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h1><p>特征值和特征向量在线性代数领域非常重要</p>
<p><br></p>
<h2 id="什么是特征值和特征向量"><a href="#什么是特征值和特征向量" class="headerlink" title="什么是特征值和特征向量"></a>什么是特征值和特征向量</h2><p>在具体介绍什么是特征值和特征向量之前, 先从更高的角度看一下我们要研究的这个东西到底是什么</p>
<p>上一章曾经说过, 行列式可以理解为是方阵的一个属性, 而特征值和特征向量也是方阵的一个<strong>属性</strong></p>
<p>随着后续的深入, 就会明白特征值和特征向量描述的是方阵的”特征”</p>
<p><br></p>
<p>更进一步, 对于一个矩阵, 我们既可以把它理解为是变换, 也可以把它理解为是空间</p>
<p>而特征值和特征向量是从变换的视角来看方阵, 当我们把方阵理解成是一个变换的时候, 这个变换拥有一些特征, 这些特征就将被特征值和特征向量所表征出来, 即:把方阵当作变换的时候的一个”特征”</p>
<p>当然, 把矩阵看作是变换和看作是空间这两个视角是可以相互转换, 统一起来的</p>
<p><br></p>
<p>具体地看一个例子:</p>
<p><img src="/Blog/intro/Eigenvalues.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们可以把这个矩阵理解为是一个表示二维空间的基</p>
<p><img src="/Blog/intro/Eigenvalues_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>与此同时, 这个矩阵也可以表示成这个基所对应的坐标系的坐标转换矩阵</p>
<p><img src="/Blog/intro/Eigenvalues_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个矩阵可以做到把二维平面中的任意一个点, 通过一个固定的规则, 转换到另外一个位置</p>
<p><img src="/Blog/intro/Eigenvalues_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>把蓝色向量转化为绿色向量</p>
<p>可以理解为A矩阵代表的基和标准基之间的基变换</p>
<p><br></p>
<p>而在这个变换中, 我们比较关心一些特殊的向量</p>
<p><img src="/Blog/intro/Eigenvalues_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>(2, 2) 转换到 (4, 4)</p>
<p>这个转换有什么特点? 转换前后的方向没有改变</p>
<p>区别只是转换后的向量是原来向量的某一个常数倍而已</p>
<p>同理, 对于矩阵A还可以再举出一个例子: </p>
<p><img src="/Blog/intro/Eigenvalues_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于向量(2, 1) 转换后的方向还在原来的方向</p>
<p>对于矩阵A来说, 我们对这样的向量感兴趣: 在矩阵A的转换下, 得到的结果向量方向并没有发生改变, 只不过变为原来向量的某一个常数倍</p>
<p>用数学式表达为</p>
<p><img src="/Blog/intro/Eigenvalues_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>λ是一个常数, 就是说得到的结果向量和原来的u是一样的, 他们的差别只在于λ这个常数</p>
<p>我们对这样的λ和u感兴趣, 这二者就是我们这一章要研究的重点: <strong>特征值和特征向量</strong></p>
<p>λ 称为A的<strong>特征值(eigenvalue)</strong></p>
<p>u 称为A对应于λ的<strong>特征向量(eigenvector)</strong></p>
<blockquote>
<p>这个定义本身是非常好理解的, 然而最难的, 最令人迷惑的是我们定义完这两个东西后, 他们具体有什么用?</p>
<p>具体干什么用? 后续介绍 </p>
<p>另外一个通常的疑问就是: 为什么这样定义的值和向量叫特征值和特征向量, 它到底描述了怎样的特征. 这个问题其实和他们具体有什么用是联系在一起的</p>
</blockquote>
<p><br></p>
<h3 id="给定矩阵A如何求特征值和特征向量"><a href="#给定矩阵A如何求特征值和特征向量" class="headerlink" title="给定矩阵A如何求特征值和特征向量?"></a>给定矩阵A如何求特征值和特征向量?</h3><p><img src="/Blog/intro/sove_eigenvalue.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>首先, 零向量肯定满足. 是平凡解. 就是说这个解和<strong>A一点关系都没有</strong>, 无论A怎么变, u = 0 永远是这个方程的解. 正因为如此, u = 0 不能反应A的特征.</p>
<p>所以, <strong>研究特征向量的时候, 不考虑零向量</strong></p>
<p><img src="/Blog/intro/sove_eigenvalue_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>然而 λ = 0 并不平凡, 如果 λ = 0, 则有Au = 0, 而我们又规定了 u 不等于 0. 这个方程就变成了一个其次线性方程组, 对于这个齐次线性方程组来说它是有非零解的.</p>
<p>显然对于这种情况A是比较特殊的, 如果A可逆, 齐次线性方程组只有零解, 而现在这个情况我们希望u有非零解, A就是不可逆</p>
<p>所以 λ = 0 是这个矩阵的一个特征值的话, 就意味着A不可逆. λ = 0 反映了矩阵的一个特征. 并不是对于任意矩阵 λ = 0 都是它的特征值的. 所以<strong>特征值可以为0</strong></p>
<p><br></p>
<p>具体求解</p>
<p><img src="/Blog/intro/sove_eigenvalue_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而我们不能把u提出来, 因为λ是一个常数, 一个矩阵不能和一个常数做减法, 所以我们要对常数进行变形, 因为 Iu = u</p>
<p><img src="/Blog/intro/sove_eigenvalue_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时我们就可以把u提出来</p>
<p><img src="/Blog/intro/sove_eigenvalue_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意: 我们希望这个方程有非零解, 因为u是我们希望找的特征向量, 而我们不希望特征向量是零向量. </p>
<p>此时我们又可以把这些等价命题拿出来了</p>
<p><img src="/Blog/intro/sove_eigenvalue_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这些命题大多数都是叙述性的命题, 具体和细算相关的命题其实只有一个: det(A) != 0</p>
<p>这个命题就能帮助我们计算上面特征值和特征向量所对应的方程</p>
<p>不过由于我们现在是期望Ax = 0 存在非零解, 即不能只有唯一的解. 这其实是红色标识命题的一个<strong>逆命题</strong></p>
<p>因此和它等价的也是这个列表中其他命题的逆命题, det(A) != 0 的逆命题就是 det(A) = 0</p>
<p> 换句话说, 我们希望 det(A - λI) = 0</p>
<p><img src="/Blog/intro/sove_eigenvalue_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><strong>因此我们把求特征值和特征向量的问题转换成了求行列式的问题</strong></p>
<blockquote>
<p>这也是为什么说学习行列式是学习特征值和特征向量的基础</p>
<p>我们为了求解特征值和特征向量对应的这个方程</p>
<p><img src="/Blog/intro/sove_eigenvalue_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>必须借助行列式.</p>
<p>这是因为这个方程非常特殊, 不是简单的Ax = b, A和b都已知这样的方程.</p>
<p>在这个方程中存在两个未知量 λ 和 u 都是未知量</p>
<p>为了求解这样一个方程, 我们使用了一个技巧, 把它们转换成了求解行列式的一个问题</p>
<p>而这个方程 <strong>det(A- λI) = 0</strong> 此时只含有一个未知量 λ, 此时它就可以求解了, 而且这个方程对于任意的A都是适用的</p>
<p>通常我们称这个方程为<strong>特征方程</strong></p>
<p>我们可以用这个式子求特征值, 当有了特征值, 再代入式子中求特征向量就特别容易了</p>
</blockquote>
<p>具体求解:</p>
<p><img src="/Blog/intro/sove_eigenvalue_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/sove_eigenvalue_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/sove_eigenvalue_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>也就是说对于矩阵A来说, 有两个特征值, 2和3</p>
<p>而在求出这两个特征值之后, 就可以去寻找这两个特征值对应的特征向量了</p>
<p><img src="/Blog/intro/sove_eigenvalue_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>在这个式子中, u其实实在 (A-2I) 这个矩阵的零空间中</p>
<p><img src="/Blog/intro/sove_eigenvalue_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此可以把u写成这样, 第一个1是-1取反得到的, 第二个1就是对于自由列的值我们尝试每一个都取一个1, 只有一个自由列, 所以直接给这个位置取1就好了.</p>
<p>(1, 1) 其实就是 λ = 2 这个特征值的一个特征向量</p>
<p><img src="/Blog/intro/sove_eigenvalue_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>至此我们就把矩阵A的特征值和特征向量求出来了</p>
<p><img src="/Blog/intro/sove_eigenvalue_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>(1, 1) (2, 2)… 都是 λ = 2 对应的特征向量</p>
<p>(2, 1) (6, 2)… 都是 λ = 3 对应的特征向量</p>
<p><br></p>
<p><br></p>
<h2 id="特征值和特征向量的相关概念"><a href="#特征值和特征向量的相关概念" class="headerlink" title="特征值和特征向量的相关概念"></a>特征值和特征向量的相关概念</h2><p>特征值和特征向量其实就是满足这个方程中所对应的 λ和u</p>
<p><img src="/Blog/intro/sove_eigenvalue_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而求解这个方程我们需要用到行列式, 我们其实是求解 det(A- λI)=0 这个方程(特征方程)</p>
<p>通过这个方程我们就可以求得一个矩阵对应的不同的特征值, 针对每一个特征值再带回原方程中, 我们让原来的方程有非零的解. 这些非零解就是对应的特征向量</p>
<p><br></p>
<h3 id="特征向量由无数多个"><a href="#特征向量由无数多个" class="headerlink" title="特征向量由无数多个"></a>特征向量由无数多个</h3><p><strong>对于每一个 λ 的特征向量不唯一, 有无数多个</strong></p>
<p>这是因为我们求特征向量本质是在求一个齐次线性方程组的解 <strong>(A- λI) u=0</strong> </p>
<p>而对于一个齐次线性方程组来说, 它一旦不仅有非零解的话, 就一定是有无数个解</p>
<p>而我们的特征向量不为零, 所以特征向量一旦不为零, 那它就也不可能唯一了, 它一定是有无数个</p>
<p><br></p>
<p>对于这个结论我们可以再进行证明:</p>
<p><img src="/Blog/intro/infinity_eigenvec.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/infinity_eigenvec_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>或者从空间的角度去看待</p>
<p><img src="/Blog/intro/infinity_eigenvec_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><strong>特征空间</strong>: 一个特征值所对应的所有的特征向量所组成的空间加上零向量组成的向量空间</p>
<p>这里没有证明特征空间是向量空间, 因为特征空间本质是零空间, 而零空间一定是一个向量空间</p>
<p><br></p>
<p>我们可以发现特征值是取决定性作用的属性</p>
<p>因为一旦特征值确定了, 相应的特征空间就确定了, 在这个特征空间中有无数个特征向量.</p>
<p>下面变来看看特征值对应有怎样的一些可能性</p>
<p>求特征值和特征向量时首先求得就是特征值</p>
<p><img src="/Blog/intro/infinity_eigenvec_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>n就是A的阶数, (这个结论如果要证明需要对行列式的代数表达有深刻的理解, 就像上一节的例子, 对于二阶方阵, 行列式最后 λ 的2次方程)</p>
<p> <strong>λ 有n个解</strong>(不仅在实数范围里, 实数范围可能没有解), 在复数空间中, 对于 λ 的n次方程, 有n个解</p>
<p>例如:</p>
<p><img src="/Blog/intro/infinity_eigenvec_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于这种解均为实数, 且各不相同的情况, 我们称为<strong>简单特征值</strong></p>
<p>对于复杂的情况有两种</p>
<p><br></p>
<h3 id="特征方程的解相同"><a href="#特征方程的解相同" class="headerlink" title="特征方程的解相同"></a>特征方程的解相同</h3><p><img src="/Blog/intro/infinity_eigenvec_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个特征方程, 我们也说它有两个解, 但这两个解相同, 这种情况我们称为: <strong>多重特征值</strong> (又称重数. 例如上面例子中, 特征值为1, 重数为2)</p>
<p><br></p>
<h3 id="特征方程在实数范围无解"><a href="#特征方程在实数范围无解" class="headerlink" title="特征方程在实数范围无解"></a>特征方程在实数范围无解</h3><p><img src="/Blog/intro/infinity_eigenvec_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<blockquote>
<p>多重特征值和复数特征值这两种情况我们不重点讨论</p>
<p>因为我们可以通过一些技巧, 使得我们真正使用特征值和特征向量时, 规避掉这两种情况, 而只去应用简单特征值</p>
<p>实际应用时, 我们更喜欢使用简单特征值</p>
</blockquote>
<p><br></p>
<p><br></p>
<h2 id="特征值和特征向量的性质"><a href="#特征值和特征向量的性质" class="headerlink" title="特征值和特征向量的性质"></a>特征值和特征向量的性质</h2><h3 id="方阵A可逆的最后一条性质"><a href="#方阵A可逆的最后一条性质" class="headerlink" title="方阵A可逆的最后一条性质"></a>方阵A可逆的最后一条性质</h3><p>当 λ = 0 时会发生什么?</p>
<p><img src="/Blog/intro/property_eigenvalue.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>u时特征向量, 特征向量不能为0向量. 所以就相当于让这个<strong>线性系统</strong>不仅有零解. 说明<strong>矩阵A不可逆</strong>!</p>
<p>换句话说, 如果 λ = 0 <strong>是</strong>A的特征值的话, 相应我们就能推断出A是不可逆的. 反过来, 如果 λ = 0 <strong>不是</strong>A的特征值的话, 我们就知道A是可逆的</p>
<p>这就是对于方阵A可逆的等价列表的最后一条</p>
<p><img src="/Blog/intro/property_eigenvalue_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>如果A可逆, 0 就不能是 A 的特征值.</p>
<p><br></p>
<h3 id="特殊的方阵的特征值和特征向量"><a href="#特殊的方阵的特征值和特征向量" class="headerlink" title="特殊的方阵的特征值和特征向量"></a>特殊的方阵的特征值和特征向量</h3><p>求解特征值和特征向量的过程主要是<strong>先求解特征值</strong>. 一旦有了特征值再求解特征向量就简单了</p>
<p>而求解特征值是靠特征方程, 特征方程本质是让这个行列式等于0</p>
<p><img src="/Blog/intro/sove_eigenvalue_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>有哪些特殊的矩阵, 我们可以非常简单地直接求出它的行列式?</p>
<p>回忆: 对于对角矩阵, 它的行列式就是对角线的乘积</p>
<p><img src="/Blog/intro/property_eigenvalue_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>它的特征值就是这样, 让A-λI这个矩阵的行列式等于0的λ</p>
<p>如果A是一个对角矩阵, A-λI 也是一个对角矩阵</p>
<p><img src="/Blog/intro/property_eigenvalue_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>特征方程就是让这个式子等于0, 显然这个式子等于 0 对应的解是</p>
<p><img src="/Blog/intro/property_eigenvalue_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>就是说, 对于<strong>对角矩阵</strong>来说, 它的<strong>对角线上的元素</strong>就是它的特征值</p>
<p><br></p>
<p>同理, 能直接求出行列式的值的矩阵不仅仅有对角矩阵, 还有上三角矩阵和下三角矩阵</p>
<p><img src="/Blog/intro/property_eigenvalue_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/property_eigenvalue_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="λ的m次幂和A的m次幂的关系"><a href="#λ的m次幂和A的m次幂的关系" class="headerlink" title="λ的m次幂和A的m次幂的关系"></a>λ的m次幂和A的m次幂的关系</h3><p>性质:</p>
<p><img src="/Blog/intro/property_eigenvalue_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>后续就能看到, 在很多应用中我们都会对矩阵A的幂感兴趣</p>
</blockquote>
<h4 id="数学归纳法证明"><a href="#数学归纳法证明" class="headerlink" title="数学归纳法证明:"></a>数学归纳法证明:</h4><p>当m = 1时 =&gt; 成立</p>
<p>m=1描述的是: 若λ是A的特征值, 则λ的一次幂是A的一次幂的特征值</p>
<p>而 λ 的一次幂就是 λ , A的一次幂就是A, 这就是已知的条件, 自然成立</p>
<blockquote>
<p>注意: λ 是 A 的特征值就满足下面等式</p>
<p><img src="/Blog/intro/property_eigenvalue_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
</blockquote>
<p><img src="/Blog/intro/property_eigenvalue_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>注意: 当 m = -1 时, 上面的<strong>证明</strong>是不成立的, 因为上面的证明是针对m&gt;=1的情况来说的.</p>
<p>然而当m为-1时, 这条性质依然成立</p>
<p><img src="/Blog/intro/property_eigenvalue_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h4 id="m-1时的证明"><a href="#m-1时的证明" class="headerlink" title="m=-1时的证明:"></a>m=-1时的证明:</h4><p><img src="/Blog/intro/property_eigenvalue_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而如果把 1 / λ 看作 λ’, 把A的逆看作A’, 则又可以使用数学归纳法证明当m=-2时结论依然成立</p>
<p><br></p>
<p>万变不离其宗的是: 一旦我们直到 λ 是 A 的特征值, 那么就意味着存在这样一个等式</p>
<p><img src="/Blog/intro/property_eigenvalue_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="补充-Fundamental-Theorem-of-Algebra"><a href="#补充-Fundamental-Theorem-of-Algebra" class="headerlink" title="补充: Fundamental Theorem of Algebra"></a>补充: Fundamental Theorem of Algebra</h3><p>如果f(x)是一个最高项为n次幂的多项式, 则在复数范围内有n个解(可能相同)</p>
<p><img src="/Blog/intro/fundamental_theorem.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/fundamental_theorem_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>即: <strong>A的行列式为所有特征值的乘积</strong></p>
<p>同时还有一条性质: A的对角线的和 tr(A) = 所有特征值的和</p>
<p><br></p>
<p><br></p>
<h2 id="直观理解特征值与特征向量"><a href="#直观理解特征值与特征向量" class="headerlink" title="直观理解特征值与特征向量"></a>直观理解特征值与特征向量</h2><p>虽然在引入特征值和特征向量时介绍了它们的几何含义, 但其实后续却一直围绕着<strong>代数定义</strong>打转, 即<strong>Au=λu</strong>. 这一节就回归到特征值和特征向量的几何含义中, 通过几个例子看一看什么是特征值和特征向量</p>
<p><br></p>
<h3 id="投影变换"><a href="#投影变换" class="headerlink" title="投影变换"></a>投影变换</h3><p>关于什么是投影, 在之前讲正交性的时候曾经介绍过. 但是对于投影这种变换, 它所对应的矩阵是什么样子我们一直没有求解. 这也是举这个例子的原因</p>
<p>如果我们理解了特征值和特征向量的几何含义, 我们根本不需要知道对于一个投影变换的对应的矩阵到底是什么.</p>
<p>既然不知道矩阵, 就不用列出特征方程, 也不用求解特征方程, 直接通过几何的方式, 就可以得到一个投影变换它所对应的特征值和特征向量</p>
<p><br></p>
<p>以二维空间为例, 假设投影变换是把任何二维空间中的向量投影到(2, 1)这个向量所在直线</p>
<p><img src="/Blog/intro/visuallize_elgenvalue.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/visuallize_elgenvalue_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>下面我们就来研究这个投影变换所对应的矩阵, 它的特征值和特征向量是多少</p>
<p>在此之前, 我们也来复习一下前面的一个其他的知识</p>
<p>我们首先来简单地看一下, 这个变换到底是不是<strong>对应一个矩阵</strong>, 因为变换这个词的含义是非常广的, 并不是随便给出一个变换就能求特征值和特征向量的, 必须要保证这个变换对应一个矩阵</p>
<p>什么意思? 回忆坐标系转换和线性变换那一章, 这就意味着我们必须要看投影变换这样一种变换方式它是不是一个<strong>线性变换</strong>, 如果它是一个线性变换, 那么它就一定对应了一个矩阵</p>
<p>怎么看一个变换是不是线性变换? 套线性变换的定义</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>显然投影变换是满足这两条性质的</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此我们就知道投影变换是一种线性变换, 它一定对应一个矩阵</p>
<p>而这个例子中, 就是从<strong>二维空间向二维空间变换</strong>, 所以它对应的是一个<strong>二阶方阵</strong>, 我们就知道我们可以对投影变换所对应的矩阵来考察它的特征值和特征向量</p>
<p>具体什么是特征值和特征向量? 当我们用几何的视角来研究这个问题的时候先看特征向量是比较方便的</p>
<p>所谓的特征向量就是经过这种变换之后, 结果向量和原向量还在一条直线上, 他们之间只相差某个常数倍, 这个常数就是特征值</p>
<p>显然, 在我们要投影到的直线, 蓝色的直线上的所有向量经过投影之后自然还是它自己, 还在蓝色的直线上</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>所以在这根蓝色直线上的所有向量都是特征向量, 它满足特征向量的定义, 而这些向量投影之后还是它自己, 所以特征值为1</p>
<blockquote>
<p>我们根本不知道投影变换对应的矩阵是什么, 仅仅根据它的几何含义, 就推导出来了所对应的一组特征值和特征向量</p>
</blockquote>
<p>有没有其他的向量, 也满足投影到蓝色的直线上所得到的结果和原来的向量在同样一根直线上呢? 和蓝色的直线相垂直的向量</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个向量过原点, 同时和蓝色的向量垂直, 而它进行投影变换的结果向量是<strong>零向量</strong></p>
<p>所以, 和蓝色直线相垂直的直线上的所有向量也是特征向量, 它所对应的特征值为0</p>
<p>我们就找到了两个特征值(0和1)和它们对应的<strong>特征空间</strong>(蓝色直线, 和蓝色直线垂直的直线)</p>
<p><br></p>
<h3 id="翻转变换"><a href="#翻转变换" class="headerlink" title="翻转变换"></a>翻转变换</h3><p>矩阵A是单位矩阵进行一次行交换的结果, 所以它是一个初等矩阵</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>用A乘以一个矩阵(或向量), 乘以的目标也将做行交换</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个变换是什么意思?</p>
<p>就是将所有的向量沿y=x这条直线进行一次翻转</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>(2, 5)变换为(5, 2), 这两个向量是关于y=x对称的</p>
<p>这就是矩阵A的几何含义, 知道几何含义是否就能找到特征值和特征向量了呢?</p>
<p>同样是先看特征向量: 有哪些向量经过这种变换之后还在原来的方向上呢?</p>
<p>显然在y=x这根直线上的向量经过反转之后方向不变, 它们都是特征向量, 同时所对应的特征值就是 λ=1</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而y=-x这条直线上的向量反转后也在同一条直线, 结果向量和原向量相差常数倍, 这个常数就是-1, λ=-1</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>通过分析这个变换在几何上是什么意思就找到了这个变换所对应的两个特征值(1 和 -1)以及和这两个特征值所对应的两个特征空间</p>
<p>同样既然我们知道了A, 就可以用代数的方式来验证一下</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>这两个例子的特征值都是<strong>简单特征值</strong>, 都是实数, 且没有重复</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个例子也是之前分析的, 也是简单特征值. </p>
<p>对于这三个例子来说, 得到的特征向量, 或者说特征空间都是<strong>两根</strong>直线</p>
<p>对于每一个特征值的特征空间是一根直线这并不奇怪, 这是因为我们之前证明过, 对于Au=λu, 把特征向量伸缩k倍结果还是λ所对应的特征向量所以这种情况下每一个特征值所对应的特征向量就是一根直线</p>
<p>关键在于这两个特征值所对应的特征向量它们不会重合, 换句话说, <strong>这两个向量是线性无关的</strong></p>
<p>这个性质很重要, 我们最后证明一下这个性质:</p>
<h3 id="证明-任意两个不同特征值的特征向量线性无关"><a href="#证明-任意两个不同特征值的特征向量线性无关" class="headerlink" title="证明: 任意两个不同特征值的特征向量线性无关"></a>证明: 任意两个不同特征值的特征向量线性无关</h3><p>如果矩阵A含有两个不同的特征值, 则他们对应的特征向量线性无关</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>即: 证明u和v是线性无关的</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>把Au和Av分别代入等式就得到</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同时因为u=kv, 等式两边同时乘以 λ1, 即:</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这两个式子相减就是:</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_15.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>把k和v提出来</p>
<p><img src="/Blog/intro/visuallize_elgenvalue_16.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>已知</p>
<ol>
<li>k不为0     (线性相关定义)</li>
<li>λ2-λ1不为0 (因为我们假设有两个不同的特征值)</li>
<li>v不为0      (特征向量定义)</li>
</ol>
<p>所以乘积不为0, 矛盾</p>
<p><br></p>
<p><img src="/Blog/intro/visuallize_elgenvalue_17.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>然而这种方法只能证明任意<strong>两个</strong>特征向量线性无关, 但却无法证明所有特征值线性无关, 但证明的思路也都是相通的</p>
<h3 id="证明-所有不同特征值对应的特征向量线性无关"><a href="#证明-所有不同特征值对应的特征向量线性无关" class="headerlink" title="证明: 所有不同特征值对应的特征向量线性无关"></a>证明: 所有不同特征值对应的特征向量线性无关</h3><p>使用数学归纳法(MI)证明</p>
<p>假设A是n阶方阵, 有不同特征值 λ1 … λm, 对应的特征向量为 v<sub>1</sub>, … v<sub>m</sub></p>
<p><img src="/Blog/intro/eigen_vec_linear_indep.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42682806/article/details/84774325">参考</a> : 范德蒙的思想证明</p>
<p>特征向量和主成分分析法PCA也有关系</p>
</blockquote>
<p><br></p>
<p><br></p>
<h2 id="“不简单”的特征值"><a href="#“不简单”的特征值" class="headerlink" title="“不简单”的特征值"></a>“不简单”的特征值</h2><p>简单的特征值就是<strong>不重复</strong>的<strong>实数</strong>这样的特征值</p>
<p>不简单的特征值就是不满足这两个条件的特征值, 或者为复数, 或者有重复</p>
<p><br></p>
<h3 id="第一种不简单的特征值-复数"><a href="#第一种不简单的特征值-复数" class="headerlink" title="第一种不简单的特征值(复数)"></a>第一种不简单的特征值(复数)</h3><p>首先看一下这个变换</p>
<p><img src="/Blog/intro/complex_eigenvalue.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们想把浅色的F变成深色的F, 相当于逆时针选择90°</p>
<p>这个变换所对应的矩阵是旋转矩阵, 这个旋转矩阵是</p>
<p><img src="/Blog/intro/complex_eigenvalue_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>矩阵的两列就是旋转后的x轴(0, 1)和y轴(-1, 0)</p>
<p>首先从<strong>几何的角度</strong>来看这个矩阵的特征值和特征向量是谁</p>
<p><br></p>
<h4 id="几何角度"><a href="#几何角度" class="headerlink" title="几何角度"></a>几何角度</h4><p><img src="/Blog/intro/complex_eigenvalue_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>先看特征向量</p>
<p>通过旋转变换之后, 哪些向量和原来的向量处在同一个直线上</p>
<p>而答案是没有这样的向量, 任何向量只要一旋转, 肯定偏离的原来所在的直线</p>
<p>所以从几何的角度去看的话没有任何向量是这个矩阵的特征向量</p>
<p><br></p>
<h4 id="代数角度"><a href="#代数角度" class="headerlink" title="代数角度"></a>代数角度</h4><p>从代数角度再验证一下</p>
<p><img src="/Blog/intro/complex_eigenvalue_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个特征方程再实数范围内没有解, 复数范围内的解为 i 和 -i </p>
<p>从代数的角度来看, 这个矩阵的特征值是两个复数. 相应地从几何的角度看就相当于: 我们直观地去看, 根本就不存在特征值和特征向量</p>
<p>这就是复数特征值复杂的地方, <strong>它所对应的几何含义已经消失了</strong>, 我们不能从特征值和特征向量的几何意义上理解它们了</p>
<p><br></p>
<h3 id="第二种不简单的特征值-重复"><a href="#第二种不简单的特征值-重复" class="headerlink" title="第二种不简单的特征值(重复)"></a>第二种不简单的特征值(重复)</h3><p>看一下单位矩阵的特征值和特征向量</p>
<p><img src="/Blog/intro/complex_eigenvalue_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>单位矩阵做的是什么事情? 把原来的向量还转化成他自己, 向量根本不动</p>
<p>从几何的角度去理解, 什么样的向量是单位矩阵的特征向量? 换句话说, 什么样的向量在单位矩阵的变换下还在它原来自己的方向上? <strong>所有向量</strong>都满足这个条件, 并且相应的特征值为1</p>
<p>我们特征值只有一个, 就是1, 但是这个特征向量的方向却是二维平面的任意方向, 用代数的方式看一下:</p>
<p><img src="/Blog/intro/complex_eigenvalue_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>代入求特征向量</p>
<p><img src="/Blog/intro/complex_eigenvalue_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>当λ是多重特征值的时候, 它所对应的特征空间不一定是一条直线了, 有可能是一个<strong>高维空间</strong></p>
<p>虽然λ是相同的特征值, 但由于特征空间是平面, 我们又找到了一组基, 他们是两个线性无关的向量. 但是<strong>对于多重特征值来说, 不能保证重数为2, 相应的特征空间就是2维的, 我们就能找到2个基向量</strong></p>
<p>例如:</p>
<p><img src="/Blog/intro/complex_eigenvalue_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个矩阵中, 没有两个自由列, 只有一个自由列. 或者从另一个角度看, 这个矩阵的秩是1, 而这又是一个二阶方阵, 所以对应零空间的维度为1</p>
<p>u所对应的解只有</p>
<p><img src="/Blog/intro/complex_eigenvalue_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这就是之前看到情况的一个反例, 这种情况下, 方阵有二重特征值, 而它对应<strong>线性无关的特征向量只有一个</strong>, 这个二重特征值所对应的特征空间是一维的</p>
<p>这就是多重特征值复杂的地方, 对于多重特征值来说, 它所对应的线性无关的特征向量有可能是多重的, 但也有可能退化, 数目相应地更少, <strong>特征空间的维度是不固定的</strong></p>
<p><br></p>
<p>实际上在数学的角度, 是存在这样一个非常重要的性质的:</p>
<p><strong>如果矩阵A的某个特征值的重数=k, 则对应的特征空间的维度&lt;=k</strong></p>
<p>这里的重数是从代数的角度去看的, 特征值可能有复数</p>
<p> 我们对这两个数字定义两个新的概念, 我们把特征值的重数称为<strong>代数重数</strong>, 而把这个特征值所对应的特征空间(零空间)的维度定义为<strong>几何重数</strong></p>
<p><img src="/Blog/intro/complex_eigenvalue_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>数学上就有这样一个重要的定理: <strong>几何重数不大于代数重数</strong></p>
<p>若重数为1, 则很简单, 其特征空间的维度为1</p>
<p><br></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>复数特征值之所以复杂, 是因为它的几何意义丢失了</p>
</li>
<li><p>多重特征值之所以复杂, 是因为特征值所对应的特征空间的维度不固定</p>
</li>
<li><p>简单特征值太好了, 每一个特征值一定对应一个一维的特征空间, 不同特征值对应的特征向量还都是线性无关的</p>
</li>
</ul>
<p>基本概念介绍完了, 下面就要介绍为什么学习特征值和特征向量</p>
<p>这里所说的特征到底是什么特征?</p>
<p><br></p>
<p><br></p>
<h2 id="矩阵相似和背后的重要含义"><a href="#矩阵相似和背后的重要含义" class="headerlink" title="矩阵相似和背后的重要含义"></a>矩阵相似和背后的重要含义</h2><p>为了搞清楚特征值和特征向量到底表达了什么样的特征, 我们要引入一个新的概念: <strong>矩阵相似型/相似矩阵</strong></p>
<p><br></p>
<h3 id="矩阵相似型-相似矩阵"><a href="#矩阵相似型-相似矩阵" class="headerlink" title="矩阵相似型/相似矩阵"></a>矩阵相似型/相似矩阵</h3><p>定义:</p>
<p><img src="/Blog/intro/similer_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>P是某一个可逆矩阵</p>
<p>单纯地看这个定义是很难看明白为什么AB满足这个式子就叫A和B相似</p>
<p>但是一旦直观地从几何的角度理解这个式子, 其实它是非常容易理解的</p>
<p>从几何的角度理解可以类比一下<strong>相似三角形</strong></p>
<p><img src="/Blog/intro/similar_triangle.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>两个三角形如果三个内角都相等, 就称两个三角形相似</p>
<p>两个三角形相似就是他们的大小不一样, 但形状一样</p>
<p>我们怎么理解<strong>形状一样, 大小不一样</strong>? 可以理解为是从不同的视角观测相同的内容</p>
<p>可以想象成如果这两个三角形就是一个三角形, 只不过观察它的视角可能是远近不同, 就造成了我们看到的这两个三角形大小不同, 但形状一样. 这种情况下, 我们就说这两个三角形是相似的</p>
<p><br></p>
<p><img src="/Blog/intro/similer_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这样的视角同样可以用于理解<strong>矩阵的相似</strong></p>
<p>两个矩阵相似其实本质也是<strong>从不同的视角观察相同的内容</strong>, 由于视角不同, 看到这两个矩阵不同, 但是他们本质是一样的. 我们就说AB是相似的</p>
<p>具体我们是怎样从不同视角观察相同内容? 这个视角就藏在矩阵P中</p>
<p><img src="/Blog/intro/similer_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于这个式子, 如果我们把矩阵P看作是一个坐标系, P是一个<strong>坐标转换矩阵</strong>, 而矩阵A,B对应两个<strong>线性变换</strong>的矩阵的话</p>
<p>这个式子其实实在反应这样一件事情: <strong>P是一个坐标系, 则A变换是在P坐标系下观察的B的变换</strong></p>
<p>换一句话说, A和B这两个矩阵表示的变换是同一个变换, 只不过我们观察它所在的坐标系是不同的, 我们观察B变换就是在标准坐标系下, 而观察A变换则是在P所对应的坐标系下</p>
<p>我们在两个不同坐标系下观察同一变换, 得到的结果是不同的, 得到的结果就是矩阵A和矩阵B, 但是他们本质是<strong>同一个变换</strong>, 我们就说A和B是相似的</p>
<blockquote>
<p>我们还没有证明为什么这个式子表示的是A变换在P坐标系下观察的B变换</p>
<p><img src="/Blog/intro/similer_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>先和相似三角形一样类比一下, 不同的视角观察相同的内容</p>
<ul>
<li><p>两个相似三角形本质是同一个三角形, 只不过观察的远近不同</p>
</li>
<li><p>AB两个矩阵本质是同一个变换, 只是我们观察它们所在坐标系不同</p>
</li>
</ul>
</blockquote>
<p><br></p>
<h3 id="证明这个结论"><a href="#证明这个结论" class="headerlink" title="证明这个结论"></a>证明这个结论</h3><p><img src="/Blog/intro/similer_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>为什么A变换是在P坐标系下观察的P变换</p>
<p><br></p>
<p>首先在等式两边的右侧, 同时乘以P坐标系下的一个向量</p>
<p><img src="/Blog/intro/similer_matrix_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>为什么乘以P坐标系下的x而不是标准坐标系下的一个向量x呢</p>
<p>这是因为乘完以后等式的右边P先和x相乘, P是一个坐标转换矩阵, 这个坐标转换矩阵要乘以这个坐标系下的一个向量(坐标), 把它<strong>转换成是在标准坐标系下的一个坐标</strong>, 这个转换它才有几何意义</p>
<p>所以其实实在等号两边同时乘以一个向量, 只不过我们要把这个向量x<strong>理解</strong>成是在P坐标系下的向量</p>
<p>做完这个工作后, 就可以看看等式两边所对应的几何含义了</p>
<p><br></p>
<h4 id="等式左边"><a href="#等式左边" class="headerlink" title="等式左边"></a>等式左边</h4><p><img src="/Blog/intro/similer_matrix_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于P坐标系下的x, 左乘一个A就相当于直接在P坐标系下进行A变换, 结果就是A[x]<sub>p</sub></p>
<p><img src="/Blog/intro/similer_matrix_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>关键就是看右边乘法</p>
<p><br></p>
<h4 id="等式右边"><a href="#等式右边" class="headerlink" title="等式右边"></a>等式右边</h4><p>显然用坐标转换矩阵P乘以P坐标系下的坐标x, 就是把这个坐标转换成了<strong>标准坐标系</strong>下的一个坐标[x]<sub>ε</sub></p>
<p><img src="/Blog/intro/similer_matrix_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>之后乘以矩阵B, 矩阵B本身是一个变换, 就是在标准坐标系下对[x]<sub>ε</sub>进行了B变换, 最终得到的结果是B[x]<sub>ε</sub></p>
<p><img src="/Blog/intro/similer_matrix_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>B[x]<sub>ε</sub>是将x坐标在标准坐标系下进行B变换得到的坐标, 这个坐标依然在标准坐标系下, 如果我们想把它转到P坐标系下只需要<strong>左乘坐标转化矩阵P的逆</strong></p>
<p>我们再把标准坐标系下的坐标转换到P坐标系下只需要乘以P的逆就好了</p>
<p>由于我们假设的是</p>
<p><img src="/Blog/intro/similer_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里我们就直到乘以P的逆的结果和A[x]<sub>p</sub>是一样的</p>
<p><img src="/Blog/intro/similer_matrix_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个图饶了一圈就是在说: 一个P坐标系下的坐标先转换到标准坐标系下, 再给它进行B变换, 此时结果还在标准坐标系下, 我再给他转回P坐标系下再左乘P的逆. 最终结果其实和我们再P的坐标系下直接进行A变换得到的结果是一致的</p>
<p>这就说明<strong>A和B两个变换本质是一个变换, 只不过我们所观察的坐标系不同</strong></p>
<ul>
<li>上面我们观察A变换是在P坐标系下</li>
<li>下面我们观察B变换是在标准坐标系下</li>
</ul>
<p><strong>P是一个坐标系, 则A变换是在P坐标系下观察的B变换, B是在标准坐标系下观察的</strong></p>
<p><br></p>
<p>在理解这个式子之后, 我们就可以再玩一些别的内容了</p>
<p>对这个式子简单变一下型, 就变成这个样子</p>
<p><img src="/Blog/intro/similer_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/similer_matrix_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>右边式子在阐述几何意义时是更常用的, 因为A这个变换实在P这个坐标系下观察的, 因此干脆把所有和P相关的项都放到等号的一侧</p>
<p>标准坐标系下的一个变换B, 到了P这个坐标系下看和A是一样的</p>
<blockquote>
<p>当我们说矩阵相似时, A和B相似, B和A也就相似. 此时这两个式子用哪个式子都是可以的. 在这种情况下P和P逆谁在前谁在后不重要</p>
<p>但是一旦我们有了几何的解释的时候, P和P逆谁在前谁在后决定了我们到底是把那个变换放在P坐标系下观察</p>
<p><br></p>
<p>如果P逆在前P在后</p>
<p><img src="/Blog/intro/similer_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们是把A变换放在P坐标系下观察, 此时等式同时右乘P坐标系下的坐标才有意义</p>
<p><br></p>
<p>如果P在前P逆在后</p>
<p><img src="/Blog/intro/similer_matrix_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们是把A变换放在P坐标系下观察, 此时等式同时右乘标准坐标系下的坐标才有意义</p>
</blockquote>
<p><br></p>
<h3 id="A和B的相同之处"><a href="#A和B的相同之处" class="headerlink" title="A和B的相同之处"></a>A和B的相同之处</h3><p>既然A和B是一个变换, 它们肯定有一些地方是相同的</p>
<p>就像两个三角形相似, 它们的角是相同的</p>
<p>A和B相似, 什么相同? <strong>特征方程</strong>相同, <strong>特征值</strong>相同</p>
<p>这就是我们为什么称特征值和特征向量为<strong>特征</strong></p>
<p><strong>因为这些特征是永远不变的, 不管在哪个坐标系下观察, A和B的特征方程依然相同, 特征值依然相同</strong></p>
<blockquote>
<p>下一节就会看到特征值的作用</p>
<p>可以使用特征值来构造一个矩阵, 这个矩阵就来表示A和B所对应的变换, 只不过需要从一个特殊的坐标系中来看</p>
<p>A和B的其余相同之处:</p>
<ol>
<li>det(A) = det(B)     行列式相等 (因为行列式的值为特征值的乘积)</li>
<li>rank(A) = rank(B) 秩相等</li>
<li>tr(A) = tr(B)           对角线之和相等 (因为对角线之和为特征值之和)</li>
</ol>
<p>(1) 和 (3) 的证明非常简单, 利用特征值相等这一特性即可</p>
<p>(2) 的证明:</p>
<p>相似变换是形如B=P<sup>-1</sup>AP。称A与B相似，记A~B。（要求A和B都为方阵，P可逆）</p>
<p>而矩阵A与可逆（满秩）矩阵相乘不改变A的秩，A乘以可逆矩阵也相当于对方阵A进行初等变换。</p>
</blockquote>
<p><br></p>
<h3 id="证明-如果A和B相似-它们的特征方程相同"><a href="#证明-如果A和B相似-它们的特征方程相同" class="headerlink" title="证明: 如果A和B相似, 它们的特征方程相同"></a>证明: 如果A和B相似, 它们的特征方程相同</h3><p>已知: <img src="/Blog/intro/similer_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>A的特征方程即</p>
<p><img src="/Blog/intro/similer_matrix_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>A和B的特征方程是一样的</p>
<p>这就意味着A和B的特征值是一样的</p>
<p>这就是特征值和特征向量所真正表征的特征: 它表示当我们把这个矩阵看作是一个变换的话, 不管从哪个角度, 哪个坐标系去看, 他们都有相同的特征. 这些相同的特征就被特征值表征了出来</p>
<p><br></p>
<p>既然这个变换可以从不同角度去看它, 我们肯定就希望找到一个<strong>最好的角度</strong>, 用这个最好的角度去看它让我们的计算及我们对变换的理解是最为容易的</p>
<p>最好的角度在哪里, 答案也隐藏在特征值和特征向量当中</p>
<p><br></p>
<p><br></p>
<h2 id="矩阵对角化"><a href="#矩阵对角化" class="headerlink" title="矩阵对角化"></a>矩阵对角化</h2><p>矩阵的相似性的定义与含义</p>
<p><img src="/Blog/intro/matrix_sim.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>既然我们在P坐标系观察B是A这个样子, 那么我们就可以变换P这个坐标系, 每一变化坐标系P, 相应的A的样子也产生变化. </p>
<p>我们希望找到最好的A, 使得B这个变化变得非常简单. 这就是<strong>矩阵对角化</strong>所做的任务</p>
<p>实际上<strong>矩阵对角化</strong>是指对于变换A<strong>有可能(不是一定可能, 有条件)</strong>写成下面形式</p>
<p><img src="/Blog/intro/diagonlization_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>D代表对角矩阵</p>
<p><img src="/Blog/intro/d_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个式子代表对于A这个变换, 我们在P这个坐标系下观察, 观察到的结果是D (注意一定不要搞混<strong>在谁的坐标系下观察谁</strong>, P在前P的逆在后, 所以A这个变换实在标准基下的变换)</p>
<p>而一个对角矩阵代表什么? 它代表的是对于一个向量x, 每一个维度的向量只要<strong>伸缩第i倍</strong>就好了, 不牵扯到和其他维度的向量进行组合</p>
<p>所以对角矩阵所表示的变换是最简单的</p>
<p>矩阵的对角化, 就是对于任意一个变换, 我们试图找到一个合适的坐标系P, 在这个坐标系下观察, A变换变成了一个对角矩阵的样子, 这个样子是非常方便我们进行很多操作的</p>
<p>现在问题就是, 对于一个方阵A, 它到底能不能在某一个坐标系P下观察, 看到的结果是对角矩阵的样子. 可以, 但是有条件</p>
<p><br></p>
<h3 id="矩阵对角化的前提条件"><a href="#矩阵对角化的前提条件" class="headerlink" title="矩阵对角化的前提条件"></a>矩阵对角化的前提条件</h3><p>条件: <strong>如果A有n个线性无关的特征向量, 则A和某个D相似</strong>, 称A可以被对角化</p>
<blockquote>
<p>可以先不用去管这个条件, 后面讲解矩阵对角化具体怎么操作就会发现这个条件是非常自然的</p>
</blockquote>
<p>矩阵对角化也可以理解为矩阵分解的一种形式, 它把A分解成三部分</p>
<p><img src="/Blog/intro/diagonlization_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>A如何分解成这个形式? 答案就藏在特征值和特征向量中. 直接抛出结论:</p>
<p>如果A可以进行对角化, 对角矩阵D对角线上的所有元素就是A这个矩阵的<strong>特征值</strong>, P这个坐标系就是把特征值所对应的<strong>特征向量</strong>按列排列的结果(由于是由所有的特征向量排列得到的, 所以通常也称它为<strong>特征向量矩阵</strong>)</p>
<p><img src="/Blog/intro/diagonlization_matrix_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>换句话说, 对于矩阵A来说, 如果它可以被对角化(或者说A和某个对角矩阵D相似), 是在特征向量所代表的坐标系下才能看到这个结果, 对角矩阵D每一个主对角线上的元素就全都是特征值.</p>
<p>通过这个结论, 前提条件就很好理解为什么A要有n个线性无关的特征向量. 这n个线性无关的特征向量要组成这个特征向量矩阵</p>
<p><img src="/Blog/intro/diagonlization_matrix_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个特征向量矩阵所有的列元素构成了一个空间的基, 它是我们观察A的另外的一个坐标系.</p>
<p>如果A没有n个线性无关的特征向量的话, 我们就不能构成矩阵P, A也就不能对角化</p>
<p>另一个思考的方向是在对角化的过程中, 矩阵P必须是可逆的, 所以P的每一个列元素必须是线性无关的</p>
<p><br></p>
<h3 id="证明这个结论-1"><a href="#证明这个结论-1" class="headerlink" title="证明这个结论"></a>证明这个结论</h3><p>为什么这样的D和P就满足A=PDP<sup>-1</sup></p>
<p><img src="/Blog/intro/diagonlization_matrix_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>我们已知P和D, 但是P的逆我们很难处理, 所以把等式两边同时乘以P</p>
<p>我们来证明 AP=PD</p>
<p><img src="/Blog/intro/diagonlization_matrix_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>由因为P每一列都线性无关所以P可逆, 我们就可以写成这个样子, A=PDP<sup>-1</sup></p>
<p>也就是A和D是相似的</p>
<p><br></p>
<p>我们可以从特征向量的矩阵的视角来看A变换, 它就是一个对角矩阵所表示的变换</p>
<p>如果我们可以这样构造出P和D的话, 我们就称A这个矩阵是可以对角化的</p>
<p>在这里对于 <strong>“A有n个线性无关的特征向量, A才可以被对角化”</strong> 这个前提条件再来分析一下</p>
<p><br></p>
<h3 id="深入分析矩阵对角化的前提条件"><a href="#深入分析矩阵对角化的前提条件" class="headerlink" title="深入分析矩阵对角化的前提条件"></a>深入分析矩阵对角化的前提条件</h3><p>回忆: 如果矩阵包含两个不同的特征值, 则他们对应的特征向量线性无关</p>
<p>推广一下: 如果A有n个不相同的特征值,则A可以被对角化</p>
<blockquote>
<p>注意: 如果A没有n个不相同的特征值, A不一定不能被对角化</p>
<p>关键在于如果A中含有某个特征值它的重数大于1, 这样一个A不一定不能被对角化, 关键还是看重数是否等于特征空间的维度(例如重数为2不代表特征空间的维度为2)</p>
<p><br></p>
<p>要找到n个线性无关的特征向量</p>
<ul>
<li><p>首先不同的特征值的特征向量一定线性无关</p>
</li>
<li><p>其次就是看相同的k个特征值能否找到k个线性无关的特征向量. 而如果能找到k个线性无关的特征向量, 则这个特征值的特征空间就是k维的. </p>
</li>
</ul>
<p>所以要看重数和特征空间的维度</p>
</blockquote>
<p>例如:</p>
<p><img src="/Blog/intro/diagonlization_matrix_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>虽然这个矩阵有两个相同的特征值, 但所对应的特征空间是一维空间, 所以找不到两个线性无关的特征向量</p>
<p>所以这个矩阵我们无法将其对角化</p>
<p>这也是当存在重数的时候, 事情变得复杂的原因</p>
<ul>
<li><p>如果所有特征值的重数为1, 矩阵只含有<strong>简单特征值</strong>, 则它一定可以被对角化, 一定可以从另外一个P视角来看它的本质就是一个对角矩阵所代表的变换, 这也是称之为”简单”特征值的原因</p>
</li>
<li><p>但是如果A没有n个不相同的特征值, 它含有具有重数的特征值的话, 它能不能对角化我们就不能一眼知道了, 需要具体求重数特征值的<strong>特征空间的维度</strong></p>
</li>
</ul>
<p>不管怎样, 如果A能找到n个线性无关的特征向量, A就可以被对角化</p>
<p><img src="/Blog/intro/diagonlization_matrix_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h2 id="矩阵对角化的应用-求矩阵的幂和动态系统"><a href="#矩阵对角化的应用-求矩阵的幂和动态系统" class="headerlink" title="矩阵对角化的应用: 求矩阵的幂和动态系统"></a>矩阵对角化的应用: 求矩阵的幂和动态系统</h2><p><img src="/Blog/intro/diagonalization.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>由于D变换是对角矩阵, 所以这个变换非常简单, 只不过是在每一个维度上伸缩对角矩阵对角线上相应元素那么多倍  </p>
<p>而如果我们把A变换看成是D变换, 很多运算就会变得非常简单, 例如矩阵的幂</p>
<p><br></p>
<h3 id="求矩阵的幂"><a href="#求矩阵的幂" class="headerlink" title="求矩阵的幂"></a>求矩阵的幂</h3><p><img src="/Blog/intro/diagonalization_power.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这样我们就把计算A的n次方这个问题转换成了计算D的n次方这个问题</p>
<p>D就是对角矩阵, 对于一个对角矩阵来说, 它的幂很容易计算</p>
<p><img src="/Blog/intro/diagonalization_power_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>可以看作是矩阵点乘的结果, 也可以看作是对后面那个矩阵进行一系列初等变换, 这个结论可以推广到n次幂</p>
<p><img src="/Blog/intro/diagonalization_power_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>D的n次幂就是对对角线上所有元素取n次幂后的矩阵, 根本不需要进行矩阵乘法</p>
<p>因此A的n次幂就是</p>
<p><img src="/Blog/intro/diagonalization_power_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>整个过程只进行了<strong>两次矩阵乘法运算</strong>, 其他的运算全都是对一个数字进行n次幂的运算</p>
<p>显然这样计算矩阵的幂是比直接进行矩阵乘法运算简单很多</p>
<p><br></p>
<h3 id="动态系统"><a href="#动态系统" class="headerlink" title="动态系统"></a>动态系统</h3><p>我们为什么要关心矩阵的幂?</p>
<p><img src="/Blog/intro/diagonalization_power_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>如果u<sub>0</sub>是一种状态, A的k次方乘以u<sub>0</sub>就是u<sub>k</sub></p>
<p>为什么u有下角标? 因为它处理的是一种所谓的<strong>动态系统</strong></p>
<p>动态系统就是随着时间不断变化, u这个向量所表征的状态在不断变化, 这个变换是被A这个矩阵所表示的</p>
<p>动态系统这样的方程形式被用于非常多的领域, 比如说经济系统, 物理学的粒子系统, 生态系统, 随机系统等等, 具体可以看马尔可夫链</p>
<p>而这个系统的<strong>核心</strong>就是求A的k次幂</p>
<p>一旦矩阵A可以被对角化, 就可以把原方程转化为</p>
<p><img src="/Blog/intro/diagonalization_power_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而计算D的n次方是非常容易的</p>
<p>更为重要的是, 一旦我们有了对角化这个视角, 其实我们还能对想研究的系统有更深刻的认识</p>
<p>这是因为<strong>A这个变化本质就是D这个变化</strong>, 只不过我们要在P这个坐标系下去观察而已</p>
<p>而D这个矩阵表示的就是u<sub>0</sub>这个向量各个维度随着时间的推移, 每走一个时间点相应怎样变化, D这个矩阵对角线上的元素又都是A的特征值</p>
<p>所以其实: <strong>特征值反应系统的各个分量的变化速率</strong>, 只不过变换实在P这个坐标系下的</p>
<p>这也是特征值另一层隐含的含义, 进一步我们对特征值的分析就可以让我们对系统本身有更深刻的认识, 衍生出: 稳定的系统, 不稳定的系统…等等相应的概念 </p>
<p><br></p>
<h2 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h2><p>注意:</p>
<p>A必须要有n个线性无关的特征向量才可以进行对角化</p>
<p>这一章所说的所有内容都只对方阵适用, 但对于更加一般化的矩阵该怎么办? 下一章将延续这一章的话题来看怎么解决这些问题</p>
<p><br></p>
<p><br></p>
<h1 id="对称矩阵和矩阵的SVD分解"><a href="#对称矩阵和矩阵的SVD分解" class="headerlink" title="对称矩阵和矩阵的SVD分解"></a>对称矩阵和矩阵的SVD分解</h1><p>这一章的主线就是对称矩阵, 借助对阵矩阵可以处理任何矩阵, 把任何矩阵都分解成为我们希望的形式, 即: SVD分解</p>
<p><br></p>
<h2 id="完美的对称矩阵"><a href="#完美的对称矩阵" class="headerlink" title="完美的对称矩阵"></a>完美的对称矩阵</h2><h3 id="什么是对称矩阵"><a href="#什么是对称矩阵" class="headerlink" title="什么是对称矩阵?"></a>什么是对称矩阵?</h3><p><strong>定义: 就是矩阵中所有元素关于主对角线对称</strong></p>
<blockquote>
<p>注意: 主对角线上的元素不一定是一样的</p>
</blockquote>
<p><img src="/Blog/intro/symmetric_matrix.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>就是说主对角线元素随便取, a<sub>ij</sub>和a<sub>ji</sub>的元素是一样的</p>
<p>用数学的方式表达就是: A=A<sup>T</sup></p>
<p>转置操作就是行变列, 列变行. 如果一个矩阵等于它的转置, 就说明它是对称矩阵</p>
<p><br></p>
<h3 id="为什么说对称矩阵是完美的"><a href="#为什么说对称矩阵是完美的" class="headerlink" title="为什么说对称矩阵是完美的"></a>为什么说对称矩阵是完美的</h3><p>首先, 讲对称矩阵, 对称矩阵一定是一个方阵. </p>
<p>对于一个方阵我们就可以求它的<strong>特征值</strong>和<strong>特征向量</strong>, 但是有可能不是<strong>简单特征值</strong></p>
<p>或者求出<strong>复数特征值</strong>, 或者求出<strong>多重特征值</strong></p>
<p>当多重特征值所对应的<strong>特征空间的维度小于重数</strong>时, 也就是所谓的<strong>几何重数小于代数重数</strong>时, 这就为矩阵对角化造成了麻烦, 因为我们无法找到n个线性无关的特征向量来形成矩阵P, 也就无法对角化</p>
<p><br></p>
<p>但是对于对称矩阵来说, 这两种情况完全不用担心, 因为对称矩阵有两个重要的性质(不做证明):</p>
<ul>
<li>对称矩阵的特征值一定是实数</li>
<li>对称矩阵的多重特征值, 其对应的特征空间的维度一定等于重数! (即: 对称矩阵的几何重数等于代数重数)</li>
</ul>
<p>对称矩阵一定有n个线性无关的特征向量</p>
<p>这就意味着对称矩阵一定可以被<strong>对角化</strong></p>
<p><br></p>
<p><br></p>
<h2 id="正交对角化"><a href="#正交对角化" class="headerlink" title="正交对角化"></a>正交对角化</h2><p>对称矩阵不仅一定可以被对角化, 还可以被<strong>正交对角化</strong></p>
<p>什么叫对称矩阵可以被正交对角化?</p>
<p>对角化就是可以把A分解为: PDP<sup>-1</sup>这个形式</p>
<p>把它分解成这个形式是什么意思? 就是在另外一个坐标系P下观察, 就是一个对角矩阵的变化. 而对角矩阵非常简单, 帮助我们研究A矩阵代表的变换. </p>
<p>但是这里我们还是要变换一个视角, 这个视角由矩阵P决定. 而矩阵P是由所有的线性无关的特征向量排列组成的坐标转换矩阵</p>
<p>换句话说, 我们要转换到矩阵P所代表的坐标系中, 但是这个坐标系可能是非常<strong>扭曲</strong>的一个坐标系</p>
<p><img src="/Blog/intro/org_dia.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>就是由图中蓝色箭头(或者说绿色箭头, 他们都是特征向量)所对应的坐标系, 是很扭曲的</p>
<p>我们喜欢什么样的坐标系? 我们喜欢<strong>正交坐标系</strong>, 就像我们平时使用的标准坐标系一样, 所有坐标轴都是相互垂直的, 这种坐标系我们处理起来非常容易</p>
<p><br></p>
<p>对于对称矩阵来说, 它就具有这样的性质</p>
<p><strong>对称矩阵的所有不同的特征值对应的特征向量互相垂直</strong></p>
<p>例如之间提到的两个例子, 这两种变换对应的矩阵都是对称矩阵, 它们对角化后的P坐标系也是正交坐标系</p>
<p><img src="/Blog/intro/org_dia_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="证明-对称矩阵的所有不同的特征值对应的特征向量互相垂直"><a href="#证明-对称矩阵的所有不同的特征值对应的特征向量互相垂直" class="headerlink" title="证明: 对称矩阵的所有不同的特征值对应的特征向量互相垂直"></a>证明: 对称矩阵的所有不同的特征值对应的特征向量互相垂直</h3><p>假设矩阵A的两个特征向量v1, v2对应不同的特征值 λ1, λ2</p>
<p>证明 v1, v2 垂直就相当于证明它们的点乘结果为0</p>
<p><img src="/Blog/intro/org_dia_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>但是如果单单从v1, v2的点乘出发根本用不上λ1, λ2这些量, 因此我们从下面式子出发</p>
<p><img src="/Blog/intro/org_dia_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>λ1 乘以 v1 相当于把v1伸缩 λ1 倍, 但不改变它的方向. 如果v1, v2互相垂直, 则上面的式子也应该为0</p>
<p>注意: v1点乘v2是两个向量的运算, 这里我们把这个运算转换成两个矩阵的运算, 把v1, v2看作是矩阵</p>
<p><img src="/Blog/intro/org_dia_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而 λ1v1 可以看作是 Av1, 根据矩阵转置的性质得到</p>
<p><img src="/Blog/intro/org_dia_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而A是一个<strong>对称矩阵</strong>, 所以A的转置就是A</p>
<p><img src="/Blog/intro/org_dia_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>最后反向推导成向量的点乘</p>
<p><img src="/Blog/intro/org_dia_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>整理一下</p>
<p><img src="/Blog/intro/org_dia_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这两项肯定有一项为0, 而λ1, λ2是两个不同的特征值, 所以λ1- λ2不为0</p>
<p>因此</p>
<p><img src="/Blog/intro/org_dia_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这样就证明出来了对于<strong>对称矩阵</strong>的不同的两个特征值的特征向量互相垂直</p>
<p><br></p>
<p>而对于相同的特征值?</p>
<p>相同特征值的时候, k重特征值的特征空间为k维, 肯定能找到k个互相垂直的特征向量</p>
<p><em>因此对于对称矩阵, 我们可以找到n个互相垂直的特征向量</em></p>
<p><br></p>
<p><img src="/Blog/intro/org_dia_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>为什么用字母Q, 因为QR分解时说过我们习惯用Q表示正交矩阵, 甚至标准正交矩阵(将每一个特征向量标准化即可)</p>
<p>而标准正交矩阵有什么性质? <strong>标准正交矩阵的逆就是它的转置</strong></p>
<p>这样一来就可以写成这种形式</p>
<p><img src="/Blog/intro/org_dia_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个式子就叫做对矩阵A进行正交对角化</p>
<p>所谓正交对角化, 就是在对角化之后保证P是标准正交矩阵</p>
<p><br></p>
<p>除了对称矩阵是否还有其他矩阵可以被正交对角化? 没有</p>
<p>这个结论也非常好证明</p>
<h3 id="证明-如果A可以被正交对角化则A一定是对称矩阵"><a href="#证明-如果A可以被正交对角化则A一定是对称矩阵" class="headerlink" title="证明: 如果A可以被正交对角化则A一定是对称矩阵"></a>证明: 如果A可以被正交对角化则A一定是对称矩阵</h3><p><img src="/Blog/intro/org_dia_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>Q是标准正交矩阵, D是对角矩阵</p>
<p><br></p>
<p><img src="/Blog/intro/org_dia_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个也被称为是<strong>谱定理</strong></p>
<p>在一些领域, 我们将矩阵的特征值或奇异值称为谱</p>
<p>什么是奇异值? 这也是本章重点, 下一小节揭晓</p>
<p><br></p>
<p><br></p>
<h2 id="什么是奇异值"><a href="#什么是奇异值" class="headerlink" title="什么是奇异值"></a>什么是奇异值</h2><p>从讨论行列式开始, 我们一直围绕着方阵打转</p>
<p><strong>特征值, 特征向量, 相似型, 对角化, 对称矩阵, 正交对角化</strong>… 都是基于方阵的</p>
<p>然而很多时候处理的数据都是非方阵的</p>
<p>而且对称矩阵再完美, 它所要求的条件也太苛刻了, 怎么会那么容易处理的矩阵恰好是方阵, 同时正好是对称呢?</p>
<p><strong>对于每一个非方阵来说, 都可以找到一个对称矩阵和它对应</strong>, 而这个矩阵是这样构建的</p>
<p><img src="/Blog/intro/singular_value.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个方阵是一个<strong>对称矩阵</strong>, 简单看一下为什么对称</p>
<p><img src="/Blog/intro/singular_value_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而A<sup>T</sup>的第i行就是A的第i列</p>
<p><img src="/Blog/intro/singular_value_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同理A<sup>T</sup>的第j行就是A的第j列</p>
<p><img src="/Blog/intro/singular_value_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>向量的点乘是遵守交换律的, 所以A<sup>T</sup>A的第i行j列元素等于第j行i列元素</p>
<p>所以A<sup>T</sup>A这个方阵是对称的</p>
<p><br></p>
<p>所以之前说到的所有对于对阵矩阵的优美的性质对A<sup>T</sup>A都是适用的</p>
<p>这意味着:</p>
<p><img src="/Blog/intro/singular_value_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>λ 和 v 依然是<strong>特征值</strong>和<strong>特征向量</strong>, 但他们不再是A的特征值和特征向量</p>
<p>而是从现在开始 λ 和 v 代表的是 A<sup>T</sup>A这个方阵的特征值和特征向量</p>
<p>我们要观察一下</p>
<p><img src="/Blog/intro/singular_value_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>v<sub>i</sub>是A<sup>T</sup>A的一个特征向量</p>
<blockquote>
<p>一个疑问就是我们为什么要让A和A<sup>T</sup>A的一个特征向量相乘?</p>
<p>在后续就会看到这个操作是非常有意义的</p>
<p>在最后还会给出这个操作的几何含义</p>
</blockquote>
<p><img src="/Blog/intro/singular_value_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>再一次把向量的点乘操作转为矩阵的操作</p>
<p>换句话说, Av<sub>i</sub>原本是一个列向量, 把它看作是n <em> 1的矩阵, 它的转置就是1 </em> n 的矩阵</p>
<p><img src="/Blog/intro/singular_value_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而 v<sub>i</sub> 是标准特征向量(进行过归一化处理), 因此 v<sub>i</sub> 的模为 1</p>
<p><img src="/Blog/intro/singular_value_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>非常神奇地推导出:</p>
<p><img src="/Blog/intro/singular_value_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p> λ<sub>i</sub> 和 v<sub>i</sub> 本身是 A<sup>T</sup>A这个矩阵的特征值和特征向量, 但是这二者和A这个矩阵本身存在着这样的联系</p>
<p>因此A<sup>T</sup>A这个矩阵的特征值一定<strong>大于等于</strong>0, 同样还可以为这个特征值开根号:</p>
<p><img src="/Blog/intro/singular_value_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>通常记作σ<sub>i</sub> , 也被称为奇异值(Singular Value)</p>
<p>σ<sub>i</sub> 是 λ<sub>i</sub>开根的结果, 而 Av<sub>i </sub>模的平方就是 λ<sub>i</sub></p>
<p>所以: <strong>奇异值就是向量 Av<sub>i</sub> 的长度</strong></p>
<p><br></p>
<p>我们为什么对Av<sub>i</sub> 这个向量这么好奇?</p>
<p>这个向量其实是有非常重要的作用的</p>
<p><br></p>
<p><br></p>
<h2 id="奇异值的几何含义"><a href="#奇异值的几何含义" class="headerlink" title="奇异值的几何含义"></a>奇异值的几何含义</h2><p><img src="/Blog/intro/meaning_of_singular_value.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>Av<sub>i</sub>是有意义的, 它的意义就是:</p>
<p><img src="/Blog/intro/meaning_of_singular_value_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>零向量除外, 因为 λ<sub>i</sub> 可能为 0</p>
<p><br></p>
<h3 id="证明-Avi-是A的列空间的一组正交基"><a href="#证明-Avi-是A的列空间的一组正交基" class="headerlink" title="证明 {Avi} 是A的列空间的一组正交基"></a>证明 {Av<sub>i</sub>} 是A的列空间的一组正交基</h3><h4 id="证明正交性"><a href="#证明正交性" class="headerlink" title="证明正交性"></a>证明正交性</h4><p>即证明下面式子的结果为0</p>
<p><img src="/Blog/intro/meaning_of_singular_value_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>证明方法和上小节相似, 一样是把向量的点乘转化为矩阵的点乘</p>
<p><img src="/Blog/intro/meaning_of_singular_value_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因为 v<sub>i</sub> 和 v<sub>j</sub>  是对称矩阵A<sup><em>T</em></sup>A的<strong>标准特征向量</strong>, 它们是互相垂直的, 换句话说, 它们点乘的结果为0</p>
<p><img src="/Blog/intro/meaning_of_singular_value_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h4 id="证明-Avi-是A的列空间的一组基"><a href="#证明-Avi-是A的列空间的一组基" class="headerlink" title="证明{Avi} 是A的列空间的一组基"></a>证明{Av<sub>i</sub>} 是A的列空间的一组基</h4><p><img src="/Blog/intro/meaning_of_singular_value_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这是因为v<sub>i<sub>是对称矩阵A<sup><em>T</em></sup>A的<strong>标准特征向量</strong>, 所以这组向量是两两互相垂直的一组标准正交基, 这组正交基一共有n个, 所以它是n维空间的一组标准正交基</p>
<p>换句话说在n维空间中, 我们任意拿出一个向量x, 都可以表示成v这一组向量的线性组合</p>
<p><img src="/Blog/intro/meaning_of_singular_value_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>在这种情况下, 要研究A的<strong>列空间</strong>, A的列空间的向量都是什么样子的? 假设列空间的任意向量y</p>
<p><img src="/Blog/intro/meaning_of_singular_value_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>A这个矩阵是m * n的, 所以它可以乘以有n个元素的向量x, 得到的结果向量是m维的向量, 就是y向量</p>
<p>之前又说任意一个n维的x向量可以表示为v的线性组合</p>
<p><img src="/Blog/intro/meaning_of_singular_value_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>所以Ax这个向量, 也就是y这个向量就可以写作</p>
<p><img src="/Blog/intro/meaning_of_singular_value_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们就得到了A空间中, 任意一个向量y都可以写成这样一个式子</p>
<p>这组式子本身就已经是Av<sub>i</sub>的线性组合了</p>
<blockquote>
<p>不过还是要注意: Av<sub>i</sub>这组向量不一定两两线性无关, 关键就是在于有一些Av<sub>i</sub>的模可能为0</p>
<p>如果把这些向量刨去的话, 剩下的向量一定是两两线性无关的, 因为它们是两两正交(垂直)的, 上面证明过了</p>
</blockquote>
<p>因此更严谨地, 假设A<sup>T</sup>A有r个非零特征值, 则有n-r个等于0特征值, 这n-r个特征值所对应Av<sub>i</sub>的结果一定为0, 因为 λ<sub>i</sub> 表示的就是Av<sub>i</sub>模的平方, 当 λ<sub>i</sub> = 0, Av<sub>i</sub>一定是零向量</p>
<p><img src="/Blog/intro/meaning_of_singular_value_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>我们对Av<sub>i</sub>感兴趣不仅仅因为它的模是 λ<sub>i</sub> 的开根, 更关键的是这组向量如果把0向量都扔掉的话可以用来描述A, 它是A的列空间的一组正交基</p>
<p> 我们一直强调要把 λ<sub>i</sub> = 0的情况扔掉, 而 λ<sub>i</sub> = 0 的时候就是奇异值等于0的时候.</p>
<blockquote>
<p>注意奇异值是和矩阵A对应的, 而 λ<sub>i</sub> 是和 A<sup>T</sup>A 对应的</p>
</blockquote>
<p>因此, 如果A有r个不为0的奇异值, 或者说 A<sup>T</sup>A 有r个不为0的特征值, 则:</p>
<p><img src="/Blog/intro/meaning_of_singular_value_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>其实这个两个叙述是一样的:</p>
<p><img src="/Blog/intro/meaning_of_singular_value_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>同时我们也知道了A的列空间所对应的这组基中一共有r个向量, r为A所对应的奇异值中不为0的奇异值. 这意味着这组基所对应的空间是一个r维空间</p>
<p>换句话说, A的列空间的维度为r; rank(A) = r</p>
<p><br></p>
<p>奇异值本身就是Av<sub>i</sub>对应的长度, 把每一个 Av<sub>i</sub> 都除以奇异值就相当于把这些向量都进行归一化, 所得到的结果就是A的列空间的一组<strong>标准正交基</strong></p>
<p><img src="/Blog/intro/meaning_of_singular_value_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>对于这一组向量, 其实我们也把等于0的奇异值都扔掉了, 所以我们通常在做这个处理的时候求出矩阵A所对应的奇异值后, 会把奇异值从大到小排序</p>
<p>这样所有等于0的奇异值就都在最后了</p>
<p>我们通常又把Av<sub>i</sub>除以特征值写作u, 就是另外起一个名字, 称呼起来方便</p>
<p><img src="/Blog/intro/meaning_of_singular_value_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/meaning_of_singular_value_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h2 id="奇异值的SVD分解"><a href="#奇异值的SVD分解" class="headerlink" title="奇异值的SVD分解"></a>奇异值的SVD分解</h2><p>经过前面几个小节的铺垫, 终于我们可以来看大名鼎鼎的<strong>矩阵的SVD分解 - (Singular Value Decomposition)</strong></p>
<p>不是把矩阵分解成SVD三部分, SVD是这个矩阵分解方式的英文名称首字母缩写, 它的意思是<strong>矩阵的奇异值分解</strong></p>
<p>为什么矩阵的SVD分解这么重要? <strong>因为它没有任何限制, 对于任意形状的矩阵都适用</strong></p>
<blockquote>
<p>当然把矩阵分成几部分肯定是有原因的</p>
<p>具体SVD分解是做什么的后续介绍</p>
</blockquote>
<p>SVD分解是将任意一个矩阵A分解成三部分</p>
<p><img src="/Blog/intro/SVD_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>由于A可以是任意形状, 所以</p>
<p><img src="/Blog/intro/SVD_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>实际上这三部分具体是什么之前都介绍过了</p>
<p><br></p>
<h3 id="V"><a href="#V" class="headerlink" title="V"></a>V</h3><p><img src="/Blog/intro/SVD_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>之前说A<sup>T</sup>A是对称矩阵, 则它一定有n个互相垂直的<strong>特征向量</strong>, 将这些特征向量标准化后最终得到的结果V是一个<strong>标准正交矩阵</strong></p>
<p>而在SVD分解中取的值V的转置, 既然V是一个<strong>标准正交矩阵</strong>, <strong>它的转置本质就是它的逆</strong></p>
<p><br></p>
<h3 id="U"><a href="#U" class="headerlink" title="U"></a>U</h3><p><img src="/Blog/intro/SVD_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>每一个u都是上一小节介绍的u</p>
<p><img src="/Blog/intro/SVD_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>注意: r &lt;= m, r &lt;= n, 这是因为这r个u都不是零向量, 他们就组成了A的列空间所对应的一组(标准正交)基, 也就是说r就是A的秩, 因此 r &lt;=m 同时 r &lt;= n</p>
<p>我们把前 r 个 u 放在 U 的前 r 列, 后面就还后可能剩余一些位置, 剩余的位置怎么办?</p>
<p><img src="/Blog/intro/SVD_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>向量u有多少个元素? 它有m个元素, 这是因为v有n个元素而A是m * n的矩阵, 所以u是含有m个元素的向量</p>
<p>但是现在我们只有r个u, 这r个u虽然是A的列空间的一组基, 但它肯定不是整个m维空间的一组基</p>
<p>这种情况下我们怎样构成方阵U? 后面的u使用Gram-Schmidt进行拓展</p>
<blockquote>
<p>现在相当于是对于整个m维空间, 我们已经直到有r个标准正交向量了, 我们就可以用Gram-Schmidt法则不断去求和这r个向量也互相垂直的标准正交向量, 直到得到整个m维空间的一组标准正交基</p>
</blockquote>
<p>不过其实后面我们就会看到, 对于这个矩阵的SVD分解, 其实我们后面添加到m-r个向量是没有作用的</p>
<p>U和V都是标准正交矩阵</p>
<p><br></p>
<h3 id="∑"><a href="#∑" class="headerlink" title="∑"></a>∑</h3><p><img src="/Blog/intro/SVD_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>∑是一个奇异值矩阵, 很好理解, 我们一直用σ表示A的奇异值, 只不过对于A来说能找到r个非零的奇异值, 但∑是m * n的矩阵, 这r个非零的奇异值怎么分布在这∑矩阵中?</p>
<p>我们把这r个奇异值用对角矩阵的方式塞入∑这个矩阵中, 但是有可能塞不满, 这是因为r&lt;=m. </p>
<p><img src="/Blog/intro/SVD_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p> 塞不满的地方填0即可, 可以简单的分块, 有四部分, 有三部分都是0, 而左上角部分是对角矩阵, 主对角线上的元素就是矩阵A的r个非零的奇异值</p>
<p><img src="/Blog/intro/SVD_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>和码放矩阵U一样, 这r个奇异值要从大到小排好序, 和前面码放U矩阵每一列的 u<sub>i</sub> 要是相对应的</p>
<p><br></p>
<p>这样就讲清楚了对于矩阵的SVD分解中将矩阵A所分解成的这三部分, 每一部分都是什么样子</p>
<p>下一个问题就是为什么对于一个矩阵A, 它等于这三部分的乘积</p>
<h3 id="证明-矩阵等于这三部分的乘积"><a href="#证明-矩阵等于这三部分的乘积" class="headerlink" title="证明: 矩阵等于这三部分的乘积"></a>证明: 矩阵等于这三部分的乘积</h3><p>和证明矩阵的对角化一样, A=PDP<sup>-1</sup>, P逆不好处理, 就把它移到等式的左边, 这里V是一个标准正交矩阵, V的转置就是V的逆</p>
<p>换句话说, 我们只需要证明 AV=U∑ 即可</p>
<p><br></p>
<h4 id="等式左边-1"><a href="#等式左边-1" class="headerlink" title="等式左边"></a>等式左边</h4><p>首先回忆一下V中的每个列向量:</p>
<p><img src="/Blog/intro/SVD_proof_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>所以AV就是</p>
<p><img src="/Blog/intro/SVD_proof_2.jpg" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而它和u有密切的关系</p>
<p><img src="/Blog/intro/SVD_proof_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此矩阵可以写成</p>
<p><img src="/Blog/intro/SVD_proof_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>不过注意, 之前说过假设A只有r个<strong>非零奇异值</strong>, 对于其他为0的奇异值, 乘以 u<sub>i</sub> 结果一定为0, 随意最后AV化成的就是这个样子</p>
<p>前r列都是 σ<sub>i</sub>u<sub>i</sub> , 后面的部分就都变成了0</p>
<p><br></p>
<h4 id="等式右边-1"><a href="#等式右边-1" class="headerlink" title="等式右边"></a>等式右边</h4><p><img src="/Blog/intro/SVD_proof_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里又可以使用矩阵乘法的列式角, 每一次前面的矩阵取出一列, 后一个矩阵取出一行, 这一列一行相乘的结果是一个矩阵</p>
<p>左边有m列, 右边有m行, 我们就得到m个矩阵, 把m个矩阵相加的结果</p>
<p>而一列乘以一行时, 一行当中只有第i个元素是非零的, 所以最终得到的结果之后第i列是有值的</p>
<p><img src="/Blog/intro/SVD_proof_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>∑只有前r行是不全为0的所以最终的结果矩阵只有前r列是有值的, 后面的所有列就都为0了</p>
<p>等式左侧等于等式后侧</p>
<p>这样我们就证明出来了对于任意矩阵A都可以写成是 U∑V<sup>T</sup> 这个形式</p>
<p><br></p>
<h3 id="整理SVD的过程"><a href="#整理SVD的过程" class="headerlink" title="整理SVD的过程"></a>整理SVD的过程</h3><p>如果已知m * n的矩阵A, 如何进行SVD分解, 即算法的过程</p>
<ol>
<li>首先求A<sup>T</sup>A的特征值和特征向量</li>
<li>非零特征值开根后 ( 奇异值 ) 得到m <em> n的∑, 奇异值从大到小排序沿对角线码放. ∑也被称为<em>*奇异值矩阵</em></em></li>
<li>特征向量标准化后得到的就是n * n的V, 要和码奇异值的顺序相对应</li>
<li>用 u<sub>i</sub> = Av<sub>i</sub> / σ<sub>i</sub> 的方式求出前r个 u<sub>i</sub> , 再经过Gram-Schmidt扩展得到m * m的U, 同样 u<sub>i</sub> 要和奇异值的顺序对应</li>
</ol>
<p><br></p>
<p>这一节主要是证明这样的分解是成立的, 那么SVD分解有什么用?</p>
<p><br></p>
<p><br></p>
<h2 id="SVD分解的应用"><a href="#SVD分解的应用" class="headerlink" title="SVD分解的应用"></a>SVD分解的应用</h2><p>矩阵SVD分解的应用非常广泛, 甚至可以说用到线性代数的地方就逃离不开SVD分解的具体应用, 这里只简单推导SVD分解的两个应用</p>
<p><br></p>
<h3 id="把矩阵A看作是一个变换"><a href="#把矩阵A看作是一个变换" class="headerlink" title="把矩阵A看作是一个变换"></a>把矩阵A看作是一个变换</h3><p>把矩阵A看作是一个变换, 把它看作是 U∑V<sup>T</sup> 有什么意义?</p>
<p>如果A是任意一个m * n的矩阵, 虽然把A看作是变换时A为方阵居多, 但这一小节的推导中, 为了不失一般性, 还是让A是非方阵</p>
<p>A是对n维向量作变换, 得到的结果是m维的向量, 也就是A这个变换是把一个n维向量转换成m维的向量</p>
<p>这里要注意V这个矩阵</p>
<p>V 是 n 维空间的一个标准正交基, 因此任意一个n维向量都可以表示为V这个矩阵中的每一个列向量相应的一个线性组合</p>
<p><img src="/Blog/intro/meaning_of_singular_value_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>也可以整理为矩阵V乘以向量k</p>
<p><img src="/Blog/intro/SVD_proof_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>如果A是一个变换, 任何一个向量x通过A的变换后得到的结果是<strong>Ax</strong></p>
<p>此时可以把A分解成 U∑V<sup>T</sup> </p>
<p><img src="/Blog/intro/SVD_proof_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>而x又可以写成Vk, 所以</p>
<p><img src="/Blog/intro/SVD_proof_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>V是<strong>标准正交矩阵</strong>, V的逆就是V的转置</p>
<p><img src="/Blog/intro/SVD_proof_10.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时就要分析一下∑是一个什么样的矩阵? 它是一个类似与对角矩阵的矩阵, 它可以分为四部分, 左上角是一个对角矩阵</p>
<p>因此左上角这一部分非零的奇异值就将和k这个向量中前r个元素进行乘法, 剩下的结果为0</p>
<p><img src="/Blog/intro/SVD_proof_11.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>注意: 0代表后面所有元素为0, 不是只有一个0</p>
</blockquote>
<p><img src="/Blog/intro/SVD_proof_12.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个形式和原来x的形式非常像, 区别在于</p>
<ol>
<li><p>原来x这个向量是被V这个标准正交矩阵相乘</p>
<p>而Ax这个向量是被U这个标准正交矩阵相乘</p>
</li>
<li><p>原本x后面的部分就是这些k</p>
<p>而现在Ax后面的这个向量每一个 k<sub>i</sub> 都和 σ<sub>i</sub> 相乘, 也就是被一个奇异值相乘, 后面的那些零也可以理解为被等于0的奇异值相乘</p>
<p>因此我们可以这样解读:</p>
</li>
</ol>
<p>原来的x, 它在V这个坐标系下相应的坐标是 k<sub>1</sub>, k<sub>2</sub>…k<sub>n</sub> , 经过A转换之后我们可以这样看待A转换后的向量x, 在U这个坐标系下看相应每一个维度的坐标值不再是k<sub>1</sub>, k<sub>2</sub>…k<sub>n</sub>, 而要再拉伸<strong>奇异值</strong>那么多倍</p>
<p>如果我们仅仅用矩阵乘法的视角看, 这个变换可能非常扭曲, 但如果从U这个坐标系的视角去看的话, 它的本质就是在这个坐标系下, 每一个维度的值再拉伸奇异值那么多倍</p>
<p><img src="/Blog/intro/SVD_proof_13.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="把矩阵A看成是一片数据"><a href="#把矩阵A看成是一片数据" class="headerlink" title="把矩阵A看成是一片数据"></a>把矩阵A看成是一片数据</h3><p>A是一片m * n的二维数据, SVD分解又有什么意义?</p>
<p><img src="/Blog/intro/SVD_proof_14.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>U∑的值之前已经计算过了, 因此A就是</p>
<p><img src="/Blog/intro/SVD_proof_15.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时又可以用矩阵乘法的列视角看待</p>
<p><img src="/Blog/intro/SVD_proof_16.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>此时uv不是点乘而是矩阵相乘</p>
<p>问题来了, 把A写成这样有什么意义?</p>
<p><img src="/Blog/intro/SVD_proof_17.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们在进行奇异值分解的时候已经将这些奇异值从大到小排列了, 所以可以理解这些奇异值是一些权值</p>
<p>也就是说A这个矩阵可以理解为是一系列m * n的矩阵相加, 每一个矩阵都有一个权值, 第一个矩阵的权值最大由第一个奇异值表示, 以此类推</p>
<p>其实后面还有m * n的矩阵, 但它们权值为0</p>
<p>从这个视角看, 我们就可以很自然地想象, 我们是不是不仅可以把奇异值为0的矩阵拿掉, 而且把那些奇异值比较小的矩阵也给拿掉, 相应对于A来说, 它的改动并不是很大</p>
<p>这样从某种程度来讲我们就做到了对A这个数据进行<strong>压缩, 去噪(把权值较小的矩阵看作是噪音), 降维(把A看作是r维的数据, 拿去更多的一些数据就可以理解为是降维)</strong></p>
<blockquote>
<p>值得一提的是这个应用可以非常好地直接应用在图像领域, 如果矩阵A表示的是一个高为m, 宽为n这样的一个图像的话</p>
<p>把奇异值较小的项扔掉的话, 剩下的项依然可以很好地表达这个图像的语义 (人的肉眼还能看懂图像的意思是什么), 但是表征这个图像所使用的数据量, 存储空间都变低了</p>
</blockquote>
<p>其实可以试一试随着扔的项越来越多, 观察图像是怎样变得越来越模糊</p>
<p><br></p>
<h3 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h3><p>线性代数领域中可以定义伪逆, 只有方阵有逆, 但对于非方阵来说可以定义一个伪逆, 它的性质和逆非常的像, 在非方阵上为我们可以用伪逆代替逆, 伪逆背后的数学依据也是和SVD有关的</p>
<p>推荐系统, 自然语言处理, 搜索引擎….</p>
<p><br></p>
<p><br></p>
<h2 id="更广阔的线性代数世界"><a href="#更广阔的线性代数世界" class="headerlink" title="更广阔的线性代数世界"></a>更广阔的线性代数世界</h2><p>这门课很多细节没有推导证明</p>
<p>投影矩阵的推导; 几何重数与代数重数的相关证明</p>
<p>数值分析</p>
<p>非欧几里得空间</p>
<p>广义向量空间</p>
<p>广义内积空间</p>
<p><br></p>
<p>线性代数和其他学科结合:</p>
<p>统计学: PCA, 最小二乘法</p>
<p>微积分: 差分方程, 微分方程, 最优化原理</p>
<p>计算机科学: 机器学习, 推荐系统, 搜索引擎, 图像学</p>
<p>物理学, 经济学, 电子学… …</p>
<p><br></p>
<p><br></p>
<h1 id="补充-复数-complex-number"><a href="#补充-复数-complex-number" class="headerlink" title="补充: 复数(complex number)"></a>补充: 复数(complex number)</h1><p>我们把形如z=a+bi（a,b均为实数）的数称为复数，其中a称为实部(real component)，b称为虚部(imaginary component)，i称为虚数单位</p>
<p>对i的定义是: i^2 = -1 或 i = root(-1)</p>
<p><img src="/Blog/intro/virtual_number.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>复数不能表示在一个数轴上, 因为它有两个未知数a和b. 它需要表示在一个平面上</p>
<p><img src="/Blog/intro/virtual_number_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h2 id="复数的定义与性质"><a href="#复数的定义与性质" class="headerlink" title="复数的定义与性质"></a>复数的定义与性质</h2><p><br></p>
<h3 id="复数的加法"><a href="#复数的加法" class="headerlink" title="复数的加法"></a>复数的加法</h3><p><img src="/Blog/intro/virtual_number_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="复数的共轭-Conjugate-of-complex-number"><a href="#复数的共轭-Conjugate-of-complex-number" class="headerlink" title="复数的共轭(Conjugate of complex number)"></a>复数的共轭(Conjugate of complex number)</h3><p><img src="/Blog/intro/virtual_number_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="复数的乘法"><a href="#复数的乘法" class="headerlink" title="复数的乘法"></a>复数的乘法</h3><p><img src="/Blog/intro/virtual_number_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="复数的除法"><a href="#复数的除法" class="headerlink" title="复数的除法"></a>复数的除法</h3><p><img src="/Blog/intro/virtual_number_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这里只是展示计算方法, 具体数值就不计算了</p>
<p><br></p>
<h3 id="复数的模-Modules-of-complex-number"><a href="#复数的模-Modules-of-complex-number" class="headerlink" title="复数的模(Modules of complex number)"></a>复数的模(Modules of complex number)</h3><p>|Z| 定义为: 原点到Z的距离</p>
<p><img src="/Blog/intro/virtual_number_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/virtual_number_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="复数与共轭的相乘"><a href="#复数与共轭的相乘" class="headerlink" title="复数与共轭的相乘"></a>复数与共轭的相乘</h3><p>一个复数乘以它的共轭</p>
<p><img src="/Blog/intro/virtual_number_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="用Polar-Form-表示复数"><a href="#用Polar-Form-表示复数" class="headerlink" title="用Polar Form 表示复数"></a>用Polar Form 表示复数</h3><p><img src="/Blog/intro/virtual_number_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>向量我们可以通过它的长度和方向来进行表示, 而复数也是如此</p>
<p>如果 z = a+bi, 则 z 可以用 r 和 θ 表示. r为到原点的距离</p>
<p><img src="/Blog/intro/virtual_number_9.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><strong>我们为什么要使用这个表示法?</strong></p>
<p>因为这样我们就可以通过下面的方法<strong>可视化</strong>理解复数的乘法和除法</p>
<p><img src="/Blog/intro/complex_number.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>有根据三角函数两角差和公式:</p>
<p><img src="/Blog/intro/complex_number_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>因此可以把z乘以w的等式整理为</p>
<p><img src="/Blog/intro/complex_number_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>就是说z乘w在平面的表示为: 从原点射向(alpha+beta)这个角度的线段, 长度为z和w长度的乘积</p>
<p><img src="/Blog/intro/complex_number_7.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>即: <strong>幅角相加, 模长相乘</strong></p>
<p><br></p>
<p>同样除法也可以这样理解:</p>
<p><img src="/Blog/intro/complex_number_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>即:<strong>幅角相减, 模长相除</strong></p>
<p><img src="/Blog/intro/complex_number_8.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<h3 id="De-Moivre’s-Theorem"><a href="#De-Moivre’s-Theorem" class="headerlink" title="De Moivre’s Theorem"></a>De Moivre’s Theorem</h3><p>通过上面的结论我们就可以快速地计算幂(power)</p>
<p>如果w=z, 则w*z就是z的平方</p>
<p><img src="/Blog/intro/complex_number_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这就是Demoivre’s Theorem</p>
<p><br></p>
<p>ex)</p>
<p><img src="/Blog/intro/complex_number_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>抽象一下上面的步骤, 就可以得到 w^n=z 的通解, z和n已知</p>
<p><img src="/Blog/intro/complex_number_6.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><br></p>
<h1 id="补充-马尔可夫链-Markov-Chain"><a href="#补充-马尔可夫链-Markov-Chain" class="headerlink" title="补充: 马尔可夫链(Markov Chain)"></a>补充: 马尔可夫链(Markov Chain)</h1><p>是特征值和特征向量的一个应用</p>
<p>在具体讲解之前要先介绍一些定义:</p>
<h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><ul>
<li>动态系统 (dynamic system)<ul>
<li>Finite collection of variables that change with respect time t, 随着时间t进行变化的变量的有限集合</li>
</ul>
</li>
<li>变量状态 (state of variable)<ul>
<li>The value of variable at time t, 变量在时间t时的值</li>
</ul>
</li>
<li>系统状态 (state of the system)<ul>
<li>A vector that keeps track of all the states of the variable, 一个跟踪变量状态的向量</li>
</ul>
</li>
</ul>
<p>例如: </p>
<p><img src="/Blog/intro/state_of_system.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<ul>
<li>马尔可夫链 (Markov Chain)<ul>
<li>This is a dynamical system where the state of a variable, x<sub>j</sub>(t) = probability that the system is in stack j at time t. 一个线性系统, x<sub>j</sub>(t) 代表在时间t系统在j的可能性. 0 &le; x<sub>j</sub>(t) &le; 1</li>
</ul>
</li>
</ul>
<p><br></p>
<p>我们的目的就是研究当 t 趋近无穷时的系统</p>
<p><br></p>
<h2 id="从一个例子出发"><a href="#从一个例子出发" class="headerlink" title="从一个例子出发"></a>从一个例子出发</h2><p>假设有两家咖啡店 A 和 B, 200个人构成了系统, 最开始有120人去A店, 80人去B店.</p>
<p>每过一段时间t, A店就会有30%的人取B店, 70%留在A店, B店有20%的人去A店, 80%留在B店</p>
<p><img src="/Blog/intro/markov_chain.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>我们想知道哪家店最后掌控最大的市场, 或者说当t趋近无穷有多少人选择A店, 多少人选择B店</p>
<p><img src="/Blog/intro/markov_chain_1.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>x<sub>1</sub>(t) 代表选择店A的可能性</p>
<p>x<sub>2</sub>(t) 代表选择店B的可能性</p>
<p><br></p>
<p><img src="/Blog/intro/markov_chain_2.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>这个乘法很像矩阵乘法</p>
<p><img src="/Blog/intro/markov_chain_3.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>同理</p>
<p><img src="/Blog/intro/markov_chain_4.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>可以看出P的第一列是A的可能(和为1), 第二列为B的可能(和为1), 对于P来说, P<sub>1</sub><sub>1</sub>代表从1(A)到1(A)的概率, P<sub>2</sub><sub>1</sub>代表从1(A)到2(B)的概率</p>
<p>因此, 通常我们认为P<sub>i</sub><sub>j</sub>为下一步从 状态j 到 状态i 的可能</p>
<p><br></p>
<p>定义: 正则转移矩阵(regular transition matrix) </p>
<p>我们说转移矩阵P 是regular 如果某个正整数k使得P^k的所有元素大于0</p>
<ul>
<li>单位矩阵是转移矩阵, 因为每一列的和为1, 但不regular因为无论k取多少有元素0</li>
<li>P=[ [0.5  1],  [0.5  0] ] 是正则转移矩阵, 因为P的平方所有元素大于0, 同时P的每一列的和都为1</li>
</ul>
<p><br></p>
<p><strong>定理</strong>: 如果P是<strong>转移</strong>矩阵, <strong>X(0)</strong> 是任意初始条件, 那么</p>
<ol>
<li>存在唯一向量q使得Pq=q(q是一个概率向量(probability vector), q的所有元素和为1)</li>
<li><strong>X(0), X(1)…X(t)</strong> 向<strong>q</strong>收敛</li>
</ol>
<p><br></p>
<p>回到问题, </p>
<p><img src="/Blog/intro/markov_chain_5.PNG" srcset="/img/loading.gif" lazyload alt=""></p>
<p>已知有q使得Pq = q, 求特征空间. </p>
<p>可以解出特征空间是(2/3, 1)这个向量所在直线, 进行归一化后q=(0.4 0.6)</p>
<p>就是说40%的人在A店, 60%的人在B店</p>
<p><br></p>
<h3 id="证明-如果转移矩阵P的每一行的和为x-则P有特征值x"><a href="#证明-如果转移矩阵P的每一行的和为x-则P有特征值x" class="headerlink" title="证明: 如果转移矩阵P的每一行的和为x, 则P有特征值x"></a>证明: 如果转移矩阵P的每一行的和为x, 则P有特征值x</h3><p>如果P有特征值x, 则det(P-xI) = 0, 则有P-XI不可逆</p>
<p>因此只要证明P-XI不可逆即可,</p>
<p>同时P-XI这个矩阵每一行的和为0(x-x), 因此如果这个矩阵乘以向量(1, 1, … 1), 结果为0矩阵</p>
<p>因此P-XI不可逆, 因此P有特征值x</p>
<h3 id="证明-如果转移矩阵P的每一列的和为x-则P有特征值x"><a href="#证明-如果转移矩阵P的每一列的和为x-则P有特征值x" class="headerlink" title=" 证明: 如果转移矩阵P的每一列的和为x, 则P有特征值x"></a><br> 证明: 如果转移矩阵P的每一列的和为x, 则P有特征值x</h3><p>此时我们希望同样希望证明det(P-xI) = 0, 而这个行列式等于det((P-xI)的转置)</p>
<p>(P-xI)的转置每一行为0, 通过上面证明的结论就可以得证</p>
<p><br></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%95%B0%E5%AD%A6/">数学</a>
                    
                      <a class="hover-with-bg" href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">线性代数</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2020/03/06/play-with-linear-algebra-4/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">线性代数补充知识</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2019/12/20/play-with-linear-algebra-2/">
                        <span class="hidden-mobile"><专为程序员设计的线性代数>学习笔记(2)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
