

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/Blog/img/favicon.ico">
  <link rel="icon" href="/Blog/img/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Daolin">
  <meta name="keywords" content="">
  
    <meta name="description" content="决策树 信息熵与基尼系数">
<meta property="og:type" content="article">
<meta property="og:title" content="python3入门机器学习(10)-决策树">
<meta property="og:url" content="https://daolinzhou.github.io/Blog/2020/05/16/machine-learning-10/">
<meta property="og:site_name" content="Daolin&#39;s Repository">
<meta property="og:description" content="决策树 信息熵与基尼系数">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/decision_tree_2.PNG">
<meta property="article:published_time" content="2020-05-16T07:05:12.000Z">
<meta property="article:modified_time" content="2020-05-17T23:06:46.151Z">
<meta property="article:author" content="Daolin">
<meta property="article:tag" content="python3">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daolinzhou.github.io/Blog/intro/decision_tree_2.PNG">
  
  
  <title>python3入门机器学习(10)-决策树 - Daolin&#39;s Repository</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/Blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"daolinzhou.github.io","root":"/Blog/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/Blog/local-search.xml"};
  </script>
  <script  src="/Blog/js/utils.js" ></script>
  <script  src="/Blog/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/Blog/">
      <strong>Daolin&#39;s Repo</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/Blog/intro/roaming.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="python3入门机器学习(10)-决策树">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-05-16 00:05" pubdate>
        2020年5月16日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      13k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      109 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">python3入门机器学习(10)-决策树</h1>
            
            <div class="markdown-body">
              <link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><p>决策树</p>
<p>信息熵与基尼系数</p>
<span id="more"></span>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="决策树算法的思路"><a href="#决策树算法的思路" class="headerlink" title="决策树算法的思路"></a>决策树算法的思路</h2><p>决策树: 非参数学习的算法</p>
<p>其实决策树的思路在生活中并不少见, 甚至不自觉地使用这种思路进行判断</p>
<p><br></p>
<p>假设一个公司招聘算法工程师, 那么他们在招聘时可能就是采用这样的流程:</p>
<p><img src="/Blog/intro/decision_tree.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这样一个过程形成了树的结构, 这棵树的叶子节点的位置就是决策</p>
<p>这个决策也可以理解为对应聘者的信息进行了分类, 分成录用或者考察两类.</p>
<p>这就是决策树, 它有计算机领域中树结构所有的性质: 根节点, 叶子节点, 树的深度(上图的树的深度为3, 最多进行3次判断就能进行分类)</p>
<p><br></p>
<p>上面的例子中, 每一个节点判断时都可以用Yes或者No来回答, 实际上真实的数据都是具体的数值</p>
<p>那么对于具体的数值决策树是怎么表征的?</p>
<p>现用scikit-learn包装的决策树算法进行具体的分类, 然后根据分类的结果再具体学习一下决策树</p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<br><br><span class="hljs-comment"># 画图函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_decision_boundary</span>(<span class="hljs-params">model, axis</span>):<br>    x0, x1 = np.meshgrid(<br>        np.linspace(axis[<span class="hljs-number">0</span>], axis[<span class="hljs-number">1</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">1</span>] - axis[<span class="hljs-number">0</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>        np.linspace(axis[<span class="hljs-number">2</span>], axis[<span class="hljs-number">3</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">3</span>] - axis[<span class="hljs-number">2</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>    )<br>    X_new = np.c_[x0.ravel(), x1.ravel()]<br>    y_predict = model.predict(X_new)<br><br>    zz = y_predict.reshape(x0.shape)<br>    custom_cmap = ListedColormap([<span class="hljs-string">&#x27;#EF9A9A&#x27;</span>, <span class="hljs-string">&#x27;#FFF59D&#x27;</span>, <span class="hljs-string">&#x27;#90CAF9&#x27;</span>])<br><br>    plt.contourf(x0, x1, zz, cmap=custom_cmap)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    iris = datasets.load_iris()<br>    X = iris.data[:, <span class="hljs-number">2</span>:]<br>    y = iris.target<br><br>    plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/decision_tree_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 指定最大深度为2, 评判标准为熵</span><br>   dt_clf = DecisionTreeClassifier(max_depth=<span class="hljs-number">2</span>, criterion=<span class="hljs-string">&quot;entropy&quot;</span>)<br>   dt_clf.fit(X, y)<br><br>   plot_decision_boundary(dt_clf, axis=[<span class="hljs-number">0.5</span>, <span class="hljs-number">7.5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])<br>   plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>   plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>   plt.scatter(X[y == <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>   plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/decision_tree_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>什么是决策树:</p>
<p><img src="/Blog/intro/decision_tree_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>此时, 数据的特征就全都是数据特征, 每一个特征都是实数. 有两个特征, 横轴x和纵轴y</p>
<p><img src="/Blog/intro/decision_tree_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>如果 x&lt;2.4 分类为蓝色的点, 反之再判断如果 y&lt;1.8 分类为橙色的点, 反之分类为绿色的点</p>
<p>这样就形成了决策树</p>
<p><br></p>
<h3 id="决策树算法的特点"><a href="#决策树算法的特点" class="headerlink" title="决策树算法的特点"></a>决策树算法的特点</h3><p>总结一下决策树:</p>
<ul>
<li>非参数学习算法</li>
<li>可以解决分类问题</li>
<li>天然可以解决多分类问题</li>
<li>也可以解决回归问题 (依然是创建这样一颗树, 经过每个节点的判断原则就会落到叶子节点, 可以用落在叶子节点中所有的样本数据的平均值当作回归问题的预测值)</li>
<li>有非常好的可解释性</li>
</ul>
<p><br></p>
<p><img src="/Blog/intro/decision_tree_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>既然了解了什么是决策树, 问题就变成怎么构建决策树</p>
<p>每一个节点都首先找到一个维度, 其次找到这个维度中的一个阈值, 以这个维度的这个阈值为依据判断继续进行划分.</p>
<p>关键的问题其实就是: </p>
<ol>
<li><p>每个节点到底在哪个维度做划分?</p>
</li>
<li><p>某个维度的哪个值上做划分?</p>
</li>
</ol>
<p><br></p>
<p><br></p>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息熵是<strong>信息论</strong>的一个基础概念</p>
<p>熵在信息论中代表: <strong>随机变量不确定度的度量</strong></p>
<ul>
<li><p>熵越大, 数据的不确定性越高, 数据越随机</p>
</li>
<li><p>熵越小, 数据的不确定性越低</p>
</li>
</ul>
<p>其实熵这个概念是从<strong>物理-热力学</strong>中引申出的概念</p>
<p>也可以这样理解, 熵越大, 粒子无规则地运动越剧烈, 不确定性越高; 熵越小, 粒子越倾向与静止的状态, 不确定性越低.</p>
<p><br></p>
<p>信息熵的计算公式: 香农提出的</p>
<script type="math/tex; mode=display">
H=-\sum_{i=1}^k\ p_i\ log(p_i)</script><p>一个系统信息熵的总和是这样计算的, p<sub>i</sub>是什么意思?</p>
<p>就是对于一个系统中可能有<strong>k类的信息, 每一类信息所占比例就叫p<sub>i</sub></strong></p>
<p>例如鸢尾花数据集中有3种鸢尾花, 每种鸢尾花所占比例可能为1/3, 那么p<sub>1</sub>, p<sub>2</sub>, p<sub>3</sub>就分别等于1/3</p>
<p>为什么有一个负号? 因为p<sub>i</sub>一定是一个小于1的数, 由于log函数中如果传入的参数小于1, log(x) 的结果就会是负数, 再加上这个负号就变成大于0的数. </p>
<p>所以最终计算出来的熵(H)肯定是大于0的</p>
<p><br></p>
<script type="math/tex; mode=display">
H=-\sum_{i=1}^k\ p_i\ log(p_i)</script><p>但是这个公式还是太抽象, 我们举几个例子:</p>
<p><img src="/Blog/intro/example_entropy.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>显然右边的数据的信息熵比左边的信息熵小.</p>
<p>熵代表不确定性的度量, 熵越小, 不确定度就越低, 确定性越强</p>
<p>如果熵越大, 系统就越不确定, 越随机.</p>
<p><br></p>
<p><img src="/Blog/intro/example_entropy.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这么看就是说: 右边的数据比左边的数据更确定</p>
<p>由于右边的数据很多数据都在第三类里, 占70%之多, 所以右边数据是更确定的. 而左边数据每个类别各占1/3, 所以数据不确定性越强.</p>
<p>按照设个思路在强一点: 如果第一类数据占100%, {1, 0, 0}</p>
<script type="math/tex; mode=display">
H=-1\cdot log(1)=0</script><p>这个信息熵就是这个公式的最低值. 放进信息熵的物理定义中就是: <strong>此时数据的不确定性是最低的, 它是最确定的</strong> (所有数据都在一类中没有任何不确定性)</p>
<p><br></p>
<script type="math/tex; mode=display">
H=-\sum_{i=1}^k\ p_i\ log(p_i)</script><p>以二分类问题为例, 感性地看一下信息熵的公式是什么样子的.</p>
<p>如果数据类别是两类, 一类是x, 另一类占得比例就是1-x, 此时信息熵的公式就是:</p>
<script type="math/tex; mode=display">
H=-xlog(x)-(1-x)log(1-x)</script><p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">p</span>):<br>    <span class="hljs-keyword">return</span> -p * np.log(p) - (<span class="hljs-number">1</span>-p) * np.log(<span class="hljs-number">1</span>-p)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    x = np.linspace(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.99</span>, <span class="hljs-number">200</span>)<br>    y = entropy(x)<br><br>    plt.plot(x, y)<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/example_entropy_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这样一根曲线像是抛物线的样子, 且这个曲线的对称轴是0.5</p>
<p>换句话说, 当x=0.5, 曲线取到了最大值, 相当于对于信息熵来说, 两个类别都是0.5时, 整个信息熵是最大的</p>
<p>此时的数据是最不稳定的, 确定性最低. 此时数据到底是第一类还是第二类各有50%的可能性, 所以是最不确定的.</p>
<p>无论是x值更小还是更大, 整个数据的信息熵都在降低. 无论更小还是更大我们的数据都更偏向某一类, 数据整体确定性更高了, 信息熵就变低了</p>
<p>如果系统中有3类, 绘制的就是立体的曲面</p>
<p><br></p>
<p>如果系统偏向某一类, 就相当于一定程度有了确定性</p>
<p><br></p>
<p><img src="/Blog/intro/decision_tree_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<ol>
<li><p>每个节点到底在哪个维度做划分?</p>
</li>
<li><p>某个维度的哪个值上做划分?</p>
</li>
</ol>
<p>当有信息熵的概念, 这两个问题就好解决了.</p>
<p>我们希望在<strong>每一个节点</strong>上都希望在<strong>某一个维度</strong>上基于<strong>某一个阈值</strong>划分</p>
<p>我们在划分以后, 要做到事情, 就是要让划分为两部分之后相应的系统的整体信息熵降低, 换句话说让整个系统变得更加确定</p>
<p>极端情况下, ABC三个叶子节点每个叶子节点中的数据只属于A类, B类, C类的话, 此时整个系统的信息熵就达到了0. </p>
<p>我们要找在每一个节点上某一个维度的取值, 根据这个维度的这个取值进行划分使得总体信息熵比其他划分方式的信息熵都小. 我们称这样的划分就是当前<strong>最好的划分</strong></p>
<p><br></p>
<p>怎么找到这样的划分? 对所有划分可能性进行一次搜索即可</p>
<h2 id="使用信息熵寻找最优划分"><a href="#使用信息熵寻找最优划分" class="headerlink" title="使用信息熵寻找最优划分"></a>使用信息熵寻找最优划分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">y</span>):<br>    counter = Counter(y)<br>    res = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> counter.values():<br>        p = num / <span class="hljs-built_in">len</span>(y)<br>        res += -p * log(p)<br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-comment"># 基于维度d的value值进行划分</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">split</span>(<span class="hljs-params">X, y, d, value</span>):<br>    <span class="hljs-comment"># 维度d的value &lt;= value</span><br>    index_a = (X[:, d] &lt;= value)<br>    index_b = (X[:, d] &gt; value)<br>    <span class="hljs-keyword">return</span> X[index_a], X[index_b], y[index_a], y[index_b]<br><br><br><span class="hljs-comment"># 尝试划分</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">try_split</span>(<span class="hljs-params">X, y</span>):<br>    best_entropy = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>    best_d, best_v = -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>     <span class="hljs-comment"># 初始随便取值</span><br>    <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>        <span class="hljs-comment"># 阈值可选的值就是每两个样本点在d这个维度上中间的值</span><br>        sorted_index = np.argsort(X[:, d])<br>        <span class="hljs-comment"># 每次寻找i-1和i在d这个维度的中间的值</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(X)):<br>            <span class="hljs-keyword">if</span> X[sorted_index[i-<span class="hljs-number">1</span>], d] != X[sorted_index[i], d]:<br>                v = (X[sorted_index[i-<span class="hljs-number">1</span>], d] + X[sorted_index[i], d]) / <span class="hljs-number">2</span><br>                X_l, X_r, y_l, y_r = split(X, y, d, v)<br>                <span class="hljs-comment"># 计算两个区域的信息熵的和</span><br>                e = entropy(y_l) + entropy(y_r)<br>                <span class="hljs-keyword">if</span> e &lt; best_entropy:<br>                    best_entropy, best_d, best_v = e, d, v<br>    <span class="hljs-keyword">return</span> best_entropy, best_d, best_v<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    iris = datasets.load_iris()<br>    <span class="hljs-comment"># 保留后两个特征</span><br>    X = iris.data[:, <span class="hljs-number">2</span>:]<br>    y = iris.target<br><br>    best_entropy, best_d, best_v = try_split(X, y)<br>    <span class="hljs-built_in">print</span>(best_entropy)  <span class="hljs-comment"># 熵: 0.6931</span><br>    <span class="hljs-built_in">print</span>(best_d)        <span class="hljs-comment"># 维度: 0</span><br>    <span class="hljs-built_in">print</span>(best_v)        <span class="hljs-comment"># 阈值: 2.45</span><br>    X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)<br>    <span class="hljs-built_in">print</span>(entropy(y1_l))       <span class="hljs-comment"># 信息熵为0, 把蓝色数据全都化为左边</span><br>    <span class="hljs-built_in">print</span>(entropy(y1_r))       <span class="hljs-comment"># 信息熵为0.69</span><br><br>    best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)<br>    <span class="hljs-built_in">print</span>(best_entropy2)  <span class="hljs-comment"># 熵: 0.4132</span><br>    <span class="hljs-built_in">print</span>(best_d2)        <span class="hljs-comment"># 维度: 1</span><br>    <span class="hljs-built_in">print</span>(best_v2)        <span class="hljs-comment"># 阈值: 1.75</span><br>    X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)<br>    <span class="hljs-built_in">print</span>(entropy(y2_l))       <span class="hljs-comment"># 信息熵为0.3084</span><br>    <span class="hljs-built_in">print</span>(entropy(y2_r))       <span class="hljs-comment"># 信息熵为0.1047</span><br></code></pre></td></tr></table></figure>
<p>可以看到</p>
<ul>
<li>第一次决策节点为: 维度=0, 阈值=2.45</li>
<li>第二次决策节点为: 维度=1, 阈值=1.75</li>
</ul>
<p>这和我们之前使用scikit-learn中得到结果的大致是一样的</p>
<p><img src="/Blog/intro/decision_tree_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>模拟划分并没有把决策树建立起来, 就可以用构建二叉树的方式把这棵树构建起来</p>
<blockquote>
<p>kNN优化加速的思路就是用K-D tree把数据组织起来</p>
<p>从底层实现机器学习算法很多时候是需要传统数据结构和算法的知识的, 不应该将二者分离地来看待</p>
</blockquote>
<p>尝试实现一个简单的基于熵的决策树</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> log<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_decision_boundary</span>(<span class="hljs-params">model, axis</span>):<br>    x0, x1 = np.meshgrid(<br>        np.linspace(axis[<span class="hljs-number">0</span>], axis[<span class="hljs-number">1</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">1</span>] - axis[<span class="hljs-number">0</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>        np.linspace(axis[<span class="hljs-number">2</span>], axis[<span class="hljs-number">3</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">3</span>] - axis[<span class="hljs-number">2</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>    )<br>    X_new = np.c_[x0.ravel(), x1.ravel()]<br>    y_predict = model.predict(X_new)<br><br>    zz = y_predict.reshape(x0.shape)<br>    custom_cmap = ListedColormap([<span class="hljs-string">&#x27;#EF9A9A&#x27;</span>, <span class="hljs-string">&#x27;#FFF59D&#x27;</span>, <span class="hljs-string">&#x27;#90CAF9&#x27;</span>])<br><br>    plt.contourf(x0, x1, zz, cmap=custom_cmap)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Node</span>:<br>    <span class="hljs-comment"># res 表示判断完成后的类别, next是子节点用于继续判断</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d, v, res, left_is_small</span>):<br>        self.d = d<br>        self.v = v<br>        self.res = res<br>        <span class="hljs-comment"># Ture, 则判断&lt;=v; False, 则判断&gt;v</span><br>        self.left_is_small = left_is_small<br>        self.<span class="hljs-built_in">next</span> = <span class="hljs-literal">None</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecisionTree</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, max_depth</span>):<br>        self.max_depth = max_depth<br>        self.root = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, X_train, y_train</span>):<br>        X_r = X_train<br>        y_r = y_train<br><br>        self.root = Node(-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-literal">True</span>)<br>        cur_node = self.root<br><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.max_depth):<br>            e, d, v = self.try_split(X_r, y_r)<br>            X_l, X_r, y_l, y_r = self.split(X_r, y_r, d, v)<br>            left_is_small = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">if</span> self.entropy(y_l) &gt; self.entropy(y_r):<br>                X_l, X_r, y_l, y_r = X_r, X_l, y_r, y_l<br>                left_is_small = <span class="hljs-literal">False</span><br>            cur_node.<span class="hljs-built_in">next</span> = Node(d, v, Counter(y_l).most_common(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], left_is_small)<br>            cur_node = cur_node.<span class="hljs-built_in">next</span><br>        cur_node.<span class="hljs-built_in">next</span> = Node(d, v, Counter(y_r).most_common(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], <span class="hljs-keyword">not</span> left_is_small)<br>        self.root = self.root.<span class="hljs-built_in">next</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> np.array([self._predict(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X], dtype=<span class="hljs-string">&#x27;int&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_predict</span>(<span class="hljs-params">self, x</span>):<br>        cur_node = self.root<br>        <span class="hljs-keyword">while</span> cur_node:<br>            <span class="hljs-keyword">if</span> cur_node.left_is_small:<br>                <span class="hljs-keyword">if</span> x[cur_node.d] &lt;= cur_node.v:<br>                    <span class="hljs-keyword">return</span> cur_node.res<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> x[cur_node.d] &gt; cur_node.v:<br>                    <span class="hljs-keyword">return</span> cur_node.res<br>            cur_node = cur_node.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">self, y</span>):<br>        counter = Counter(y)<br>        res = <span class="hljs-number">0.0</span><br>        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> counter.values():<br>            p = num / <span class="hljs-built_in">len</span>(y)<br>            res += -p * log(p)<br>        <span class="hljs-keyword">return</span> res<br><br>    <span class="hljs-comment"># 基于维度d的value值进行划分</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">split</span>(<span class="hljs-params">self, X, y, d, value</span>):<br>        <span class="hljs-comment"># 维度d的value &lt;= value</span><br>        index_a = (X[:, d] &lt;= value)<br>        index_b = (X[:, d] &gt; value)<br>        <span class="hljs-keyword">return</span> X[index_a], X[index_b], y[index_a], y[index_b]<br><br>    <span class="hljs-comment"># 尝试划分</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">try_split</span>(<span class="hljs-params">self, X, y</span>):<br>        best_entropy = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>        best_d, best_v = -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>  <span class="hljs-comment"># 初始随便取值</span><br>        <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-comment"># 阈值可选的值就是每两个样本点在d这个维度上中间的值</span><br>            sorted_index = np.argsort(X[:, d])<br>            <span class="hljs-comment"># 每次寻找i-1和i在d这个维度的中间的值</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(X)):<br>                <span class="hljs-keyword">if</span> X[sorted_index[i - <span class="hljs-number">1</span>], d] != X[sorted_index[i], d]:<br>                    v = (X[sorted_index[i - <span class="hljs-number">1</span>], d] + X[sorted_index[i], d]) / <span class="hljs-number">2</span><br>                    X_l, X_r, y_l, y_r = self.split(X, y, d, v)<br>                    <span class="hljs-comment"># 计算两个区域的信息熵的和</span><br>                    e = self.entropy(y_l) + self.entropy(y_r)<br>                    <span class="hljs-keyword">if</span> e &lt; best_entropy:<br>                        best_entropy, best_d, best_v = e, d, v<br>        <span class="hljs-keyword">return</span> best_entropy, best_d, best_v<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    iris = datasets.load_iris()<br>    <span class="hljs-comment"># 保留后两个特征</span><br>    X = iris.data[:, <span class="hljs-number">2</span>:]<br>    y = iris.target<br><br>    dt_clf = DecisionTree(max_depth=<span class="hljs-number">2</span>)<br>    dt_clf.fit(X, y)<br><br>    plot_decision_boundary(dt_clf, axis=[<span class="hljs-number">0.5</span>, <span class="hljs-number">7.5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])<br>    plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y == <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p>绘图的结果和scikit-learn中的一样</p>
<blockquote>
<p>这个实现依然有不足之处, 待改进的部分:</p>
<ol>
<li>并不每次只判断熵最大的一边, 而是从两侧同时尝试split</li>
<li>predict可以优化速度, 批量判断</li>
<li>Node节点不够优雅</li>
</ol>
<p>但最起码实现了一部分.</p>
</blockquote>
<p><br></p>
<p><br></p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>之前注意介绍使用信息熵来对节点进行划分,其实还可以使用另外一个指标来划分: 基尼系数</p>
<script type="math/tex; mode=display">
G=1-\sum_{i=1}^k p_i^2</script><p>这个式子和信息熵是有同样的性质的</p>
<p><br></p>
<p>同样举几个例子来看</p>
<p><img src="/Blog/intro/gini_coef.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>用样, 基尼系数越高意味着数据整体随机性越强; 基尼系数越低意味着数据整体不确定性越弱</p>
<p><br></p>
<p>同样可以用一根曲线来看一下是什么样子, 依然是使用二分类的方式, 此时基尼系数就是:</p>
<script type="math/tex; mode=display">
G=1-x^2-(1-x)^2</script><script type="math/tex; mode=display">
= -2x^2+2x \ \ \ \ \ \ \ \</script><p>此时基尼系数就是抛物线, 开口向下, 最大值就是在抛物线对称轴的地方 x=0.5</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dt_clf = DecisionTreeClassifier(max_depth=<span class="hljs-number">2</span>, criterion=<span class="hljs-string">&quot;gini&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>所做的只是把criterion改为gini</p>
<p>而得到的图和使用信息熵是差不多的. 通常二者结果区分不大</p>
<p><br></p>
<h2 id="模拟使用基尼系数划分"><a href="#模拟使用基尼系数划分" class="headerlink" title="模拟使用基尼系数划分"></a>模拟使用基尼系数划分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gini</span>(<span class="hljs-params">y</span>):<br>    counter = Counter(y)<br>    res = <span class="hljs-number">1.0</span><br>    <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> counter.values():<br>        p = num / <span class="hljs-built_in">len</span>(y)<br>        res -= p**<span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> res<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">split</span>(<span class="hljs-params">X, y, d, value</span>):<br>    <span class="hljs-comment"># 维度d的value &lt;= value</span><br>    index_a = (X[:, d] &lt;= value)<br>    index_b = (X[:, d] &gt; value)<br>    <span class="hljs-keyword">return</span> X[index_a], X[index_b], y[index_a], y[index_b]<br><br><br><span class="hljs-comment"># 尝试划分</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">try_split</span>(<span class="hljs-params">X, y</span>):<br>    best_g = <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br>    best_d, best_v = -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>     <span class="hljs-comment"># 初始随便取值</span><br>    <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>        sorted_index = np.argsort(X[:, d])<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(X)):<br>            <span class="hljs-keyword">if</span> X[sorted_index[i-<span class="hljs-number">1</span>], d] != X[sorted_index[i], d]:<br>                v = (X[sorted_index[i-<span class="hljs-number">1</span>], d] + X[sorted_index[i], d]) / <span class="hljs-number">2</span><br>                X_l, X_r, y_l, y_r = split(X, y, d, v)<br>                g = gini(y_l) + gini(y_r)<br>                <span class="hljs-keyword">if</span> g &lt; best_g:<br>                    best_g, best_d, best_v = g, d, v<br>    <span class="hljs-keyword">return</span> best_g, best_d, best_v<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    iris = datasets.load_iris()<br>    <span class="hljs-comment"># 保留后两个特征</span><br>    X = iris.data[:, <span class="hljs-number">2</span>:]<br>    y = iris.target<br><br>    dt_clf = DecisionTreeClassifier(max_depth=<span class="hljs-number">2</span>, criterion=<span class="hljs-string">&quot;gini&quot;</span>)<br>    dt_clf.fit(X, y)<br><br>    plot_decision_boundary(dt_clf, axis=[<span class="hljs-number">0.5</span>, <span class="hljs-number">7.5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])<br>    plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y == <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br><br>    best_g, best_d, best_v = try_split(X, y)<br>    <span class="hljs-built_in">print</span>(best_g)  <span class="hljs-comment"># 基尼系数: 0.5</span><br>    <span class="hljs-built_in">print</span>(best_d)  <span class="hljs-comment"># 维度: 0</span><br>    <span class="hljs-built_in">print</span>(best_v)  <span class="hljs-comment"># 阈值: 2.45</span><br>    X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)<br>    <span class="hljs-built_in">print</span>(gini(y1_l))  <span class="hljs-comment"># 基尼系数=0, 把蓝色数据全都化为左边</span><br>    <span class="hljs-built_in">print</span>(gini(y1_r))  <span class="hljs-comment"># 基尼系数为0.5</span><br><br>    best_g2, best_d2, best_v2 = try_split(X1_r, y1_r)<br>    <span class="hljs-built_in">print</span>(best_g2)  <span class="hljs-comment"># 基尼系数: 0.2105</span><br>    <span class="hljs-built_in">print</span>(best_d2)  <span class="hljs-comment"># 维度: 1</span><br>    <span class="hljs-built_in">print</span>(best_v2)  <span class="hljs-comment"># 阈值: 1.75</span><br>    X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)<br>    <span class="hljs-built_in">print</span>(gini(y2_l))  <span class="hljs-comment"># 基尼系数为0.1680</span><br>    <span class="hljs-built_in">print</span>(gini(y2_r))  <span class="hljs-comment"># 基尼系数为0.0425</span><br></code></pre></td></tr></table></figure>
<p>划分结果和使用信息熵得到的结果一模一样. </p>
<p>而实现也和信息熵一模一样, 只不过把计算信息熵的函数改为计算基尼系数的函数.</p>
<p><br></p>
<h2 id="信息熵-VS-基尼系数"><a href="#信息熵-VS-基尼系数" class="headerlink" title="信息熵 VS 基尼系数"></a>信息熵 VS 基尼系数</h2><p>信息熵的计算比基尼系数稍慢, 因为信息熵要计算log</p>
<p>因此scikit-learn中默认为基尼系数</p>
<p>大多数时候, 二者没有特别的效果优劣</p>
<p>的确我们可以把”选择使用信息熵”或”选择使用基尼系数”作为超参数, 但很少有情况是其中一个效果非常好, 而另一个效果较差</p>
<p>与其纠结这个超参数, 不如调整其他超参数</p>
<p><br></p>
<h2 id="CART-与-决策树中的超参数"><a href="#CART-与-决策树中的超参数" class="headerlink" title="CART 与 决策树中的超参数"></a>CART 与 决策树中的超参数</h2><p>之前实现的决策树又有一个名字: <strong>CART</strong> (Classification And Regression Tree)</p>
<p>这种决策树既可以解决分类问题, 又可以解决回归问题</p>
<p>思想就是: 根据某一个维度d和某一个阈值v进行二分</p>
<p>scikit-learn实现的决策树都是CART</p>
<blockquote>
<p>ID3, C4.5, C5.0 … 是用其他的方式创建决策树的方法</p>
</blockquote>
<p><br></p>
<p>决策树复杂度:</p>
<p><img src="/Blog/intro/complex_dt.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>决策树还有一个问题, 就是容易产生过拟合, 这一点和kNN是一样的. </p>
<p>实际上所有的<strong>非参数学习算法</strong>都非常容易产生过拟合</p>
<p>基于这些原因, 实际创建决策树时, 必须进行剪枝: 降低复杂度, 解决过拟合</p>
<p>具体怎么剪枝? 其实之前创建决策树一直进行了剪枝</p>
<p>因为在创建决策树时max_depth这个参数一直传的是2, 就是限制了整棵树的高度最多是2, 这实际上就是一个剪枝手段</p>
<p>那么处理max_depth还有什么剪枝手段?</p>
<p><br></p>
<p>如果不指定max_depth</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 生成的非线性数据, 为了能更好看到过拟合的样子</span><br>    X, y = datasets.make_moons(noise=<span class="hljs-number">0.25</span>, random_state=<span class="hljs-number">666</span>)<br><br>    dt_clf = DecisionTreeClassifier()<br>    dt_clf.fit(X, y)<br><br>    plot_decision_boundary(dt_clf, axis=[-<span class="hljs-number">1.5</span>, <span class="hljs-number">2.5</span>, -<span class="hljs-number">1.0</span>, <span class="hljs-number">1.5</span>])<br>    plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/ofer_fitting_dt.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>如果depth为2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dt_clf = DecisionTreeClassifier(max_depth=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/complex_dt_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>这个参数指定至少要有多少节点数据才对它进行拆分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dt_clf = DecisionTreeClassifier(min_samples_split=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/complex_dt_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这个参数调的越高就越不容易过拟合</p>
<p><br></p>
<p>对于一个叶子节点来说, 至少要有几个样本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dt_clf = DecisionTreeClassifier(min_samples_leaf=<span class="hljs-number">6</span>)<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/complex_dt_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这个参数如果太低就会过拟合, 极端情况下等于1时, 某些参数就会由这一个样本决定, 因此模型会对特殊的样本点非常敏感</p>
<p><br></p>
<p>指定最多有几个叶子节点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dt_clf = DecisionTreeClassifier(max_leaf_nodes=<span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/complex_dt_6.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>实际使用这些参数时要避免欠拟合, 齐次参数间可以互相组合, 所以可以使用网格搜索的方式</p>
<p>实际还有很多其他参数</p>
<p><img src="/Blog/intro/complex_dt_7.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>然而实际上, 使用决策树解决分类问题, 回归问题. 无论怎么条件这些参数, 结果都不会特别的好</p>
<p>决策树是有局限性的, 但是它也非常有用, 随即森林其中就使用了决策树的思路, 因此这些参数也可以用于随机森林的调参</p>
<p><br></p>
<p><br></p>
<h2 id="决策树解决回归问题"><a href="#决策树解决回归问题" class="headerlink" title="决策树解决回归问题"></a>决策树解决回归问题</h2><p><img src="/Blog/intro/decision_tree_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>当使用CART将决策树建立出来之后, 每个节点都包含若干个数据</p>
<p>如果这些数据的输出值是类别的话, 就可以在叶子节点中让数据进行投票, 哪个类别的节点多, 就将样本归为哪个类别.</p>
<p>如果数据的输出是具体的数的话, 来了一个新的样本点, 经过决策树来到某一个叶子节点, 就可以叶子节点的平均值来作为结果</p>
<p><br></p>
<p>scikit-learn中对决策树解决回归问题的封装</p>
<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    boston = datasets.load_boston()<br>    X = boston.data<br>    y = boston.target<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="hljs-number">666</span>)<br><br>    <span class="hljs-comment"># 参数和DecisionTreeClassifier是一样的</span><br>    <span class="hljs-comment"># 区别只是最后在叶子节点得到一个分类结果还是回归结果</span><br>    dt_reg = DecisionTreeRegressor()<br>    dt_reg.fit(X_train, y_train)<br><br>    <span class="hljs-comment"># 显然过拟合了, 在训练数据集上表现好, 但在测试数据集上表现很差</span><br>    <span class="hljs-built_in">print</span>(dt_reg.score(X_test, y_test))     <span class="hljs-comment"># 0.589</span><br>    <span class="hljs-built_in">print</span>(dt_reg.score(X_train, y_train))   <span class="hljs-comment"># 1,0</span><br></code></pre></td></tr></table></figure>
<p>决策树的模型非常适合绘制复杂度曲线, 因为有很多参数可以控制决策树模型的复杂度</p>
<p><img src="/Blog/intro/predict_poly_reg_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/liuyubobobo/Play-with-Machine-Learning-Algorithms/blob/master/12-Decision-Tree/Optional-01-Learning-Curve-for-Decision-Tree/Optional-01-Learning-Curve-for-Decision-Tree.ipynb">决策树的学习曲线代码</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/liuyubobobo/Play-with-Machine-Learning-Algorithms/blob/master/12-Decision-Tree/Optional-02-Model-Complexity-Curve-for-Decision-Tree/Optional-02-Model-Complexity-Curve-for-Decision-Tree.ipynb">模型复杂度曲线</a></p>
</blockquote>
<p><br></p>
<h2 id="决策树的局限性"><a href="#决策树的局限性" class="headerlink" title="决策树的局限性"></a>决策树的局限性</h2><p><img src="/Blog/intro/decision_tree_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>其实看鸢尾花的分类结果时就能看出来, 决策树的边界都是横平竖直的. </p>
<p>这样的边界显然是有局限性的, 例如这样的数据点的划分</p>
<p><img src="/Blog/intro/complex_dt_8.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这样的决策边界很可能无法反应数据的真实情况</p>
<p><img src="/Blog/intro/complex_dt_9.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>肯可能真正的决策边界是一条斜线, 而决策树永远无法产生这样的决策边界.</p>
<p><br></p>
<p>还有更严重的事情, 假设数据集是这个样子</p>
<p><img src="/Blog/intro/complex_dt_10.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>此时非常好区分, 就在中间划一根线条, 左边就是蓝色的类, 右边是红色的类</p>
<p><img src="/Blog/intro/complex_dt_11.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>可是, 同样是这个分布, 它可能只是稍微有些倾斜, 划分结果可能就是这个样子</p>
<p><img src="/Blog/intro/complex_dt_12.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这样的决策边界很有可能是不对的, 尤其是数据点在两侧的时候</p>
<p><br></p>
<p>另外还有一个<strong>大多数非参数学习共有的局限性</strong>: 对个别数据敏感</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    iris = datasets.load_iris()<br>    <span class="hljs-comment"># 保留后两个特征</span><br>    X = iris.data[:, <span class="hljs-number">2</span>:]<br>    y = iris.target<br><br>    tree_clf = DecisionTreeClassifier(max_depth=<span class="hljs-number">2</span>, criterion=<span class="hljs-string">&quot;entropy&quot;</span>)<br>    tree_clf.fit(X, y)<br>    plot_decision_boundary(tree_clf, [<span class="hljs-number">0.5</span>, <span class="hljs-number">7.5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])<br>    plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br><br>    <span class="hljs-comment"># 删除第138行的元素(数据点)</span><br>    X_new = np.delete(X, <span class="hljs-number">138</span>, axis=<span class="hljs-number">0</span>)<br>    y_new = np.delete(y, <span class="hljs-number">138</span>)<br>    tree_clf2 = DecisionTreeClassifier(max_depth=<span class="hljs-number">2</span>, criterion=<span class="hljs-string">&quot;entropy&quot;</span>)<br>    tree_clf2.fit(X_new, y_new)<br>    plot_decision_boundary(tree_clf2, [<span class="hljs-number">0.5</span>, <span class="hljs-number">7.5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])<br>    plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">2</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/decision_tree_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/complex_dt_13.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>只删除了一个样本点, 得到的决策边界的形态就和之前完全不同了</p>
<p>这是所有非参数学习的缺点, 高度依赖调参, 才能得到一个较好的模型</p>
<p><br></p>
<p>决策树的主要用途并不完全是单独进行使用(虽然经济学或者相关领域对于一些现实问题很有可能就使用一颗决策树进行建模)</p>
<p>不过机器学习领域的更重要的应用是使用<strong>集成学习</strong>的方式创建一种叫做”<strong>随机森林</strong>“的算法. 随机森林可以得到非常好的学习结果</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/Blog/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/Blog/tags/python3/">python3</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/Blog/2020/05/17/machine-learning-11/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">python3入门机器学习(11)-集成学习和随机森林</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/Blog/2020/05/09/machine-learning-9/">
                        <span class="hidden-mobile">python3入门机器学习(9)-SVM支撑向量机</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/Blog/js/events.js" ></script>
<script  src="/Blog/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/Blog/js/local-search.js" ></script>



  
    <script  src="/Blog/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/Blog/js/boot.js" ></script>


</body>
</html>
