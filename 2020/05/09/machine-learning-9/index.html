

<!DOCTYPE html>
<html lang="" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/Blog/img/favicon.png">
  <link rel="icon" href="/Blog/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Daolin">
  <meta name="keywords" content="">
  
    <meta name="description" content="支撑向量机 SVM">
<meta property="og:type" content="article">
<meta property="og:title" content="python3入门机器学习(9)-SVM支撑向量机">
<meta property="og:url" content="https://daolinzhou.github.io/Blog/2020/05/09/machine-learning-9/">
<meta property="og:site_name" content="Daolin&#39;s Repository">
<meta property="og:description" content="支撑向量机 SVM">
<meta property="og:locale">
<meta property="og:image" content="https://daolinzhou.github.io/Blog/intro/svm_5.jpg">
<meta property="article:published_time" content="2020-05-09T22:42:50.000Z">
<meta property="article:modified_time" content="2020-06-02T22:07:19.949Z">
<meta property="article:author" content="Daolin">
<meta property="article:tag" content="python3">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daolinzhou.github.io/Blog/intro/svm_5.jpg">
  
  
  <title>python3入门机器学习(9)-SVM支撑向量机 - Daolin&#39;s Repository</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/Blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"daolinzhou.github.io","root":"/Blog/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/Blog/local-search.xml"};
  </script>
  <script  src="/Blog/js/utils.js" ></script>
  <script  src="/Blog/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/Blog/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Blog/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/Blog/intro/science_gate_time.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="python3入门机器学习(9)-SVM支撑向量机">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-05-09 15:42" pubdate>
        May 9, 2020 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      19k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      160 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">python3入门机器学习(9)-SVM支撑向量机</h1>
            
            <div class="markdown-body">
              <p>支撑向量机 SVM</p>
<span id="more"></span>
<h1 id="支撑向量机-SVM"><a href="#支撑向量机-SVM" class="headerlink" title="支撑向量机 SVM"></a>支撑向量机 SVM</h1><h2 id="什么叫支撑向量机"><a href="#什么叫支撑向量机" class="headerlink" title="什么叫支撑向量机?"></a>什么叫支撑向量机?</h2><p>SVM (Support Vector Machine)</p>
<p>使用支撑向量机的思想既可以解决<strong>分类问题</strong>, 也可以解决<strong>回归问题</strong></p>
<blockquote>
<p>主要介绍如何用SVM的思想解决分类问题, 最后会介绍如何用这个思想解决回归问题</p>
</blockquote>
<p><br></p>
<p>逻辑回归的算法就是在特征空间中找到一个决策边界</p>
<p><img src="/Blog/intro/svm.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>如果在决策边界的一侧, 算法就认为属于某一类, 在另一侧就是另外一类</p>
<p>不过对于一些数据存在这样的问题: <strong>决策边界不唯一</strong></p>
<p><img src="/Blog/intro/svm_1.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于这种决策边界不唯一的问题有一个术语: <strong>不适定问题</strong></p>
<p><br></p>
<p>逻辑回归算法是如何解决不适定问题?</p>
<p>逻辑回归的思路: 定义一个概率函数, Sigmoid函数, 根据概率函数进行建模, 形成一个<strong>损失函数</strong>. 通过最小化损失函数从而求得一个决策边界</p>
<p>这个损失函数完全有训练数据集所决定的.</p>
<p><br></p>
<p>支撑向量机解决的思路稍微有一些不同.</p>
<p>假设得到的决策边界是这样一根线</p>
<p><img src="/Blog/intro/svm.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>显然在训练数据集上, 非常好的将数据分成了两部分. 但是现在对于机器学习算法有一个非常重要的问题: <strong>算法的泛化能力</strong></p>
<p>换句话说, 对未知的数据进行判断是机器学习算法真正的目的</p>
<p><img src="/Blog/intro/svm_2.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>左上角有一个样本, 根据这个决策边界, 这个点会被归为蓝色样本. 但是直观地看就会觉得将这个数据点分成红色类别是更加合理的</p>
<p>换句话说: 求出的决策边界的泛化效果不够好</p>
<p>这是为什么? 这是因为<strong>决策边界离红色的点太近了</strong></p>
<p>这就导致其他的点可能依然距离红色的点很近, 却被分在了决策边界的另外一侧</p>
<p><br></p>
<p>那么什么样的泛化边界效果可能比较好?</p>
<p><img src="/Blog/intro/svm_3.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这个直线的特点: 离这个直线最近的那些点, 尽可能的远</p>
<p><img src="/Blog/intro/svm_4.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>换句话说我期望: <strong>决策边界距离红色的点尽可能远, 离蓝色点也尽可能远, 同时它还能很好地分布红色和蓝色两个类别</strong></p>
<p>相当于是要找到一个决策边界, 这个决策边界不仅要很好的将训练数据集中的样本进行划分, 同时考虑到未来, 期望泛化能力尽可能好</p>
<p class="note note-info"> SVM这种思想, 对未来的泛化能力尽可能的好的考量, 没有寄望在数据的预处理阶段, 或者是找到模型之后在对模型正则化的方式.而是将对泛化能力的考量直接放在算法内部</p>

<p>我们要找到一条决策边界, 这个边界距离样本尽可能远</p>
<p>我们直观地去看的话, 这样的边界泛化能力就是好的</p>
<blockquote>
<p>实际上这个假设背后也是有数学理论的</p>
<p>数学上可以严格地证明出使用SVM的思路找到的决策边界的泛化能力是好的</p>
<p>也正是这个原因, SVM也是统计学中非常重要的一个方法, 它的背后有极强的统计理论的支撑</p>
</blockquote>
<p><br></p>
<p><img src="/Blog/intro/svm_4.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>再看SVM要找的决策边界有什么特点?</p>
<p>这个决策边界要离这两个类别的数据点都尽可能远</p>
<p>这个意思就是这两个类别中, 离决策边界最近的那些点到决策边界的距离尽可能远</p>
<p>红色类别中有两个点, 蓝色类别中有一个点, 这三个点到决策边界的距离是一样的. 同时它们是离决策边界最近的点. (这三个点称为<strong>支撑向量</strong>)</p>
<p>这些特殊的数据点又定义出了两根直线, 这两根直线和决策边界的直线是平行的</p>
<p><img src="/Blog/intro/svm_5.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这两个直线相当于定义了一个区域. 在这两根直线中, 将不再有任何数据点. SVM最终得到的决策边界, 就相当于这个<strong>区域中间的线</strong></p>
<p><img src="/Blog/intro/svm_6.jpg" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>支撑向量定义了一个区域, 而决策边界就是被这个区域所定义的</p>
<p>换句话说, 找到的最优的决策边界是由<strong>支撑向量</strong>定义的</p>
<p>这三根直线之间的距离是一样的(称为d), 而最外面两个直线的距离称为 margin, margin = d + d</p>
<p><strong>SVM要最大化margin</strong></p>
<blockquote>
<p>又把一个机器学习的思路转化成最优值求解</p>
</blockquote>
<p><br></p>
<p>目前为止都是对<strong>线性可分</strong>的问题说的, 就是样本点存在一个直线(高维中: 存在一个超平面) 可以将这些点划分. 这种算法通常又被称为<strong>Hard Margin SVM</strong> (严格的)</p>
<p>但是在真实情况下, 很多数据是<strong>线性不可分</strong>的, 此时SVM可以通过改进得到 <strong>Soft Margin SVM</strong></p>
<p><br></p>
<p>“SVM要最大化margin” 是自然语言的表述, 要真的最大化margin, 首先要做到就是要用数学语言表达出margin</p>
<p>就是说求出margin的数学表达式, 之后就是找到其中未知数的一组取值最大化margin</p>
<p class="note note-danger">对于机器学习算法, 尤其是参数学习算法来说, 就是把解决问题的思想首先转化成一个<b>最优化问题</b>, 下面我们的目的就变成了最优化目标函数</p>

<p><br></p>
<p><br></p>
<h2 id="SVM背后的最优化问题"><a href="#SVM背后的最优化问题" class="headerlink" title="SVM背后的最优化问题"></a>SVM背后的最优化问题</h2><p><img src="/Blog/intro/svm_7.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>margin = 2d</p>
<p>换句话说, SVM 要最大化margin的话, 也就是说要最大化d.</p>
<p>也就是说, 找到d的表达式也可以求解这个问题</p>
<p>接下来就是看如何用数学语言表达d</p>
<p><br></p>
<p>回忆解析几何, 点到直线的距离</p>
<p>(x<sub>0</sub>, y<sub>0</sub>) 到 Ax+By+C=0 的距离为:</p>
<script type="math/tex; mode=display">
\frac {|Ax_0+By_0+C|} {\sqrt{A^2+B^2}}</script><blockquote>
<p>证明这个公式:</p>
<p>假设有一个平面(超平面) ax+by+cz+d=0, 求(x<sub>0</sub>, y<sub>0</sub>, z<sub>0</sub>) 到这个超平面的距离</p>
<p>也就是求这个点到平面的垂线的距离.</p>
<p>而这个平面的 norm 为 n=(a, b, c) <a href="https://daolinzhou.github.io/Blog/2020/03/06/play-with-linear-algebra-4/#General equation">General Equation</a>.</p>
<p>因此也就是寻找这个平面上任意一点到(x<sub>0</sub>, y<sub>0</sub>, z<sub>0</sub>)的向量对norm向量做scaler projection的结果(正交投影的长度)</p>
<p>假设平面上一点为(x<sub>1</sub>, y<sub>1</sub>, z<sub>1</sub>), 则Ax<sub>1</sub>+By<sub>1</sub>+Cz<sub>1</sub>+d=0</p>
<p>这一点到(x<sub>0</sub>, y<sub>0</sub>, z<sub>0</sub>) 的向量就是 b = (x<sub>1</sub>-x<sub>0</sub>, y<sub>1</sub>- y<sub>0</sub>, z<sub>1</sub>-z<sub>0</sub>)</p>
<p><img src="/Blog/intro/proof_dis.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>之后分子加上d再减去d, 因此结果就是</p>
<script type="math/tex; mode=display">
\frac {|ax_0+by_0+cz_0+d|} {\sqrt{a^2+b^2+c^2}}</script></blockquote>
<p>如果拓展到n维</p>
<p><img src="/Blog/intro/svm_8.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>向量x<sub>b</sub> 其实是在x的第0个元素的位置添加一个1, 这是因为θ向量中, 有一个θ<sub>0</sub></p>
<p>将它转化为 w<sup>T</sup>x+b=0 的形式, 相当于将θ<sub>0</sub>x<sub>0</sub>这一项拿出来当作截距. 这个截距就用b来表示</p>
<p>而将样本中真是的特征用一个向量w 表示. w是weight(权值)的简写</p>
<p>w向量相当于给x向量每一个特征附上一个权值</p>
<p>对于 θ 来说, 如果样本x有n个特征的话, θ中有n+1个元素, w中有n个元素. b是截距. b和w这n+1个元素组合起来就是θ</p>
<p><br></p>
<p>为什么用 w<sup>T</sup>x+b=0 这种形式来表示方程?</p>
<script type="math/tex; mode=display">
\frac {|Ax_0+By_0+C|} {\sqrt{A^2+B^2}}</script><p>二维屏幕中这个公式, 分母的位置是和截距C没有关系的. 这一点可以同样扩展到n维空间</p>
<p>n维空间中任意一个点 x<sub>0</sub> 到 w<sup>T</sup>x+b=0 这个直线的距离是:</p>
<script type="math/tex; mode=display">
\frac {|w^Tx_0+b|} {||w||}</script><p>下面就可以把这个知识带到SVM算法的思路中了 </p>
<p><img src="/Blog/intro/svm_9.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于SVM算法来说我们期望的是这两类: <span style="color:red;">红色样本的数据点</span>和<span style="color:blue;">蓝色样本的数据点</span>. 离最终求得的决策边界 (假设决策边界为w<sup>T</sup>x+b=0)最近的点到决策边界的距离为b.</p>
<p>换句话说: <strong>所有点到决策边界的距离都应该是大于等于b的</strong></p>
<p><br></p>
<p>把这样一个想法分别用这两类来讨论的话, 相应的就可以得到这样一个式子</p>
<p><img src="/Blog/intro/svm_10.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于这两类, 一类叫做1, 一类叫做 -1 (之前一直管一类叫做1, 一类叫做0, 然而为了后续的一些计算才叫做1和-1) </p>
<p>不过这两类在数学上起什么值是<strong>不影响</strong>的, 只要是两个不同的值, 能把这两类区分开即可</p>
<p>对于属于1的这一类, 相当于任取一个 y<sup>(i)</sup>=1 的样本, 都应该有<strong>点到直线的距离</strong>大于等于d</p>
<p>对于属于-1的这一类, 点在直线的另一侧. 它的坐标代入直线方程都小于0, 同理点到直线的距离小于等于-d</p>
<p><br></p>
<p>对这个式子在进行一下变形:</p>
<p><img src="/Blog/intro/svm_11.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>d肯定是一个正数, 因为它代表距离, 所以不等号的符号是不变的</p>
<p>对于这个式子来说可以仔细观察一下分母, w是一个n维的向量, 但是w的模是一个数, 于此同时d也是一个数, 换句话说: <strong>分母是一个数</strong></p>
<p>对于分子来说, 可以想象成将<strong>w这个向量中的每一个元素</strong>都除以分母这个数, 再加上b这个截距也除以分母这个数</p>
<p>对于这个新的w向量叫做w<sub>d</sub>, 新的截距b叫做b<sub>d</sub></p>
<p><img src="/Blog/intro/svm_12.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>换句话说, 我们就知道了上下两根直线的直线方程应该是:</p>
<p><img src="/Blog/intro/svm_13.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>不过在这里, 我们最初假设SVM最初的决策平面中间的直线是w<sup>T</sup>x+b=0, 中间直线的参数是w和b, 上下两根直线的参数是w<sub>d</sub>和b<sub>d</sub></p>
<p>他们之间就差 <strong>w的模乘以d</strong> 这个常数这么多倍.</p>
<p>所以对于 w<sup>T</sup>x+b=0 这个方程, 左右两边同时除以 w的模乘以d 这个常数</p>
<p><img src="/Blog/intro/svm_14.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>让中间的直线的方程转化为w<sub>d</sub><sup>T</sup>x+b<sub>d</sub>=0</p>
<p><br></p>
<p><img src="/Blog/intro/svm_15.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这样整理完, 相当于在式子中, 所有的未知数都使用了w<sub>d</sub>和b<sub>d</sub>.</p>
<p>w<sub>d</sub>是一个含有n个元素的向量, b<sub>d</sub>是一个数</p>
<p>既然所有的未知量都是w<sub>d</sub>和b<sub>d</sub>, 为了方便书写就把下角标d给去掉</p>
<p><img src="/Blog/intro/svm_16.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><strong>此时的w和b, 和最开始使用的w和b是不同的</strong>, 它们之间相差一个系数关系</p>
<blockquote>
<p>现在的w, d只是一个符号而已, 来代替之前公式里出现的w<sub>d</sub>和b<sub>d</sub></p>
</blockquote>
<p>对支撑向量机SVM来说就是求解这两个式子代表的条件下, w和b应该是多少</p>
<p>依然, 两个式子非常不方便, 可以使用一个技巧融合成一个式子</p>
<p><img src="/Blog/intro/svm_17.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p><img src="/Blog/intro/svm_14.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>我们的目标是最大化d, 怎么表达d呢?</p>
<p>d代表的是支撑向量到决策边界的距离</p>
<p>换句话说, 最大化d就是最大化这个式子</p>
<script type="math/tex; mode=display">
max\frac{|w^Tx+b|}{||w||}</script><p>此时x应该是某一个支撑向量, 对于这某一个支撑向量, 代入w<sup>T</sup>x+b这个式子中要么等于1, 要么等于-1</p>
<p>也就是说, 对于任意支撑向量, 要么是红色点中的支撑向量, 要么是蓝色点中的支撑向量</p>
<p>不管在那一侧取绝对值之后的结果都是1</p>
<p>所以最大化的目标就变成了:</p>
<script type="math/tex; mode=display">
max \frac {1}{||w||}</script><p>这就相当于最小化w的模</p>
<script type="math/tex; mode=display">
min||w||</script><p>然而在具体求解这个问题的时候通常最小化的不是它, 而是</p>
<script type="math/tex; mode=display">
min \frac 12||w||^2</script><p>原因也在于方便求导</p>
<p><br></p>
<p>因此整个支持向量机的算法就变成了这样一个最优化问题</p>
<p><img src="/Blog/intro/svm_18.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>最优化的目标函数是</p>
<script type="math/tex; mode=display">
min \frac 12||w||^2</script><p>不过有一个限定条件是:</p>
<script type="math/tex; mode=display">
y^{(i)}(w^Tx^{(i)}+b)>=1</script><p>对于这样一个最优化问题, 它和之前讲的线性回归, 逻辑回归的最优化问题是不一样的</p>
<p>之前讲的所有最优化问题是<strong>全局最优化问题</strong>, 换句话说是没有限定条件的</p>
<p>而这个最优化问题是一个 <strong>有条件的最优化问题</strong></p>
<p class="note note-danger">是否有条件在最优化领域求解这个问题方法是大不相同的</p>

<p>如果最优化问题是全局最优化问题, 没有任何条件. 那非常简单只需要求导让它等于0, 相应的极值点就是取最大值或最小值的位置</p>
<p>但是当是<strong>有条件的最优化问题</strong>的时候, 求解方法变得复杂了很多(需要用到<strong>拉普拉斯算子</strong>来进行求解)</p>
<blockquote>
<p>对于有条件的最优化问题具体怎么求解, 超纲了(我不会)</p>
<p>所以略去对于有条件的最优化问题求解的数学推导</p>
<p>不过一定要了解支撑向量机这样一个思想用数学表达出来就是在最优化这样一个有条件的目标函数</p>
<p><img src="/Blog/intro/svm_18.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
</blockquote>
<p>解决这个问题是hard margin SVM问题, 它假设数据是线性可分的</p>
<p>不过大多数时候数据不会那么好, 不会是线性可分的, 此时就必须用Soft Margin SVM</p>
<p><br></p>
<p><br></p>
<h2 id="Soft-Margin-和-SVM-的正则化"><a href="#Soft-Margin-和-SVM-的正则化" class="headerlink" title="Soft Margin 和 SVM 的正则化"></a>Soft Margin 和 SVM 的正则化</h2><p>正则化本身是一个概念, 并不是说所有模型添加的正则化项都是同样的L1范式和L2范式</p>
<p>对于有一些模型来说, 要改变正则化策略. 但是我们对它进行实现的效果是一样的.</p>
<p><br></p>
<h3 id="什么是Soft-Margin-SVM"><a href="#什么是Soft-Margin-SVM" class="headerlink" title="什么是Soft Margin SVM ?"></a>什么是Soft Margin SVM ?</h3><p><img src="/Blog/intro/soft_margin_svm.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>Hard Margin SVM 就是求解这样一个有条件下的最优化问题.</p>
<p>当求出对应的w和b以后就求出了支撑向量基决策边界的方程</p>
<p><br></p>
<p>然而这里有一个问题: 如果有一个蓝色点的位置在这里</p>
<p><img src="/Blog/intro/soft_margin_svm_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>此时对于Hard Margin SVM来说, 它首先要保证能正确分出这两类</p>
<p>这种情况下求出的决策边界很有可能是这样的一个直线</p>
<p><img src="/Blog/intro/soft_margin_svm_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于这样一根直线, 它的确非常正确的将训练数据集的两类数据分开了</p>
<p>但是单独看这个图我们很可能会对它的泛化能力产生怀疑</p>
<p>显然大多数蓝色的点都集中在左下角区域, 只有一个蓝色点靠近右上角. 很显然这个<strong>决策边界非常强地受到这个蓝色点的影响</strong></p>
<p>但是很有可能这样的一个蓝色的点是一个outlier, 或者它根本就是一个错误的点. 或者它是一个极度特殊的点, 并不能代表一般情况</p>
<p><br></p>
<p><img src="/Blog/intro/soft_margin_svm_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>很有可能这样的决策边界会是更好的一个决策边界</p>
<p>这个决策边界虽然将一个蓝色的点进行了错误的分类. 但很有可能当我们将这个结果放到真实的生产环境中进行实际的预测的时候效果更好: 也就是<strong>泛化能力更加高</strong></p>
<p><br></p>
<p>所以必须思考一个机制: 对于这个机制来说, SVM算法要有一定的容错能力, 在一些情况下, 应该考虑到可以把一些点进行错误地分类, 达到的结果还是希望泛化能力尽可能高</p>
<p>更一般的例子: 蓝色数据点在这个位置</p>
<p><img src="/Blog/intro/soft_margin_svm_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>此时数据是线性不可分的. 没有任何一条直线能把这组数据分成两类</p>
<blockquote>
<p>这种情况下Hard Margin SVM 算法就不是泛化能力强不强的问题了, 而是根本无法应用, 无法得到结果</p>
</blockquote>
<p>这种SVM就叫做Soft Margin SVM</p>
<p><br></p>
<p><img src="/Blog/intro/soft_margin_svm.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>Hard Margin SVM的条件是这个式子必须大于等于1</p>
<script type="math/tex; mode=display">
y^{(i)}(w^Tx^{(i)}+b)>=1</script><p>大于等于1的意思就是: 对于找到的 Margin 区域里必须是任何数据点都没有的. 所有的数据点都必须在 w<sup>T</sup>x+b=1 和 w<sup>T</sup>x+b=-1 这两条直线之外. </p>
<p>现在对于这个条件加以宽松, 让所有数据点<strong>不一定</strong>都在 Margin 区域的外面, 因此给它一个宽松量 ζ(zeta)</p>
<script type="math/tex; mode=display">
y^{(i)}(w^Tx^{(i)}+b)>=1 - \zeta_i</script><p>换句话说将有条件最优化中的条件宽松成这个样子</p>
<p><img src="/Blog/intro/soft_margin_svm_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<script type="math/tex; mode=display">
\zeta_i\ge 0</script><p>ζ也是有条件的, 就是大于等于0. 换句话说, 虚线要在上面那根直线的下面, 或者下面那根直线的上面. 才叫做可以让它犯错误</p>
<p>如果ζ是负值的话, 虚线会在直线的外头, 条件就更加严格了</p>
<p><br></p>
<p>ζ不是一个固定的值, 对于每一个样本i, 都有一个相应的 ζ<sub>i</sub> , 也就是说对于每一个数据点都求出它对应的容错空间.</p>
<p>单单有 ζ&gt;=0 这个条件是不够的, 如果取正无穷, 那么所有的数据点都将满足这样的条件, 此时容错的范围就太大了.</p>
<p>ζ要做的事情是希望它有一定的容错空间, 但这个容错空间又不能太大. 如何表征容错空间不能太大这件事情?  将<strong>最小化的方程</strong>加上这样一项</p>
<script type="math/tex; mode=display">
min \frac 12||w||^2+\sum_{i=1}^m\zeta_i</script><p>就是加上所有ζ的和, 这样一来, 最小化的式子就同时顾及了</p>
<ol>
<li>前半部分, Hard Margin SVM最优化内容, 也就是SVM思想所要做的事情</li>
<li>后半部分, 设立一堆ζ, 使得SVM算法可以容忍一定的分类错误, 但容忍的程度要尽量小</li>
</ol>
<p>二者之间要取得一个平衡. 这样写就是完整的Soft Margin SVM最优化问题的数学表达式</p>
<p><br></p>
<p>然而此时这两部分的比重占比是1:1</p>
<p>就像正则化那样, 可以在后半部分添加一个参数C, 来平衡两部分所占的比例</p>
<p><img src="/Blog/intro/soft_margin_svm_6.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>如果 C 在01之间, 整个式子将以优化前半部分为主</p>
<p>如果 C 特别大, 则整个式子将以优化后半部分为主</p>
<blockquote>
<p>这个依然是一个<strong>有条件的最优化问题</strong>, 具体怎么求解是略去的</p>
<p>scikit-learn中封装好了SVM, 调用即可</p>
</blockquote>
<p><br></p>
<p><img src="/Blog/intro/soft_margin_svm_6.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这么写又叫在Soft Margin SVM中添加<strong>L1正则</strong>, 换句话说我们可以把ζ理解为是一个正则化项, 它在避免模型向极端方向发展</p>
<blockquote>
<p>对于线性 回归, 逻辑回归…加入正则化项的本质也依然是让模型针对训练数据集有更高的容错能力.</p>
<p>当有这样的容错能力后, 使得模型对训练数据集中的极端数据点<strong>不那么敏感</strong>, 用这种方式期望它的泛化能力提升</p>
</blockquote>
<p><br></p>
<p>有L1正则相应就有L2正则</p>
<p><img src="/Blog/intro/soft_margin_svm_7.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>后半部分的表达式就变成 ζ 的平方和</p>
<blockquote>
<p>线性回归时L1正则是对 θ 的绝对值 求和, L2正则是对 θ 的平方求和</p>
<p>因为 ζ &gt;= 0, 所以不用取绝对值</p>
</blockquote>
<p>其实从表达式的形式上, 它和线性回归, 逻辑回归对应的正则化项是一致的, 只不过在具体的意思上. ζ 的几何意义和 θ 的几何意义不同.</p>
<p><br></p>
<p>另外一点, C是在正则项的前面的, 然而在逻辑回归, 线性回归中我们倾向与把C放在损失函数前面</p>
<p>但是, <strong>他们的表意是一样的</strong>: C越大, 容错空间越小</p>
<p>SVM的式子中, 如果C取正无穷, 意味着我们逼迫ζ全都为0, 此时Soft Margin SVM就变成了Hard Margin SVM, 也就是说容错空间更小</p>
<p>而线性回归, 逻辑回归中, C在MSE前面, C越大就意味着要尽量多的估计MSE这部分而不去顾及正则化那部分, 也是容错空间更小</p>
<p>而<strong>C越小就意味着有更大的容错空间</strong></p>
<p class="note note-primary">C的位置虽然不同, 但是它们的表意是一致的, 所以在上层可以使用一致的策略来解读C这个参数的含义的</p>

<p><br></p>
<p>至此线性SVM理论部分讲完了. 现在svm是求解w参数和b参数, 得到的决策边界是线性方程, 二维空间中是直线, 高维空间中是超平面</p>
<p>SVM如果拓展解决非线性问题后续会继续介绍.</p>
<p><br></p>
<p><br></p>
<h2 id="使用scikit-learn中的SVM"><a href="#使用scikit-learn中的SVM" class="headerlink" title="使用scikit-learn中的SVM"></a>使用scikit-learn中的SVM</h2><p>使用SVM之前, 和使用kNN中的算法一样, 要对数据进行<strong>标准化处理</strong></p>
<p>这是因为: SVM算法寻找的是使得margin最大的区间的中间的线</p>
<p>而衡量margin的方式是数据点之间的距离, 涉及距离!</p>
<p>如果数据点在不同的维度上的量纲不同, 就会使得对距离的估计产生问题</p>
<p><img src="/Blog/intro/use_svm.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>例如如果横轴的范围是0-1, 纵轴的范围是0-10000, 使用SVM得到的决策边界大概就是这样</p>
<p>而如果纵轴的范围也在0-1之间, 此时的决策边界就是这样的决策边界:</p>
<p><img src="/Blog/intro/use_svm_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<blockquote>
<p>对于SVM来说, 如果数据在不同的维度上对应的尺度不同, 就会非常严重地影响得到的决策边界的样子</p>
<p>为了避免这种情况的出现, 具体使用SVM之前要进行标准化处理</p>
<p><br></p>
<p>假设一种情况, 纵轴使用微米作为单位得到的决策边界和使用米作为单位得到的决策边界有很大的不同, 我们不希望因为使用的衡量尺度影响最终的结果</p>
</blockquote>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_decision_boundary</span>(<span class="hljs-params">model, axis</span>):<br>    x0, x1 = np.meshgrid(<br>        np.linspace(axis[<span class="hljs-number">0</span>], axis[<span class="hljs-number">1</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">1</span>] - axis[<span class="hljs-number">0</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>        np.linspace(axis[<span class="hljs-number">2</span>], axis[<span class="hljs-number">3</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">3</span>] - axis[<span class="hljs-number">2</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>    )<br>    X_new = np.c_[x0.ravel(), x1.ravel()]<br>    y_predict = model.predict(X_new)<br><br>    zz = y_predict.reshape(x0.shape)<br>    custom_cmap = ListedColormap([<span class="hljs-string">&#x27;#EF9A9A&#x27;</span>, <span class="hljs-string">&#x27;#FFF59D&#x27;</span>, <span class="hljs-string">&#x27;#90CAF9&#x27;</span>])<br><br>    plt.contourf(x0, x1, zz, cmap=custom_cmap)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    iris = datasets.load_iris()<br><br>    X = iris.data<br>    y = iris.target<br><br>    <span class="hljs-comment"># 解决二分类问题, 所以y&lt;2, 为了可视化所以只取两个特征</span><br>    X = X[y &lt; <span class="hljs-number">2</span>, :<span class="hljs-number">2</span>]<br>    y = y[y &lt; <span class="hljs-number">2</span>]<br><br>    standardScaler = StandardScaler()<br>    standardScaler.fit(X)<br>    X_standard = standardScaler.transform(X)<br><br>    <span class="hljs-comment"># SVC: Support Vector Classifier</span><br>    <span class="hljs-comment"># 使用支撑向量机的思想进行分类操作</span><br>    svc = LinearSVC(C=<span class="hljs-number">1e9</span>)<br>    svc.fit(X_standard, y)<br>    <br>    plot_decision_boundary(svc, axis=[-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, -<span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br>    plt.scatter(X_standard[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X_standard[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>    plt.scatter(X_standard[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_standard[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p>C=1e9时, 基本相当于Hard Margin SVM了</p>
<p><img src="/Blog/intro/use_svm_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>C=0.01时</p>
<p><img src="/Blog/intro/use_svm_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>有一个蓝色的点被错误地分类了, 这就是将C这个值放小之后的结果 </p>
<blockquote>
<p>C越大, 容错越小; C越小, 容错越大</p>
</blockquote>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 两个特征, 每个特征对应一个系数 [[ 0.43789693 -0.41091864]]</span><br>   <span class="hljs-built_in">print</span>(svc.coef_)<br>   <span class="hljs-comment"># 一根直线, 所以只有一个截距 [0.00592652]</span><br>   <span class="hljs-built_in">print</span>(svc.intercept_)<br></code></pre></td></tr></table></figure>
<p>返回的二维的数组, 这是因为sklearn封装的这个SVM直接可以处理多分类问题</p>
<p>如果有多个类别, 算法就可以有多条直线来分割特征平面, 每个直线都有它对应的系数</p>
<p><br></p>
<p>为了更加直观的看到对于这两个C得到的SVM算法, margin上下两根直线是怎样的:</p>
<p>改进绘制函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_svc_decision_boundary</span>(<span class="hljs-params">model, axis</span>):<br>    x0, x1 = np.meshgrid(<br>        np.linspace(axis[<span class="hljs-number">0</span>], axis[<span class="hljs-number">1</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">1</span>] - axis[<span class="hljs-number">0</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>        np.linspace(axis[<span class="hljs-number">2</span>], axis[<span class="hljs-number">3</span>], <span class="hljs-built_in">int</span>((axis[<span class="hljs-number">3</span>] - axis[<span class="hljs-number">2</span>]) * <span class="hljs-number">100</span>)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>    )<br>    X_new = np.c_[x0.ravel(), x1.ravel()]<br>    y_predict = model.predict(X_new)<br><br>    zz = y_predict.reshape(x0.shape)<br>    custom_cmap = ListedColormap([<span class="hljs-string">&#x27;#EF9A9A&#x27;</span>, <span class="hljs-string">&#x27;#FFF59D&#x27;</span>, <span class="hljs-string">&#x27;#90CAF9&#x27;</span>])<br><br>    plt.contourf(x0, x1, zz, cmap=custom_cmap)<br><br>    <span class="hljs-comment"># 绘制边界</span><br>    w = model.coef_[<span class="hljs-number">0</span>]<br>    b = model.intercept_[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 边界公式 w0 * x0 + w1 * x1 + b=0</span><br>    <span class="hljs-comment"># =&gt; x1 = - w0/w1 * x0 - b/w1</span><br>    plot_x = np.linspace(axis[<span class="hljs-number">0</span>], axis[<span class="hljs-number">1</span>], <span class="hljs-number">200</span>)<br>    up_y = -w[<span class="hljs-number">0</span>]/w[<span class="hljs-number">1</span>] * plot_x + (<span class="hljs-number">1</span>-b)/w[<span class="hljs-number">1</span>]<br>    down_y = -w[<span class="hljs-number">0</span>]/w[<span class="hljs-number">1</span>] * plot_x + (-<span class="hljs-number">1</span>-b)/w[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-comment"># 防止超过设定的axis的边界, 所以进行一次过滤</span><br>    up_index = ((up_y &gt;= axis[<span class="hljs-number">2</span>]) &amp; (up_y &lt;= axis[<span class="hljs-number">3</span>]))<br>    down_index = ((down_y &gt;= axis[<span class="hljs-number">2</span>]) &amp; (down_y &lt;= axis[<span class="hljs-number">3</span>]))<br><br>    plt.plot(plot_x[up_index], up_y[up_index], color=<span class="hljs-string">&quot;black&quot;</span>)<br>    plt.plot(plot_x[down_index], down_y[down_index], color=<span class="hljs-string">&quot;black&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>C=1e9时, 边界为</p>
<p><img src="/Blog/intro/use_svm_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>可以很清晰地看到上面有三个点落在线上, 下面有两个点落在线上</p>
<p>此时SVC模型相当于Hard Margin SVM算法, margin中间是没有任何点的. 这个模型就既保证了把训练数据集分成两类, 又保证了SVM的思想: 这两类中离决策边界最近的点离决策边界最远</p>
<p><br></p>
<p>C = 0.01时, 边界为</p>
<p><img src="/Blog/intro/use_svm_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>此时margin中存在很多点, 这就是将C设置小的结果, 相当于给Hard Margin SVM算法添加了相当多的容错空间</p>
<p><br></p>
<p><br></p>
<h2 id="SVM中使用多项式特征和核函数"><a href="#SVM中使用多项式特征和核函数" class="headerlink" title="SVM中使用多项式特征和核函数"></a>SVM中使用多项式特征和核函数</h2><p>之前使用SVM处理分类问题都是使用线性的方式, 现在就尝试用SVM处理非线性的问题</p>
<p>处理非线性数据, 最典型的思路就是使用多项式的方式, <strong>扩充原本的数据, 制造新的多项式的特征</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># datasets 生成数据集</span><br>    X, y = datasets.make_moons()<br>    <span class="hljs-built_in">print</span>(X.shape)  <span class="hljs-comment"># (100, 2) 100个样本两个特征, 是make_moons的默认参数</span><br>    plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/svm_poly.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>两个半月形, 这也是make moons名字的由来</p>
<p>这些数据太规则了, 添加一些噪音</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># datasets 生成数据集</span><br>    <span class="hljs-comment"># 添加一点噪音, 可以理解为增大0.15标准差</span><br>    X, y = datasets.make_moons(noise=<span class="hljs-number">0.15</span>, random_state=<span class="hljs-number">666</span>)<br>    <span class="hljs-built_in">print</span>(X.shape)  <span class="hljs-comment"># (100, 2) 100个样本两个特征, 是make_moons的默认参数</span><br>    plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/svm_poly_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>由于添加了噪音, 数据点变得更加凌乱</p>
<p><br></p>
<h3 id="使用多项式特征的SVM"><a href="#使用多项式特征的SVM" class="headerlink" title="使用多项式特征的SVM"></a>使用多项式特征的SVM</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">PolynomialSVC</span>(<span class="hljs-params">degree, C=<span class="hljs-number">1.0</span></span>):<br>    <span class="hljs-keyword">return</span> Pipeline([<br>        (<span class="hljs-string">&#x27;poly&#x27;</span>, PolynomialFeatures(degree)),<br>        (<span class="hljs-string">&#x27;std_scaler&#x27;</span>, StandardScaler()),<br>        (<span class="hljs-string">&#x27;linearSVC&#x27;</span>, LinearSVC(C=C))<br>    ])<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># datasets 生成数据集</span><br>    <span class="hljs-comment"># 添加一点噪音, 可以理解为增大0.15标准差</span><br>    X, y = datasets.make_moons(noise=<span class="hljs-number">0.15</span>, random_state=<span class="hljs-number">666</span>)<br><br>    poly_svc = PolynomialSVC(degree=<span class="hljs-number">3</span>)<br>    poly_svc.fit(X, y)<br><br>    plot_decision_boundary(poly_svc, [-<span class="hljs-number">1.5</span>, <span class="hljs-number">2.5</span>, -<span class="hljs-number">1.0</span>, <span class="hljs-number">1.5</span>])<br>    plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/svm_poly_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>依然是使用多项式特征的方式, 让决策边界不再是一条直线, 而是曲线</p>
<p><br></p>
<p>然而对于SVM算法来说, 在scikit-learn的封装中, 其实可以<strong>不使用</strong>PolynomialFeatures的方式: 先把数据转化为高维的有多项式项的特征的数据, 再扔进 linear SVM</p>
<p>SVM算法有一个特殊的方式, 可以直接使用多项式特征</p>
<p>这个特殊的方式就称为<strong>多项式核</strong></p>
<p><br></p>
<h3 id="多项式核"><a href="#多项式核" class="headerlink" title="多项式核"></a>多项式核</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">PolynomialKernelSVC</span>(<span class="hljs-params">degree, C=<span class="hljs-number">1.0</span></span>):<br>    <span class="hljs-keyword">return</span> Pipeline([<br>        (<span class="hljs-string">&quot;std_scaler&quot;</span>, StandardScaler()),<br>        (<span class="hljs-string">&quot;kernelSVC&quot;</span>, SVC(kernel=<span class="hljs-string">&quot;poly&quot;</span>, degree=degree, C=C))<br>    ])<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># datasets 生成数据集</span><br>    <span class="hljs-comment"># 添加一点噪音, 可以理解为增大0.15标准差</span><br>    X, y = datasets.make_moons(noise=<span class="hljs-number">0.15</span>, random_state=<span class="hljs-number">666</span>)<br><br>    poly_kernel_svc = PolynomialKernelSVC(degree=<span class="hljs-number">3</span>)<br>    poly_kernel_svc.fit(X, y)<br>    plot_decision_boundary(poly_kernel_svc, [-<span class="hljs-number">1.5</span>, <span class="hljs-number">2.5</span>, -<span class="hljs-number">1.0</span>, <span class="hljs-number">1.5</span>])<br>    plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p>同样需要对数据进行标准化</p>
<p>不过这个训练过程<strong>不完全是</strong>: 现将数据多项式化, 再使用Linear SVM进行训练</p>
<p>具体是怎样的后续会介绍, 目前只要知道这样一个调用方式可以指定kernel参数, 当kernel这个参数等于poly时, 同样可以达到多项式特征这样一个效果</p>
<p><br></p>
<p>此时的决策边界是这样的</p>
<p><img src="/Blog/intro/svm_poly_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>此时的决策边界虽然和上面的决策边界不一样, 不过依然可以看出来是一个非线性的决策边界</p>
<p><br></p>
<p>对于SVM算法中, 有一个很重要的概念: <strong>Kernel</strong></p>
<p>Kernel中传入poly参数其实我们也实现了使用多项式特征然后再进行SVM的过程.</p>
<p>它的计算结果和直接使用PolynomialFeatures得到多项式特征再扔给SVM的结果是不一样的</p>
<p>说明这个kernel计算是有不一样的地方, 那么这个核(Kernel)函数到底是什么意思?</p>
<p><br></p>
<p><br></p>
<h2 id="什么是核函数"><a href="#什么是核函数" class="headerlink" title="什么是核函数"></a>什么是核函数</h2><p>再看一下SVM算法的本质是什么</p>
<p><img src="/Blog/intro/soft_margin_svm_6.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>SVM算法本质就是求解这样一个最优化问题, 由于是有条件的最优化问题, 对于求解它没有进行数学推导.  不过在这里, 只是介绍性地说一下:</p>
<p>在求解这个最优化问题的过程中, 我们还是要讲这个问题变形, 变成一个在数学上更好解的问题</p>
<p><img src="/Blog/intro/kernel_function.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>不对推导过程进行详细介绍, 可以查些资料</p>
</blockquote>
<p>这里我们要关注的是, 当我们把左边的最优化问题转变成右边的最优化问题之后, 这个式子中有非常重要的一项.</p>
<p><img src="/Blog/intro/kernel_function_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这项式子的后边部分套了两层 Σ, 且都是从1到m进行遍历</p>
<p>相应的在后面的式子中有 x<sub>i</sub>x<sub>j</sub> , 换句话说, 我们转变的这个式子, 对于我们样本数据集任意的两个向量都要做一下向量间的点乘</p>
<p>如果我们想使用多项式特征</p>
<blockquote>
<p>之前使用的方式是对于本来的样本数据 x<sub>i</sub> 添加多项式特征, 让它称为 x’<sub>i</sub>, 对于另外一个样本x<sub>j</sub>添加多项式特征, 让它称为 x’<sub>j</sub></p>
<p>这样做完之后我们使用新的特征计算, 所以红色方块中的式子就变为</p>
<script type="math/tex; mode=display">
x'^{(i)}\cdot x'^{(j)}</script></blockquote>
<p>核函数是这样一个思想: 有没有可能不将 x<sub>i</sub> 和 x<sub>j</sub> 先分别转换成 x’<sub>i</sub> 和 x’<sub>j</sub>. 有没有可能找到一个函数K</p>
<script type="math/tex; mode=display">
K(x^{(i)}, x^{(j)})=x'^{(i)}\cdot x'^{(j)}</script><p>直接对原来的两个样本进行数学运算, 直接计算出 x’<sub>i</sub> 和 x’<sub>j</sub></p>
<p>如果能设计出这样的一个函数K的话, 原来红色方块的内容就可以用K( x<sub>i</sub>, x<sub>j</sub> ) 表示</p>
<p><img src="/Blog/intro/kernel_function_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><img src="/Blog/intro/kernel_function_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>K 函数 取不同的函数, 就相当于在原始的样本上进行了一个变化, 把 x<sub>i</sub> 和 x<sub>j</sub> 变成 x’<sub>i</sub> 和 x’<sub>j</sub>, 在进行乘法</p>
<p>K函数就是我们不进行这种变形了, 直接拿样本代入函数中, 用这个函数直接算出x’<sub>i</sub> 和 x’<sub>j</sub>点乘的结果.</p>
<p>这个K函数就叫做<strong>核函数</strong></p>
<p><br></p>
<p>Kernel Function有的教材也叫Kernel Trick, 它其实是一个使用核函数的技巧</p>
<p>这个描述还是挺准确的, 这是因为我们不使用核函数的话, 完全可以达到同样的效果, 它更像是一个数学的技巧, 把核函数应用在这里就免去了对原来的样本进行变形的步骤</p>
<p>尤其是对于一些复杂的变形, 一方面计算量会有所减少, 另外一方面其实也节省了存储的空间 (因为通常是把低维的样本变成高维的数据, 存储高维的数据就会花很多的存储空间)</p>
<p>不过使用核函数就不需要存储变化后的结果</p>
<p><br></p>
<p>通过上面的介绍可以发现核函数<strong>不是SVM算法特有的一种技巧</strong></p>
<p>实质上只要算法转化成最优化问题, 在求解最优化问题中存在 x<sub>i</sub> 点乘 x<sub>j</sub> 这样的式子, 或者类似这样的式子, 我们就都可以应用核函数技巧</p>
<p><br></p>
<p><br></p>
<h2 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h2><p>以多项式核函数为例, 具体理解核函数是如何运作的</p>
<p>对于两个向量x, y</p>
<script type="math/tex; mode=display">
K(x,y)=(x\cdot y+1)^2</script><blockquote>
<p>为了计算方便, 展示2次多项式, 后面会再把它拓展成任意次项</p>
</blockquote>
<p> <img src="/Blog/intro/kernel_function_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<blockquote>
<p>3次项和的平方</p>
<script type="math/tex; mode=display">
(a+b+c)^2=a^2+b^2+c^2+2ab+2bc+2ca</script><p>任意项的和的平方的表达式:</p>
<script type="math/tex; mode=display">
(\sum_{i=1}^na_i)^2=\sum_{i=1}^na_i^2+2\sum_{i=2}^n\sum_{j=1}^{i-1}a_ia_j</script><p>从算法的角度简单解释一些后半部分的原理</p>
<script type="math/tex; mode=display">
2\sum_{i=2}^n\sum_{j=1}^{i-1}a_ia_j</script><p>简单来说就是2乘以<strong>所有不相同两个a</strong>的组合, 这里有两层循环, 外循环是i从2到n, 那么内循环j首先不能和i重合, 其次i和j的组合不能重复, 因此 j只能在i的一侧, 要么j &lt; i, 要么j &gt; i.</p>
<p>例如: i=4, j=1, 2, 3. 当i=5时, j=1, 2, 3, 4.</p>
<p>可以想象一下, 原本集合中有j个已知元素, 此时来了一个新的元素, 那么就会生成j个新的组合, 新的元素和每个已知元素组成一种新的组合. 原本j=1, 之后来了一个新的元素i=2, 1和2就是新的组合, 又来了一个新的元素i=3, 3和1和2又有新的组合. 有点类似与冒泡排序的两两比较.</p>
<p>当a<sub>i</sub>=x<sub>i</sub>y<sub>i</sub>时, 同时a<sub>0</sub>=1时</p>
<script type="math/tex; mode=display">
(\sum_{i=1}^nx_iy_i+1)^2=(\sum_{i=1}^nx_iy_i)^2+1^2+2\sum_{i=2}^n\sum_{j=1}^{i-1}x_iy_ix_jy_j+2\sum_{i=1}^nx_iy_i\cdot1</script><script type="math/tex; mode=display">
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =(\sum_{i=1}^nx_iy_i)^2+\sum_{i=2}^n\sum_{j=1}^{i-1}\sqrt{2}x_ix_j\sqrt{2}y_iy_j+\sum_{i=1}^n\sqrt{2}x_i\sqrt{2}y_i+1</script></blockquote>
<p>y’的变法和x’一样</p>
<p>x’就是将x添加2次项的结果</p>
<p>不过这里有个根2, 但其实影响不大, 因为最后送入线性SVM模型, 对于线性SVM来说求得是每一个特征的系数, 对于这些系数就相当于有一些二次项, 包括这些一次项前面自带了一个系数而已</p>
<p>例如x最终结果的系数是1, 因此求出的系数就应该是根2分之1, 与根2相乘等于1</p>
<p>这个根号2不影响结果.</p>
<p><br></p>
<p>而计算时完全不用计算这个复杂的式子, 直接点乘加一再平方就好了. 这个结果和变化成x’ y’ 的结果是一致的. 这就是核函数的优势. 大大降低了计算的复杂度</p>
<p><img src="/Blog/intro/kernel_function_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p><br></p>
<p>拓展一下, 由于核函数不仅仅能制造二次项多项式特征, 可以任意degree的特征</p>
<script type="math/tex; mode=display">
K(x,y)=(x\cdot y+c)^d</script><p>这就是传过来的degree, 另外常数项也可以传c, c和d就是另外两个超参数(分别对应 scikit-learn 的 SVC 中的 degree 参数和 coef0 参数)</p>
<p>这就是<strong>多项式核函数</strong></p>
<p>也就是scikit-learn中, kernel = poly的情况</p>
<p><br></p>
<p>理解核函数的基本概念: 其实就是<strong>替换原来的样本的x点乘y的操作</strong></p>
<p>我们其实也可以用核函数的思路表示<strong>线性SVM</strong></p>
<p><img src="/Blog/intro/kernel_function_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>当理解核函数的概念之后, 就可以介绍很多核函数, 它们对应不同的对原始样本点的转换</p>
<p>最为著名的核函数就是: 高斯核函数</p>
<p><br></p>
<p><br></p>
<h2 id="高斯核函数-RBF-核"><a href="#高斯核函数-RBF-核" class="headerlink" title="高斯核函数 ( RBF 核 )"></a>高斯核函数 ( RBF 核 )</h2><p>是SVM算法使用最多的一种核函数</p>
<p>K(x, y) 其实表示的是<strong>重新定义x和y的点乘</strong></p>
<p>高斯核函数的定义是这样的</p>
<script type="math/tex; mode=display">
K(x,y)=e^{-\gamma||x-y||^2}</script><p>γ就像是多项式核函数中degree或C一样, 是一个超参数</p>
<p>γ如何影响核函数进而影响机器学习算法, 在下一章会介绍</p>
<p>首先仔细看一下这个核函数. 为什么被称为高斯核函数</p>
<p>在概率论中有一个重要的函数: 高斯函数(正态分布)</p>
<p><img src="/Blog/intro/gaosi_func.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这个函数和高斯核函数是拥有一样的形态的</p>
<ul>
<li>高斯函数前面有一个非常大的系数, 而高斯核函数没有这个系数了</li>
<li>对于高斯函数, 它的幂次方表达式中x-μ的平方的系数相当于 -1/(2σ<sup>2</sup>), 这整个系数在高斯核函数中被 γ 代替了, 负号是不变的 (其实γ的大小和1/(2σ<sup>2</sup>)是有着共同的关系的)</li>
<li>最后在高斯函数中是x-μ的平方, 而在高斯核函数中, 由于表述的是两个向量之间的关系, 所以是两个向量的差的模的平方</li>
</ul>
<p><br></p>
<p>高斯核函数又是也被叫做RBF核 Radial Basis Function Kernel, 又被翻译成 <strong>径向基函数</strong></p>
<p>之前说对于多项式核函数的本质是把所有数据点首先添加了多项式项, 再将这些有了多项式项的新的特征进行点乘就形成了多项式核函数</p>
<p>同理高斯核函数本质也应该是将原来的数据点先映射成一种新的特征向量, 然后是新的特征向量点乘的结果. 事实正是如此, 只不过高斯核函数表达出的数据的映射是非常复杂的</p>
<blockquote>
<p>高斯核函数的本质是将每一个样本点映射到一个无穷维的特征空间</p>
<p>听起来非常抽象, 不对此进行数学推导</p>
</blockquote>
<p>通过这个说法, 高斯核函数对每个样本点的变形是非常复杂的, 但是经过这样的变形在点乘的结果是非常简单的, 就是这样一个式子</p>
<script type="math/tex; mode=display">
K(x,y)=e^{-\gamma||x-y||^2}</script><p>这就是核函数的威力, 不需要具体计算每一个样本点x和y到底经历了哪些变换, 我们直接关注点乘的结果就好了</p>
<p>我们尝试用一个简单的例子来模拟一下高斯核函数到底在作什么事情</p>
<p><br></p>
<h3 id="为什么多项式特征可以处理非线性数据"><a href="#为什么多项式特征可以处理非线性数据" class="headerlink" title="为什么多项式特征可以处理非线性数据"></a>为什么多项式特征可以处理非线性数据</h3><p>首先回忆一下<strong>使用多项式特征为什么可以处理非线性的数据问题</strong>?</p>
<p>依靠<strong>升维</strong>使得原本线性不可分的数据线性可分</p>
<p>比如原本数据是x, 现在扩充x<sup>2</sup>项这就使得原本线性不可分的数据线性可分</p>
<p><br></p>
<p>另一个例子:</p>
<p>原本是一维的数据有两个类, 跟显然现在数据是线性不可分的</p>
<p><img src="/Blog/intro/gaosi_func_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>无法用一根直线把两个类别分开, 但是如果我们添加多项式特征的话, 相当于是让数据升维, 让每一个数据点不但有横轴x值, 还有纵轴, 让纵轴是x<sup>2</sup>. 所有数据点一下子就变成这个样子</p>
<p><img src="/Blog/intro/poly_lin.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这些数据点在x轴上的位置是没有变的, 不过在y轴上也有一个取值, 这个取值就是x<sup>2</sup></p>
<p>一旦这样做了,  此时就是线性可分</p>
<p><img src="/Blog/intro/poly_lin_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>通过升维让原本线性不可分的数据线性可分</p>
<p><br></p>
<h3 id="高斯核所做的事情"><a href="#高斯核所做的事情" class="headerlink" title="高斯核所做的事情"></a>高斯核所做的事情</h3><p>其实高斯核所做的事情也是一样的事情</p>
<script type="math/tex; mode=display">
K(x,y)=e^{-\gamma||x-y||^2}</script><p>为了方便可视化, 首先对核函数进行一个改变, 将y的值固定, 换句话说, y的值不取样本点, 而是取固定点, 取两个固定点当y, 分别为l1, l2</p>
<p><img src="/Blog/intro/poly_lin_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这个图依然是原本是1维的样本数据点, 依然是线性不可分的</p>
<p>在这个一维的样本空间中, 有两个特殊的点l1, l2 (landmark) 地标.</p>
<p>高斯核函数做的升维的过程是在作一件什么事?</p>
<p>如果有两个地标, 就将x升维成二维的样本点, 每个维度的取值为</p>
<p><img src="/Blog/intro/poly_lin_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这样就将原本是一维的样本点映射到二维的空间</p>
<p><br></p>
<p>用程序模拟一下, 这样的映射是怎样将原来线性不可分的数据变得线性可分的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># x就是样本数据, 只有一个特征</span><br>    x = np.arange(-<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br>    y = np.array((x &gt;= -<span class="hljs-number">2</span>) &amp; (x &lt;= <span class="hljs-number">2</span>), dtype=<span class="hljs-string">&quot;int&quot;</span>)<br><br>    plt.scatter(x[y==<span class="hljs-number">0</span>], [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(x[y==<span class="hljs-number">0</span>]))<br>    plt.scatter(x[y==<span class="hljs-number">1</span>], [<span class="hljs-number">0</span>]*<span class="hljs-built_in">len</span>(x[y==<span class="hljs-number">1</span>]))<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/poly_lin_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这显然是线性不可分的数据, 下面使用高斯核函数将一维数据映射到二维空间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># x为数据点, l为地标</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gaussian</span>(<span class="hljs-params">x, l</span>):<br>    gamma = <span class="hljs-number">1.0</span><br>    <span class="hljs-comment"># x和l是一维数据, 所以不用计算模</span><br>    <span class="hljs-keyword">return</span> np.exp(-gamma * (x-l)**<span class="hljs-number">2</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># x就是样本数据, 只有一个特征</span><br>    x = np.arange(-<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>)<br>    y = np.array((x &gt;= -<span class="hljs-number">2</span>) &amp; (x &lt;= <span class="hljs-number">2</span>), dtype=<span class="hljs-string">&quot;int&quot;</span>)<br><br>    l1, l2 = -<span class="hljs-number">1</span>, <span class="hljs-number">1</span><br><br>    X_new = np.empty((<span class="hljs-built_in">len</span>(x), <span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(x):<br>        X_new[i, <span class="hljs-number">0</span>] = gaussian(data, l1)<br>        X_new[i, <span class="hljs-number">1</span>] = gaussian(data, l2)<br><br>    plt.scatter(X_new[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X_new[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X_new[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_new[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/poly_lin_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>换句话说, 用高斯核函数将原本的一维数据映射成这样的二维数据</p>
<p>对于这样一个二维数据显然是线性可分的</p>
<p><img src="/Blog/intro/poly_lin_6.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>用一种不同于多项式核的形式, 将一维数据映射为二维数据</p>
<p><br></p>
<p>之前的方式是将一维数据点使用这样的方式映射成二维数据</p>
<p><img src="/Blog/intro/poly_lin_7.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>只不过对于这个高斯核的y的部分固定为两个地标点l1和l2</p>
<p>但实际上核函数是这样的</p>
<script type="math/tex; mode=display">
K(x,y)=e^{-\gamma||x-y||^2}</script><p>所以可以看出y是每一个数据点</p>
<p>换句话说高斯核函数干的事情和我们之前干的事情一样, 只不过地标点取得比我们之前取得要多得多.</p>
<p><strong>高斯核: 对于每一个数据点都是landmark</strong></p>
<p>对于每一个样本x, 都尝试对每一个样本y进行核函数的计算, 成为高维数据相对应的某一个维度的元素</p>
<p>高斯核做的事情也是升维, 它将<strong>m乘n</strong>的数据映射成<strong>m乘m</strong>的数据</p>
<p>原本只有n个维度的数据, 经过高斯核的映射就变成了m乘m维的数据(维度等于样本数据点的个数)</p>
<blockquote>
<p>高斯核函数的本质是将数据映射到<strong>无穷维</strong>的空间</p>
<p>由于样本数据点可以有穷多个, 所以它映射到无穷维的空间.</p>
<p>但当我们具体拿到的数据是有限个, 所以映射到m维空间</p>
</blockquote>
<p><br></p>
<p>可以想象, 高斯核函数的<strong>开销是非常大</strong>的, 但是还是有一些应用是适合使用高斯核函数的</p>
<p>例如: 初始数据维度就非常高, 但是我们样本数据的数量并不多, 换句话说 m &lt; n, 此时使用高斯核函数就非常划算了</p>
<p>最典型的一个应用领域就是自然语言处理, 在自然语言处理领域通常我们会构建一个非常高维的特征空间, 但是很有可能样本数量是并不多的</p>
<p><br></p>
<h2 id="高斯-RBF-核函数的gamma"><a href="#高斯-RBF-核函数的gamma" class="headerlink" title="高斯(RBF)核函数的gamma"></a>高斯(RBF)核函数的gamma</h2><script type="math/tex; mode=display">
K(x,y)=e^{-\gamma||x-y||^2}</script><p>高斯核函数γ这个参数是什么意思?</p>
<p>高斯核函数和高斯函数(正太分布)的形式是一致的, 而在概率论中正态分布的表达式有两个参数</p>
<p>μ和σ, μ代表均值决定了高斯函数中心轴的位置, σ代表标准差, 描述样本数据分布的情况</p>
<p><img src="/Blog/intro/gamma.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<ul>
<li>σ越小, 钟形图案就越窄越集中. </li>
<li>σ越大, 钟形图案就越平缓</li>
</ul>
<p><br></p>
<p>同理这样一个影响在高斯核函数中是一致的</p>
<p>不过高斯函数中</p>
<script type="math/tex; mode=display">
-\frac1{2\sigma^2}</script><p>这一项变为</p>
<script type="math/tex; mode=display">
-\gamma</script><p>所以γ的趋势和σ的趋势是相反的</p>
<ul>
<li>gamma越大, 高斯分布越窄, 越集中</li>
<li>gamma越小, 高斯分布越宽, 越胖</li>
</ul>
<p><br></p>
<h2 id="在scikit-learn中使用高斯核函数"><a href="#在scikit-learn中使用高斯核函数" class="headerlink" title="在scikit-learn中使用高斯核函数"></a>在scikit-learn中使用高斯核函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">RBFKernelSVC</span>(<span class="hljs-params">gamma=<span class="hljs-number">1.0</span></span>):<br>    <span class="hljs-keyword">return</span> Pipeline([<br>        (<span class="hljs-string">&quot;std_scaler&quot;</span>, StandardScaler()),<br>        (<span class="hljs-string">&quot;SVC&quot;</span>, SVC(kernel=<span class="hljs-string">&quot;rbf&quot;</span>, gamma=gamma))<br>    ])<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># datasets 生成数据集</span><br>    <span class="hljs-comment"># 添加一点噪音, 可以理解为增大0.15标准差</span><br>    X, y = datasets.make_moons(noise=<span class="hljs-number">0.15</span>, random_state=<span class="hljs-number">666</span>)<br><br>    rbf_svc = RBFKernelSVC(gamma=<span class="hljs-number">1</span>)<br>    rbf_svc.fit(X, y)<br>    plot_decision_boundary(rbf_svc, [-<span class="hljs-number">1.5</span>, <span class="hljs-number">2.5</span>, -<span class="hljs-number">1.0</span>, <span class="hljs-number">1.5</span>])<br>    plt.scatter(X[y == <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    plt.scatter(X[y == <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y == <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/Blog/intro/gamma_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>这就是使用RBF Kernel在gamma=1的时候的决策边界</p>
<p><br></p>
<p>当gamma=100时, 决策边界:</p>
<p><img src="/Blog/intro/gamma_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>之前讲过gamma取值越大, 意思就是正态分布钟形图案越窄, 而决策边界的变化就是: 针对其中的某一类, 针对这一类每一个样本点周围都形成一种钟形图案 (<strong>可以想象成我们实在俯视钟形图案, 蓝色点就是每一个钟形图案的顶点</strong>)</p>
<p>由于gamma值比较大, 钟形图案就比较窄, 所以每一个蓝色点周围, 围绕了一定的区域, 我们的决策算法判定在这个区域中才是蓝色的点, 否则就是红色的点</p>
<p>这也是RBF Kernel的几何意义</p>
<p>我们这样得到的决策边界显然<strong>过拟合</strong>了, 过于和训练数据集相关了.</p>
<p><br></p>
<p>稍微减少一下gamma, gamma=10时:</p>
<p><img src="/Blog/intro/gamma_3.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>对于每一个蓝色的点, 它的钟形图案变得宽了一些, 所以蓝色的点, 点和点离得比较近, 钟形图案就融合在一起, 最终形成这样的决策边界</p>
<p>回头看gamma=1时, 就相当于钟形图案更宽了, 所以整体红色的范围就更大了</p>
<p><br></p>
<p>再减小gamma, gamma=0.5</p>
<p><img src="/Blog/intro/gamma_4.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>结果是这样, 红色范围更大了</p>
<p><br></p>
<p>gamma=0.1</p>
<p><img src="/Blog/intro/gamma_5.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>此时决策边界已经和线性的决策边界差不多了, 此时显然是under fitting<strong>欠拟合</strong>了</p>
<p><br></p>
<ul>
<li><p>gamma非常小就欠拟合了</p>
</li>
<li><p>gamma非常大就过拟合了</p>
</li>
</ul>
<p>调整gamma相当于再调整模型的复杂度, gamma越小, 模型复杂度越低. gamma越大, 模型复杂度越高</p>
<p><br></p>
<p><br></p>
<h2 id="SVM思想解决回归问题"><a href="#SVM思想解决回归问题" class="headerlink" title="SVM思想解决回归问题"></a>SVM思想解决回归问题</h2><p><img src="/Blog/intro/svm_reg.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>回归问题本质就是找到这样的拟合线拟合所有数据点. <strong>怎么定义拟合, 就是各个回归算法的关键</strong></p>
<p>例如: 线性回归算法定义拟合的方式就是数据点到预测直线的MSE最小</p>
<p><br></p>
<p>而对于SVM算法来说, 拟合的定义时另外一个定义:</p>
<p>我们要指定一个margin值, 期望margin范围里包含的样本数据点越多越好</p>
<p><img src="/Blog/intro/svm_reg_1.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>如果margin范围里, 样本数据点越多, 就代表这个范围能够比较好地表达样本数据点. 这种情况下取中间的直线作为真正的回归的结果.</p>
<p>用SVM解决回归问题和用SVM解决分类问题是相反的思路:</p>
<p>解决分类问题是期望margin里面的点越少越好, Hard Margin 更是要求一个点不能有</p>
<p><img src="/Blog/intro/svm_reg_2.PNG" srcset="/Blog/img/loading.gif" lazyload alt=""></p>
<p>而解决回归问题期望在margin范围里点越多越好, 在具体训练SVM回归问题结果时, 是对margin进行指定, 此时就引入了一个超参数ε</p>
<p>指定上下两根直线到中间直线的距离</p>
<blockquote>
<p>不推导这样的思路会对应一个什么样的最优化问题, 以及这个最优化问题具体应该怎样解决</p>
</blockquote>
<p><br></p>
<p>scikit-learn中的SVR具体解决回归问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVR, LinearSVR<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">StandardLinearSVR</span>(<span class="hljs-params">epsilon=<span class="hljs-number">0.1</span></span>):<br>    <span class="hljs-keyword">return</span> Pipeline([<br>        (<span class="hljs-string">&quot;std_scaler&quot;</span>, StandardScaler()),<br>        <span class="hljs-comment"># 另外一个超参数C使用默认值</span><br>        (<span class="hljs-string">&quot;linear_svr&quot;</span>, LinearSVR(epsilon=epsilon))<br>    ])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">StandardSVR</span>(<span class="hljs-params">epsilon=<span class="hljs-number">0.1</span></span>):<br>    <span class="hljs-keyword">return</span> Pipeline([<br>        (<span class="hljs-string">&quot;std_scaler&quot;</span>, StandardScaler()),<br>        (<span class="hljs-string">&quot;svr&quot;</span>, SVR(kernel=<span class="hljs-string">&quot;rbf&quot;</span>, epsilon=epsilon))<br>    ])<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    boston = datasets.load_boston()<br>    X = boston.data<br>    y = boston.target<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="hljs-number">666</span>)<br><br>    svr = StandardLinearSVR()<br>    svr.fit(X_train, y_train)<br><br>    <span class="hljs-comment"># 0.63572</span><br>    <span class="hljs-built_in">print</span>(svr.score(X_test, y_test))<br></code></pre></td></tr></table></figure>
<p>许多超参数都可以调节: epsilon, C, kernel, polyFeatures的degree……</p>
<p>由于SVM涉及的数学知识太深, 我们没有从底层对它进行实现, 但是SVM的思想已经介绍完了.</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/Blog/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/Blog/tags/python3/">python3</a>
                    
                      <a class="hover-with-bg" href="/Blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/Blog/2020/05/16/machine-learning-10/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">python3入门机器学习(10)-决策树</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/Blog/2020/05/08/machine-learning-8/">
                        <span class="hidden-mobile">python3入门机器学习(8)-评价分类结果</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/Blog/js/events.js" ></script>
<script  src="/Blog/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/Blog/js/local-search.js" ></script>



  
    <script  src="/Blog/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/Blog/js/boot.js" ></script>


</body>
</html>
